#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —á–∞–Ω–∫–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    uv run python -m src.data_processing.build_index
    uv run python -m src.data_processing.build_index --test
    
    # –ò–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
    uv run python src/data_processing/build_index.py --test
"""

import argparse
import sys
from pathlib import Path

from langchain_core.documents import Document

from src.data_processing import FAISS_INDEX_DIR, PROCESSED_DATA_DIR
from src.vector_store.faiss_store import FAISSRetriever, get_embeddings


def load_chunks_from_directory(chunks_dir: Path) -> list[Document]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏."""
    documents = []

    if not chunks_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {chunks_dir}")
        return documents

    txt_files = sorted(chunks_dir.glob("*.txt"))

    if not txt_files:
        print(f"‚ö†Ô∏è –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ {chunks_dir}")
        return documents

    print(f"üìÇ {chunks_dir}")
    print(f"   –§–∞–π–ª–æ–≤: {len(txt_files)}")

    for txt_file in txt_files:
        try:
            content = txt_file.read_text(encoding="utf-8").strip()

            if not content:
                continue

            lines = content.split("\n")
            title = lines[0].lstrip("# ").strip() if lines else txt_file.stem

            # –î–ª—è e5 –º–æ–¥–µ–ª–µ–π –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "passage: "
            prefixed_content = f"passage: {content}"

            doc = Document(
                page_content=prefixed_content,
                metadata={
                    "source": str(txt_file.name),
                    "title": title,
                    "chunk_id": txt_file.stem,
                    "source_dir": str(chunks_dir.name),
                },
            )
            documents.append(doc)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {txt_file.name}: {e}")
            continue

    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(documents)}")
    return documents


def build_index(
    chunks_dirs: list[Path],
    index_path: Path,
    model_name: str = "intfloat/multilingual-e5-small",
) -> FAISSRetriever:
    """–°—Ç—Ä–æ–∏—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π."""
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞")
    print(f"üì¶ –ú–æ–¥–µ–ª—å: {model_name}")
    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    embeddings = get_embeddings(model_name)

    all_documents = []
    for chunks_dir in chunks_dirs:
        docs = load_chunks_from_directory(chunks_dir)
        all_documents.extend(docs)

    if not all_documents:
        print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        sys.exit(1)

    print(f"\nüìä –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)}")
    print("‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

    retriever = FAISSRetriever(index_path=index_path, embeddings=embeddings)
    retriever.add_documents(all_documents)
    retriever.save()

    print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω!")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {retriever.document_count}")
    print(f"   –ü—É—Ç—å: {index_path}")

    return retriever


def test_search(retriever: FAISSRetriever, queries: list[str]) -> None:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É."""
    print("\nüîç –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫:")
    print("-" * 60)

    for query in queries:
        print(f"\n‚ùì {query}")
        results = retriever.retrieve_with_scores(query, k=3)

        if not results:
            print("   (–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)")
            continue

        for i, (doc, score) in enumerate(results, 1):
            title = doc.metadata.get("title", "?")
            content = doc.page_content.replace("passage: ", "")[:100]
            print(f"   [{i}] {score:.3f} | {title}")
            print(f"       {content}...")


def main():
    parser = argparse.ArgumentParser(description="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞")
    parser.add_argument(
        "--chunks-dir",
        type=Path,
        action="append",
        dest="chunks_dirs",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏ (–º–æ–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ)",
    )
    parser.add_argument(
        "--index-path",
        type=Path,
        default=FAISS_INDEX_DIR,
        help=f"–ü—É—Ç—å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ (default: {FAISS_INDEX_DIR})",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="intfloat/multilingual-e5-small",
        help="–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã",
    )

    args = parser.parse_args()

    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if not args.chunks_dirs:
        default_dirs = [
            PROCESSED_DATA_DIR / "python" / "chunks",
        ]
        args.chunks_dirs = [d for d in default_dirs if d.exists()]

    if not args.chunks_dirs:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —á–∞–Ω–∫–∞–º–∏")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: uv run python -m src.data_processing.extract_handbook")
        sys.exit(1)

    print("=" * 60)
    print("üöÄ FAISS Index Builder")
    print("=" * 60)

    retriever = build_index(
        chunks_dirs=args.chunks_dirs,
        index_path=args.index_path,
        model_name=args.model,
    )

    if args.test:
        test_queries = [
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –≤ Python?",
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ü–∏–∫–ª for?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ list comprehension?",
            "–ö–∞–∫ —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã –≤ Python?",
            "–û–±—ä—è—Å–Ω–∏ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤",
        ]
        test_search(retriever, test_queries)

    print("\n" + "=" * 60)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print("=" * 60)


if __name__ == "__main__":
    main()

