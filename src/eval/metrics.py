"""RAG evaluation metrics."""

from __future__ import annotations

import json
import logging
import math
import re
from typing import Any

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate


def recall_at_k(
    retrieved_docs: list[Document], relevant_doc_ids: set[str], k: int
) -> float:
    """Recall@k = relevant_in_top_k / total_relevant."""
    if not relevant_doc_ids:
        return 0.0

    top_k = retrieved_docs[:k]
    retrieved_ids = {_get_doc_id(doc) for doc in top_k}

    relevant_retrieved = len(retrieved_ids & relevant_doc_ids)
    return relevant_retrieved / len(relevant_doc_ids)


def precision_at_k(
    retrieved_docs: list[Document], relevant_doc_ids: set[str], k: int
) -> float:
    """Precision@k = relevant_in_top_k / k."""
    if k == 0:
        return 0.0

    top_k = retrieved_docs[:k]
    retrieved_ids = {_get_doc_id(doc) for doc in top_k}

    relevant_retrieved = len(retrieved_ids & relevant_doc_ids)
    return relevant_retrieved / k


def mrr_at_k(
    retrieved_docs: list[Document], relevant_doc_ids: set[str], k: int
) -> float:
    """MRR@k = 1 / rank_of_first_relevant."""
    top_k = retrieved_docs[:k]

    for rank, doc in enumerate(top_k, start=1):
        doc_id = _get_doc_id(doc)
        if doc_id in relevant_doc_ids:
            return 1.0 / rank

    return 0.0


def ndcg_at_k(
    retrieved_docs: list[Document], relevant_doc_ids: set[str], k: int
) -> float:
    """NDCG@k with binary relevance."""
    top_k = retrieved_docs[:k]

    dcg = 0.0
    for rank, doc in enumerate(top_k, start=1):
        doc_id = _get_doc_id(doc)
        relevance = 1.0 if doc_id in relevant_doc_ids else 0.0
        dcg += relevance / math.log2(rank + 1)

    num_relevant = len(relevant_doc_ids)
    idcg = 0.0
    for rank in range(1, min(k, num_relevant) + 1):
        idcg += 1.0 / math.log2(rank + 1)

    if idcg == 0.0:
        return 0.0

    return dcg / idcg


def _get_doc_id(doc: Document) -> str:
    """Extracts document ID from metadata."""
    meta = doc.metadata or {}
    return (
        meta.get("chunk_id") or meta.get("source") or meta.get("title") or str(id(doc))
    )


def compute_retrieval_metrics(
    retrieved_docs: list[Document],
    relevant_doc_ids: set[str],
    k_values: list[int] | None = None,
) -> dict[str, float]:
    """Computes retrieval metrics for given k values."""
    if k_values is None:
        k_values = [1, 3, 5, 10]
    metrics: dict[str, float] = {}

    for k in k_values:
        metrics[f"recall@{k}"] = recall_at_k(retrieved_docs, relevant_doc_ids, k)
        metrics[f"precision@{k}"] = precision_at_k(retrieved_docs, relevant_doc_ids, k)
        metrics[f"mrr@{k}"] = mrr_at_k(retrieved_docs, relevant_doc_ids, k)
        metrics[f"ndcg@{k}"] = ndcg_at_k(retrieved_docs, relevant_doc_ids, k)

    return metrics


def compute_groundedness(answer: str, sources: list[Document]) -> float:
    """Checks if answer references sources. Returns 1.0 if sources section exists, 0.5 if mentions found, 0.0 otherwise."""
    answer_lower = answer.lower()
    has_sources_section = "üìö –∏—Å—Ç–æ—á–Ω–∏–∫–∏" in answer_lower or "–∏—Å—Ç–æ—á–Ω–∏–∫–∏:" in answer_lower

    has_source_mentions = False
    if sources:
        for source in sources[:3]:
            meta = source.metadata or {}
            title = meta.get("title", "")
            if title and title.lower() in answer_lower:
                has_source_mentions = True
                break

    if has_sources_section:
        return 1.0
    elif has_source_mentions:
        return 0.5
    else:
        return 0.0


async def llm_as_judge(
    question: str,
    answer: str,
    context: str,
    judge_model: Any,
) -> dict[str, float]:
    """Evaluates answer quality using LLM judge. Returns scores for correctness, completeness, clarity, usefulness."""
    logger = logging.getLogger(__name__)

    judge_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–æ—Ü–µ–Ω—â–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è–º.

–û—Ü–µ–Ω–∏ –æ—Ç–≤–µ—Ç –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–∫–∞–∂–¥—ã–π –æ—Ç 0.0 –¥–æ 1.0):
1. Correctness (–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å): –ù–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω?
2. Completeness (–ø–æ–ª–Ω–æ—Ç–∞): –ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ –æ—Ç–≤–µ—Ç –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å?
3. Clarity (—è—Å–Ω–æ—Å—Ç—å): –ù–∞—Å–∫–æ–ª—å–∫–æ —è—Å–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –∏–∑–ª–æ–∂–µ–Ω –æ—Ç–≤–µ—Ç?
4. Usefulness (–ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å): –ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é?

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ markdown —Ä–∞–∑–º–µ—Ç–∫–∏.
–§–æ—Ä–º–∞—Ç: {{"correctness": 0.0, "completeness": 0.0, "clarity": 0.0, "usefulness": 0.0}}
–ó–∞–º–µ–Ω–∏ 0.0 –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 0.0 –¥–æ 1.0.""",
            ),
            (
                "human",
                """–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç (–∏—Å—Ç–æ—á–Ω–∏–∫–∏):
{context}

–û—Ç–≤–µ—Ç:
{answer}

–û—Ü–µ–Ω–∏ –æ—Ç–≤–µ—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –æ–±—ä–µ–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
{{"correctness": 0.0, "completeness": 0.0, "clarity": 0.0, "usefulness": 0.0}}""",
            ),
        ]
    )

    try:
        chain = judge_prompt | judge_model | StrOutputParser()
        response_text = await chain.ainvoke(
            {
                "question": question,
                "context": context or "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω)",
                "answer": answer,
            }
        )

        logger.debug(f"LLM-as-judge response (first 500 chars): {response_text[:500]}")

        code_block_match = re.search(
            r"```(?:json)?\s*(\{.*?\})\s*```", response_text, re.DOTALL
        )
        if code_block_match:
            try:
                scores = json.loads(code_block_match.group(1))
                return {
                    "correctness": float(scores.get("correctness", 0.0)),
                    "completeness": float(scores.get("completeness", 0.0)),
                    "clarity": float(scores.get("clarity", 0.0)),
                    "usefulness": float(scores.get("usefulness", 0.0)),
                }
            except (json.JSONDecodeError, ValueError) as e:
                logger.debug(f"Failed to parse JSON from code block: {e}")

        json_match = re.search(
            r'\{[^{}]*"correctness"[^{}]*(?:,"[^"]+"[^{}]*)*\}',
            response_text,
            re.DOTALL,
        )
        if json_match:
            try:
                scores = json.loads(json_match.group(0))
                return {
                    "correctness": float(scores.get("correctness", 0.0)),
                    "completeness": float(scores.get("completeness", 0.0)),
                    "clarity": float(scores.get("clarity", 0.0)),
                    "usefulness": float(scores.get("usefulness", 0.0)),
                }
            except (json.JSONDecodeError, ValueError) as e:
                logger.debug(f"Failed to parse JSON from first regex: {e}")

        json_match = re.search(
            r'\{[^{}]*"correctness"[^{}]*\}', response_text, re.DOTALL
        )
        if json_match:
            try:
                scores = json.loads(json_match.group(0))
                return {
                    "correctness": float(scores.get("correctness", 0.0)),
                    "completeness": float(scores.get("completeness", 0.0)),
                    "clarity": float(scores.get("clarity", 0.0)),
                    "usefulness": float(scores.get("usefulness", 0.0)),
                }
            except (json.JSONDecodeError, ValueError) as e:
                logger.debug(f"Failed to parse JSON from second regex: {e}")

        json_match = re.search(
            r'\{[^}]*"correctness"[^}]*"completeness"[^}]*"clarity"[^}]*"usefulness"[^}]*\}',
            response_text,
            re.DOTALL,
        )
        if json_match:
            try:
                scores = json.loads(json_match.group(0))
                return {
                    "correctness": float(scores.get("correctness", 0.0)),
                    "completeness": float(scores.get("completeness", 0.0)),
                    "clarity": float(scores.get("clarity", 0.0)),
                    "usefulness": float(scores.get("usefulness", 0.0)),
                }
            except (json.JSONDecodeError, ValueError) as e:
                logger.debug(f"Failed to parse JSON from third regex: {e}")

        logger.warning(
            f"Could not parse JSON from LLM response. Full response: {response_text[:1000]}"
        )

    except Exception as e:
        logger.warning(f"LLM-as-judge error: {e}", exc_info=True)

    return {
        "correctness": 0.0,
        "completeness": 0.0,
        "clarity": 0.0,
        "usefulness": 0.0,
    }
