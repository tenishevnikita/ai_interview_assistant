<html lang="ru" class="__className_a39d3e __variable_a39d3e __variable_5a49b6 __variable_2ac229 HR-9-31-0"><head><script src="https://yastatic.net/s3/gdpr/v3/gdpr.js" type="text/javascript" charset="utf-8" async=""></script><style id="react-aria-pressable-style">@layer {
  [data-react-aria-pressable] {
    touch-action: pan-x pan-y pinch-zoom;
  }
}</style><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="viewport" content="initial-scale=1, width=device-width"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/logo_mobile_8bc5eb38fb.svg"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/edu_logo_simple_9790e70002.svg"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/social_icon_vk_97bf858cd5.svg"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/social_icon_yt_d20daea655.svg"><link rel="preload" as="image" href="https://yastatic.net/s3/education-portal/media/social_icon_tg_9faafb663e.svg"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/00e25bd25bbd0437.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb46cd5c40721db6.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/85781da132a474d7.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/df0a82ffea795a48.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/c800df164b7b8c22.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/033fa3e38f305e9b.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/80e3798aa7f5abc0.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/991aec17f3093e7d.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5f16a3a32691d755.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dbd806e03d28c8cc.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5dfa285aa1529eec.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/051319dc975c787e.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/17fdf933d7378e89.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a2f4cf3f3bb12d85.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d844ab516aaccc48.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb0e02f100b08559.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/381835674ab7c3f7.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/8268a37c4890f71e.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/e2bd809ab2e171e9.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/84f1d4eada6960dd.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cee7c32dde4c9323.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ef486034b69d75eb.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/4453bad77d6636b4.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dfbd22c99f7f399e.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5c966d7ef241f1d1.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b1ea991da29cd10f.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cb7ec9243c9cb1bf.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ca947b8fe9734df5.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a412a0891b49ea16.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ddb8c975b9776637.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9425ba6f0e81e31a.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/feca8288b0586eb3.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d238d176bd8ae895.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9fc77bcede8e7238.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/daf421f1dcbde59d.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/bd270492e7e016dc.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/500049d7c719a090.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/3e9ccf27ab1f71e9.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b6dc735f8f16428c.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="stylesheet" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/094d7d6ad311c845.css" crossorigin="anonymous" nonce="" data-precedence="next"><link rel="preload" as="script" fetchpriority="low" nonce="" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/webpack-05f11ea662126375.js" crossorigin=""><script async="" src="https://mc.yandex.ru/metrika/tag.js"></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/2a182bb7-47baf338ad7d9e66.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/9500-a8aeea1077b58079.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/main-app-3c64833220c523df.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/1281-08bad9a3d2a60887.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/5080-82f4daed94452227.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/4194-172db2b948bd13ee.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/5609-185e25579372eca4.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/8619-3a701f3773450188.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/9275-7c8f52465eb2613c.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/9515-b7b857111af67561.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/layout-e3a20ad13303521f.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/app/layout-e78d4be001a57a28.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/2665-c77f7b9517a535fe.js" async="" crossorigin="" nonce=""></script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/app/error-da5ebf8e69abf3a6.js" async="" crossorigin="" nonce=""></script><link rel="preload" href="https://yastatic.net/s3/cloud/forms/_/embed.js" as="script" nonce=""><meta name="theme-color" content="#a48eef"><meta name="next-size-adjust" content=""><link rel="icon" href="https://yastatic.net/s3/education-portal/web/favicon.ico" sizes="any"><link rel="icon" href="https://yastatic.net/s3/education-portal/web/icon.svg" type="image/svg+xml"><link rel="apple-touch-icon" href="https://yastatic.net/s3/education-portal/web/apple-touch-icon.png"><title>Чему вы научились</title><meta name="description" content="Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов."><link rel="manifest" href="/manifest.webmanifest"><meta name="mobile-web-app-capable" content="yes"><meta name="BookSlug" content="math"><link rel="canonical" href="https://education.yandex.ru/handbook/math/article/math-glava-chetire-chemu-vi-nauchilis"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Яндекс Образование"><link href="https://yastatic.net/s3/education-portal/pwa/logos/ios/512.png" rel="apple-touch-startup-image"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta property="og:title" content="Чему вы научились"><meta property="og:description" content="Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов."><meta property="og:url" content="https://education.yandex.ru/handbook/math/article/math-glava-chetire-chemu-vi-nauchilis"><meta property="og:image" content="https://yastatic.net/s3/education-portal/media/opengraf_4d3d33f601_7c25b24838.webp"><meta property="og:type" content="website"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Чему вы научились"><meta name="twitter:description" content="Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов."><meta name="twitter:image" content="https://yastatic.net/s3/education-portal/media/opengraf_4d3d33f601_7c25b24838.webp"><script id="ab-test-data" nonce="">window.__AB_TEST_DATA = {"flags":{},"experiments":"P54lnS9LcLo,"};</script><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/polyfills-42372ed130431b0a.js" crossorigin="anonymous" nomodule="" nonce=""></script><script nonce="" id="rum-error">!function(e,t){if(e.Ya=e.Ya||{},Ya.Rum)throw new Error("Rum: interface is already defined");var n=e.performance,i=n&&n.timing&&n.timing.navigationStart||Ya.startPageLoad||+new Date,s=e.requestAnimationFrame,r=function(){},a=Ya.Rum={_defTimes:[],_defRes:[],_countersToExposeAsEvents:["2325","2616.85.1928","react.inited"],enabled:!!n,version:"6.1.21",vsStart:document.visibilityState,vsChanged:!1,vsChangeTime:1/0,_deltaMarks:{},_markListeners:{},_onComplete:[],_onInit:[],_unsubscribers:[],_eventLisneters:{},_settings:{},_vars:{},init:function(e,t){a._settings=e,a._vars=t},getTime:n&&n.now?function(){return n.now()}:Date.now?function(){return Date.now()-i}:function(){return new Date-i},time:function(e){a._deltaMarks[e]=[a.getTime()]},timeEnd:function(e,t){var n=a._deltaMarks[e];n&&0!==n.length&&n.push(a.getTime(),t)},sendTimeMark:function(e,t,n,i){void 0===t&&(t=a.getTime()),a.emit({metricName:"defTimes",data:[e,t,i]}),a.mark(e,t)},sendDelta:function(e,t,n,i){var s,r=a._deltaMarks;r[e]||void 0===t||(s=i&&i.originalEndTime?i.originalEndTime:a.getTime(),r[e]=[s-t,s,n])},sendResTiming:function(e,t){a.emit({metricName:"defRes",data:[e,t]})},sendRaf:function(e){var t=a.getSetting("forcePaintTimeSending");if(s&&(t||a.isTimeAfterPageShow(a.getTime()))){var n="2616."+e;s((function(){a.getSetting("sendFirstRaf")&&a.sendTimeMark(n+".205"),s((function(){a.sendTimeMark(n+".1928")}))}))}},isVisibilityChanged:function(){return a.vsStart&&("visible"!==a.vsStart||a.vsChanged)},isTimeAfterPageShow:function(e){return"visible"===a.vsStart||a.vsChangeTime<e},mark:n&&n.mark?function(e,t){n.mark(e+(t?": "+t:""))}:function(){},getSetting:function(e){var t=a._settings[e];return null===t?null:t||""},on:function(e,t){if("function"==typeof t)return(a._markListeners[e]=a._markListeners[e]||[]).push(t),function(){if(a._markListeners[e]){var n=a._markListeners[e].indexOf(t);n>-1&&a._markListeners[e].splice(n,1)}}},noop:r,sendTrafficData:r,finalizeLayoutShiftScore:r,finalizeLargestContentfulPaint:r,getLCPAdditionalParams:r,getCLSAdditionalParams:r,getINPAdditionalParams:r,getImageGoodnessAdditionalParams:r,_eventListeners:{},_eventsBuffer:{},subscribe:function(e,t){if(!a.getSetting("noEvents"))return this._eventLisneters[e]=this._eventLisneters[e]||new Set,this._eventLisneters[e].add(t),function(){this.unsubscribe(e,t)}.bind(this)},unsubscribe:function(e,t){this._eventLisneters[e].delete(t)},emit:function(e){if(!a.getSetting("noEvents")){var t=a.getSetting("eventsLimits")&&a.getSetting("eventsLimits")[e.metricName]||20;this._eventLisneters[e.metricName]&&this._eventLisneters[e.metricName].forEach((function(t){t(e)})),this._eventsBuffer[e.metricName]=this._eventsBuffer[e.metricName]||[],this._eventsBuffer[e.metricName].push(e),this._eventsBuffer[e.metricName].length>t&&(this._eventsBuffer[e.metricName].length=Math.floor(t/2))}},getBufferedEvents:function(e){var t=this._eventsBuffer,n={};return Object.keys(t).forEach((function(i){-1!==e.indexOf(i)&&(n[i]=t[i])})),n},clearEvents:function(e){this._eventsBuffer[e]&&(this._eventsBuffer[e].length=0)}};function f(){Ya.Rum.vsChanged=!0,Ya.Rum.vsChangeTime=a.getTime(),removeEventListener("visibilitychange",f)}addEventListener("visibilitychange",f),a._onVisibilityChange=f}(window);
!function(){if(window.PerformanceLongTaskTiming){var e=function(e,n){return(e=e.concat(n)).length>300&&(e=e.slice(e.length-300)),e},n="undefined"!=typeof PerformanceLongAnimationFrameTiming,t=n?["longtask","long-animation-frame"]:["longtask"];function r(){var r=Ya.Rum._tti={events:[],loafEvents:n?[]:void 0,eventsAfterTTI:[],fired:!1,observer:new PerformanceObserver((function(t){var o=t.getEntriesByType("longtask"),s=t.getEntriesByType("long-animation-frame");r.events=e(r.events,o),n&&(r.loafEvents=e(r.loafEvents,s)),r.fired&&(r.eventsAfterTTI=e(r.eventsAfterTTI,o))}))};r.observer.observe({entryTypes:t}),Ya.Rum._unsubscribers&&Ya.Rum._unsubscribers.push((function(){r.observer.disconnect()}))}r(),Ya.Rum._onInit.push(r)}}();
Ya.Rum.observeDOMNode=window.IntersectionObserver?function(e,i,n){var t=this,o=Ya.Rum.getSetting("forcePaintTimeSending");!function r(){if(o||!t.isVisibilityChanged()){var s="string"==typeof i?document.querySelector(i):i;s?new IntersectionObserver((function(i,n){!o&&t.isVisibilityChanged()||(Ya.Rum.sendTimeMark(e),n.unobserve(s))}),n).observe(s):setTimeout(r,100)}}()}:function(){};
var rum_platform = window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop';
    var rum_segment = window.location.pathname.replace(/^\//, "").replace(/\/.*/);
    
    if (["knowledge", "journal", "profile", "handbook"].indexOf(rum_segment) === -1) {
      rum_segment = "portal";
    }

    Ya.Rum.init({ beacon: true, clck: 'https://yandex.ru/clck/click', reqid: '1768064963563285-10555172254180365541'},
    {
        rum_id: 'ru.education.' + rum_platform + '.' + rum_segment,
        '-env': 'production',
        '-project': 'education-web',
        '-page': window.location.pathname,
        '-version': 'undefined',
        '-platform': rum_platform
    });Ya.Rum.observeDOMNode('2876', 'body');!function(){var e,t,n,i=Ya.Rum,o=42,r=4e4,g=15,a=[],s="\r\n",l=i.getSetting("countersInitialDelay")||0;if(l){var c;function u(){removeEventListener("visibilitychange",h),clearTimeout(c),l=0,f()}function h(){document.hidden&&u()}c=setTimeout(u,l),addEventListener("visibilitychange",h)}function f(){if(t&&a.length){for(var n=0,i=0,l=0;i<a.length&&l<=r&&n<o;i++)(l+=(i?s.length:0)+a[i].length)<=r&&n++;var c=a.splice(0,n);d(t,c.join(s)),a.length&&(e=setTimeout(f,g))}else a.length=0}function d(e,t){if(!(navigator.sendBeacon&&n&&navigator.sendBeacon(e,t))){var o=Boolean(i.getSetting("sendCookie")),r=new XMLHttpRequest;r.open("POST",e),r.withCredentials=o,r.send(t)}}i.send=function(c,u,h,d,v,m,S,p){t=i.getSetting("clck"),n=i.getSetting("beacon"),o=i.getSetting("maxBatchCounters")||o,r=i.getSetting("maxBatchLength")||r,g=i.getSetting("countersBatchTimeout")||g,function(t,n,i,c,u,h,d,v,m,S){clearTimeout(e);var p=[t?"/reqid="+t:"",n?"/"+n.join("/"):"",i?"/path="+i:"",c?"/events="+c:"",u?"/slots="+u.join(";"):"",h?"/experiments="+h.join(";"):"",d?"/vars="+d:"","/cts="+(new Date).getTime(),"","/*"].join("");p.length>r?"undefined"!=typeof console&&console.error&&console.error("Counter length "+p.length+" is more than allowed "+r,p):(a.push(p),l||(function(){if(a.length>=o)return!0;for(var e=0,t=0;t<a.length;t++)if((e+=(t?s.length:0)+a[t].length)>=r)return!0;return!1}()?f():e=setTimeout(f,g)))}(i.getSetting("reqid"),S,u,v,i.getSetting("slots"),i.getSetting("experiments"),h)}}();
!function(){var e=Ya.Rum,n=!window.BigInt||!("PerformanceObserver"in window);function t(n){e._unsubscribers.push(n)}function i(e,i,o){if(!n){var a=o||{};if(e){a.type=e,a.hasOwnProperty("buffered")||(a.buffered=!0);var s=new PerformanceObserver((function(e,n){return i(e.getEntries(),n)}));return r((function(){try{s.observe(a)}catch(e){return void console.error(e.message)}t((function(){s.disconnect()}))}),0),s}throw new Error("PO without type field is forbidden")}}function r(e,n){var i=setTimeout(e,n);return t((function(){clearInterval(i)})),i}function o(e,n,i){addEventListener(e,n,i),t((function(){removeEventListener(e,n,i)}))}function a(e,n,t){o("visibilitychange",(function i(){if("hidden"===document.visibilityState){try{t||(removeEventListener("visibilitychange",i),e.disconnect())}catch(e){}n()}})),o("beforeunload",n)}function s(e,n){return"string"==typeof e?encodeURIComponent(e):Math.round(1e3*(e-(n||0)))/1e3}function u(e){if(!e)return"";var n=(e.tagName||"").toLowerCase(),t=e.className&&void 0!==e.className.baseVal?e.className.baseVal:e.className;return n+(t?(" "+t).replace(/\s+/g,"."):"")}function c(e){function n(){removeEventListener("DOMContentLoaded",n),removeEventListener("load",n),e()}"loading"===document.readyState?(o("DOMContentLoaded",n),o("load",n)):e()}function d(n){e._onComplete.push(n)}function f(){return e._periodicTasks}function l(){var n=e._vars;return Object.keys(n).map((function(e){return e+"="+encodeURIComponent(n[e]).replace(/\*/g,"%2A")}))}var m={connectEnd:2116,connectStart:2114,decodedBodySize:2886,domComplete:2124,domContentLoadedEventEnd:2131,domContentLoadedEventStart:2123,domInteractive:2770,domLoading:2769,domainLookupEnd:2113,domainLookupStart:2112,duration:2136,encodedBodySize:2887,entryType:2888,fetchStart:2111,initiatorType:2889,loadEventEnd:2126,loadEventStart:2125,nextHopProtocol:2890,redirectCount:1385,redirectEnd:2110,redirectStart:2109,requestStart:2117,responseEnd:2120,responseStart:2119,secureConnectionStart:2115,startTime:2322,transferSize:2323,type:76,unloadEventEnd:2128,unloadEventStart:2127,workerStart:2137},v=625;function g(n,t){Object.keys(m).forEach((function(e){if(e in t){var i=t[e];(i||0===i)&&n.push(m[e]+"="+s(i))}})),n.push("".concat(v,"=").concat(e.version))}var p,h,y,S,T,b="690.2096.2877",w="690.2096.207",E="690.2096.2044",C=3,k=e.getSetting("savedDeltasLimit")||0,L=document.createElement("link"),P=window.performance||{},M="function"==typeof P.getEntriesByType,_=0;function O(n,t,i,r,o){void 0===t&&(t=e.getTime()),void 0!==i&&!0!==i||e.mark(n,t);var a=I(n);if(a.push("207="+s(t)),x(a,r)){j(w,a,o&&o.force),p[n]=p[n]||[],p[n].push(t);var u=e._markListeners[n];u&&u.length&&u.forEach((function(e){e(t)})),e.emit({metricName:n,value:t,params:r})}}function I(n){return T.concat([e.isVisibilityChanged()?"-vsChanged=1":"","1701="+n,e.ajaxStart&&"1201.2154="+s(e.ajaxStart),e.ajaxComplete&&"1201.2052="+s(e.ajaxComplete)])}function N(){S=l(),e.getSetting("sendClientUa")&&S.push("1042="+encodeURIComponent(navigator.userAgent))}function R(){var e=window.performance&&window.performance.timing&&window.performance.timing.navigationStart;T=S.concat(["143.2129="+e])}function x(e,n){if(n){if(n.isCanceled&&n.isCanceled())return!1;var t=e.reduce((function(e,n,t){return"string"==typeof n&&(e[n.split("=")[0]]=t),e}),{});Object.keys(n).forEach((function(i){if("function"!=typeof n[i]){var r=t[i],o=i+"="+n[i];void 0===r?e.push(o):e[r]=o}}))}return!0}function j(n,t,i){var r=encodeURIComponent(window.YaStaticRegion||"unknown");t.push("-cdn="+r);var o=t.filter(Boolean).join(",");e.send(null,n,o,void 0,void 0,void 0,void 0,i)}function z(e,n,t){j(e,F().concat(n),t)}function B(n,t){var i=y[n];i&&0!==i.length&&(i.push(e.getTime(),t),A(n))}function A(n,t,i,r){var o,a,u,c=y[n];if(void 0!==t?o=(a=r&&r.originalEndTime?r.originalEndTime:e.getTime())-t:c&&(o=c[0],a=c[1],u=c[2]),void 0!==o&&void 0!==a){var d=I(n);d.push("207.2154="+s(o),"207.1428="+s(a),"2877="+s(a-o)),x(d,i)&&x(d,u)&&(j(b,d,r&&r.force),_<k&&(h[n]=h[n]||[],h[n].push(a-o),_++),e.emit({metricName:n,value:a-o,params:{start:o,end:a}}),delete y[n])}}function V(e,n){if(!M)return n(null);L.href=e;var t=0,i=100,o=L.href;r((function e(){var a=P.getEntriesByName(o);if(a.length)return n(a);t++<C?(r(e,i),i+=i):n(null)}),0)}function U(e,n,t){V(n,(function(i){i&&D(e,i[i.length-1],n,t)}))}function D(n,t,i,r){var o=I(n);e.getSetting("sendUrlInResTiming")&&o.push("13="+encodeURIComponent(i)),g(o,t),x(o,r),j(E,o)}function F(){return S}var W={bluetooth:2064,cellular:2065,ethernet:2066,none:1229,wifi:2067,wimax:2068,other:861,unknown:836,0:836,1:2066,2:2067,3:2070,4:2071,5:2768},H=navigator.connection;function Q(e){if(H){var n=W[H.type];e.push("2437="+(n||2771),void 0!==H.downlinkMax&&"2439="+H.downlinkMax,H.effectiveType&&"2870="+H.effectiveType,void 0!==H.rtt&&"rtt="+H.rtt,void 0!==H.downlink&&"dwl="+H.downlink,!n&&"rawType="+H.type)}}var Y,q,G,J,$,K,X,Z,ee="690.2096.4004",ne=!1,te=1/0,ie=1/0,re=("layout-shift",Boolean(window.PerformanceObserver&&window.PerformanceObserver.supportedEntryTypes&&-1!==window.PerformanceObserver.supportedEntryTypes.indexOf("layout-shift"))?0:null);function oe(){$>q&&(q=$,G=J,e.emit({metricName:"cls-debug",value:q,params:{clsEntries:G,target:ae(G)}}))}function ae(e){var n;if(!e)return null;var t=null;if((n=e.reduce((function(e,n){return e&&e.value>n.value?e:n})))&&n.sources&&n.sources.length){for(var i=0;i<n.sources.length;i++){var r=n.sources[i];if(r.node&&1===r.node.nodeType){t=r;break}}t=t||n.sources[0]}return t}function se(e){null==q&&(q=0);for(var n=0;n<e.length;n++){var t=e[n];t.hadRecentInput||($&&t.startTime-J[J.length-1].startTime<te&&t.startTime-J[0].startTime<ie?($+=t.value,J.push(t)):(oe(),$=t.value,J=[t]))}oe()}function ue(){q=re,Y=void 0,G=null,J=null,$=null,ne=!1}function ce(n){if(null!=q&&!ne){var t=Math.round(1e6*q)/1e6;if(Y!==t){Y=t,e.getSetting("enableContinuousCollection")||(ne=!0);var i=ae(G),r=["s="+t];r.push("target="+u(i&&i.node));var o=e.getCLSAdditionalParams(i);o&&x(r,o),z(ee,r,n),e.emit({metricName:"cls-debug",value:q,params:{clsEntries:J,target:i,isFinalized:ne}})}}}function de(n){var t=n[n.length-1];K=t.renderTime||t.loadTime,X=t,e.emit({metricName:"largest-contentful-paint-debug",value:K,params:{entry:t}}),Z||(O("largest-loading-elem-paint",K),Z=!0)}function fe(n){if(null!=K){var t=e.getLCPAdditionalParams(X);O("largest-contentful-paint",K,!1,t,n&&{force:!0}),e.emit({metricName:"largest-contentful-paint-debug",value:K,params:{additionalParams:t,entry:X,isFinalized:!0}}),K=null,X=null}}e.getLCPAdditionalParams===e.noop&&(e.getLCPAdditionalParams=function(){var n={},t=X.element;if(t){n["-className"]=e.getSelector(t),n["-tagName"]=t.tagName.toLowerCase();var i=t.getBoundingClientRect();n["-width"]=i.width,n["-height"]=i.height}return X.size&&(n["-size"]=X.size),n});var le={"first-paint":2793,"first-contentful-paint":2794},me=Object.keys(le).length,ve={},ge=window.performance||{},pe="function"==typeof ge.getEntriesByType,he=0;function ye(){if(pe&&(e.getSetting("forcePaintTimeSending")||!e.isVisibilityChanged()))for(var n=ge.getEntriesByType("paint"),t=0;t<n.length;t++){var i=n[t],r=le[i.name];r&&!ve[i.name]&&(ve[i.name]=!0,he++,O("1926."+r,i.startTime))}}var Se=3e3,Te=1;function be(){return e._tti.events||[]}function we(){return e._tti.loafEvents}function Ee(){return e._tti}function Ce(n){return n?n===e.getPageUrl()?"<page>":n.replace(/\?.*$/,""):n}function ke(n,t,i){if(Ee()){var r=e.getTime(),o="undefined"!=typeof PerformanceLongAnimationFrameTiming&&e.getSetting("sendLongAnimationFrames");Le((function(a){var u,c={2796.2797:Pe(be(),t),689.2322:s(r)};if(o){var d=function(e){var n=we();if(n)return e?n.filter((function(n){return n.startTime+n.duration>=e})):n}(t);d&&(c["loaf.2797"]=Pe(d,void 0,{useName:!1}),1===e.getSetting("longAnimationFramesMode")&&(c["-additional"]=encodeURIComponent(JSON.stringify({loaf:(u=d,u.map(Me))}))))}i&&Object.keys(i).forEach((function(e){c[e]=i[e]})),O(n||"2795",a,!0,c,{force:Boolean(o)}),e._tti.fired=!0}),t)}}function Le(n,t){var i=(arguments.length>2&&void 0!==arguments[2]?arguments[2]:{}).mode,o=void 0===i?Te:i;Ee()&&(t||(t=e.getTime()),function i(){var a,s=t,u=e.getTime(),c=o===Te?be():we()||[],d=c.length;0!==d&&(a=c[d-1],s=Math.max(s,Math.floor(a.startTime+a.duration))),u-s>=Se?n(s):r(i,1e3)}())}function Pe(e,n){var t=(arguments.length>2&&void 0!==arguments[2]?arguments[2]:{}).useName,i=void 0===t||t;return n=n||0,(e=e||[]).filter((function(e){return n-e.startTime<=50})).map((function(e){var n=Math.floor(e.startTime),t=Math.floor(n+e.duration);return i?(e.name?e.name.split("-").map((function(e){return e[0]})).join(""):"u")+"-"+n+"-"+t:n+"-"+t})).join(".")}function Me(e){var n=e.blockingDuration,t=e.duration,i=e.firstUIEventTimestamp,r=e.renderStart,o=e.scripts,a=e.startTime,s=e.styleAndLayoutStart;return[Math.round(a),Math.round(t),o.map(Oe),Math.round(n),Math.round(i),Math.round(r),Math.round(s)]}function _e(e){return{"user-callback":1,"event-listener":2,"resolve-promise":3,"reject-promise":4,"classic-script":5,"module-script":6}[e]||0}function Oe(e){var n=e.invoker,t=e.sourceURL,i=e.sourceFunctionName,r=e.sourceCharPosition,o=e.startTime,a=e.duration,s=e.windowAttribution,u=e.executionStart,c=e.forcedStyleAndLayoutDuration,d=e.pauseDuration,f=e.invokerType;return[Ce(n),Ce(t),i,r,Math.round(o),Math.round(a),s,Math.round(u),Math.round(c),Math.round(d),_e(f)]}var Ie="690.2096.361",Ne=document.createElement("a"),Re=0,xe={};function je(e){var n=e.transferSize;if(null!=n){Ne.href=e.name;var t=Ne.pathname;if(0!==t.indexOf("/clck")){var i=t.lastIndexOf("."),r="";return-1!==i&&t.lastIndexOf("/")<i&&t.length-i<=5&&(r=t.slice(i+1)),{size:n,domain:Ne.hostname,extension:r}}}}function ze(){var n=e.getSetting("maxTrafficCounters")||250;if(Re>=n)return!1;for(var t=Object.keys(xe),i="",r=0;r<t.length;r++){var o=t[r],a=xe[o];i+=encodeURIComponent(o)+"!"+a.count+"!"+a.size+";"}return i.length&&(Re++,z(Ie,["d="+i,"t="+s(e.getTime())])),xe={},Re<n}d(ze);var Be="690.1033",Ae={visible:1,hidden:2,prerender:3},Ve=window.performance||{},Ue=Ve.navigation||{},De=Ve.timing||{},Fe=De.navigationStart;function We(){var n=De.domContentLoadedEventStart,t=De.domContentLoadedEventEnd;if(0!==n||0!==t){var i=0===De.responseStart?Fe:De.responseStart,o=0===De.domainLookupStart?Fe:De.domainLookupStart,a=["2129="+Fe,"1036="+(o-Fe),"1037="+(De.domainLookupEnd-De.domainLookupStart),"1038="+(De.connectEnd-De.connectStart),De.secureConnectionStart&&"1383="+(De.connectEnd-De.secureConnectionStart),"1039="+(De.responseStart-De.connectEnd),"1040="+(De.responseEnd-i),"1040.906="+(De.responseEnd-o),"1310.2084="+(De.domLoading-i),"1310.2085="+(De.domInteractive-i),"1310.1309="+(t-n),"1310.1007="+(n-i),navigator.deviceMemory&&"3140="+navigator.deviceMemory,navigator.hardwareConcurrency&&"3141="+navigator.hardwareConcurrency];Object.keys(m).forEach((function(e){e in De&&De[e]&&a.push(m[e]+"="+s(De[e],Fe))})),e.vsStart?(a.push("1484="+(Ae[e.vsStart]||2771)),e.vsChanged&&a.push("1484.719=1")):a.push("1484="+Ae.visible),Ue&&(Ue.redirectCount&&a.push("1384.1385="+Ue.redirectCount),1!==Ue.type&&2!==Ue.type||a.push("770.76="+Ue.type)),Q(a),z(Be,a)}else r(We,50)}var He="690.2096.2892",Qe=window.performance||{},Ye="function"==typeof Qe.getEntriesByType;if(!e)throw new Error("Rum: interface is not included");if(e.enabled){function qe(){p={},h={},_=0,y=e._deltaMarks,N(),R(),e.ajaxStart=0,e.ajaxComplete=0,c(Je)}function Ge(){var n;e.sendTimeMark=O,e.sendResTiming=U,e.sendTiming=D,e.timeEnd=B;var t=(e.getBufferedEvents(["defRes"]).defRes||[]).map((function(e){return e.data}));for(n=0;n<t.length;n++)U(t[n][0],t[n][1]);e.clearEvents("defRes");var i=(e.getBufferedEvents(["defTimes"]).defTimes||[]).map((function(e){return e.data}));for(n=0;n<i.length;n++)O(i[n][0],i[n][1],!1,i[n][2]);e.clearEvents("defTimes"),Object.keys(y).forEach((function(e){A(e)}))}function Je(){var n=window.performance&&window.performance.timing&&window.performance.timing.navigationStart,t=e.getSetting("skipTiming"),a=e.getSetting("techParamsByVisible");n&&(a&&addEventListener("visibilitychange",(function n(){"visible"!==e.vsStart?"visible"===document.visibilityState&&(e.vsStart="visible",removeEventListener("visibilitychange",n),We()):removeEventListener("visibilitychange",n)})),r((function(){Ge(),(!t&&!a||a&&"visible"===e.vsStart)&&We(),e.getSetting("disableFCP")||(ye(),he<me&&i("paint",(function(e,n){ye(),n&&he>=me&&n.disconnect()}),{buffered:!0})),e.getSetting("sendAutoElementTiming")&&(!window.PerformanceObserver||!e.getSetting("forcePaintTimeSending")&&e.isVisibilityChanged()||i("element",(function(e){for(var n=0;n<e.length;n++){var t=e[n];O("element-timing."+t.identifier,t.startTime)}}))),o("pageshow",Xe),ke(),"complete"===document.readyState?$e({skipTimingApi:t}):o("load",$e.bind(void 0,{skipTimingApi:t}))}),0))}function $e(n){var r,s;e.getSetting("disableOnLoadTasks")||(removeEventListener("load",$e),n.skipTimingApi||function(){if(Ye){var e=Qe.getEntriesByType("navigation")[0];if(e){var n=[];g(n,e),Q(n);var t=Qe.getEntriesByName("yndxNavigationSource")[0];t&&n.push("2091.186="+t.value);var i=Qe.getEntriesByName("yndxNavigationToken","yndxEntry")[0];i&&n.push("2091.3649="+i.value),z(He,n)}}}(),(s=e.getSetting("periodicStatsIntervalMs"))||null===s||(s=15e3),s&&(r=setInterval(Ze,s),t((function(){clearInterval(r)})),Ke=r),o("beforeunload",Ze),function(){if(window.PerformanceObserver){xe={},Re=0;var e=function(e){!function(e){if(e&&e.length)for(var n=xe,t=0;t<e.length;t++){var i=je(e[t]);if(i){var r=i.domain+"-"+i.extension,o=n[r]=n[r]||{count:0,size:0};o.count++,o.size+=i.size}}}(e)};i("resource",e),i("navigation",e),f().push(ze)}}(),e.getSetting("disableFID")||i("first-input",(function(n,t){var i=n[0];if(i){var r=i.processingStart,o={duration:i.duration,js:i.processingEnd-r,name:i.name};i.target&&(o.target=u(i.target));var a=r-i.startTime;A("first-input",a,o),e.emit({metricName:"first-input-debug",value:a,params:{entry:i,additionalParams:o}}),t.disconnect()}}),{buffered:!0}),e.getSetting("disableCLS")||window.PerformanceObserver&&(d(ce),d(ue),q=re,G=null,J=null,$=null,te=e.getSetting("clsWindowGap")||te,ie=e.getSetting("clsWindowSize")||ie,a(i("layout-shift",se),(function(){return ce(!0)}),!0)),e.getSetting("disableLCP")||!window.PerformanceObserver||!e.getSetting("forcePaintTimeSending")&&e.isVisibilityChanged()||(d(fe),K=null,X=null,Z=!1,a(i("largest-contentful-paint",de),(function(){return fe(!0)}),!1)))}var Ke;function Xe(e){e.persisted&&O("bfcache")}function Ze(){var e=!1;f().forEach((function(n){n()&&(e=!0)})),e||clearInterval(Ke)}d(Ge),e.destroy=function(n){var t=e._unsubscribers;n.shouldComplete&&e.completeSession(!0),e._onComplete=[];for(var i=0;i<t.length;i++)t[i]();removeEventListener("visibilitychange",e._onVisibilityChange),e._unsubscribers=[],e._periodicTasks=[],e._markListeners={},e._deltaMarks={}},e.restart=function(n,t,i){e.destroy({shouldComplete:i}),e.init(n,t),addEventListener("visibilitychange",e._onVisibilityChange),qe(),function(){for(var n=0;n<e._onInit.length;n++)e._onInit[n]()}()},e.setVars=function(n){Object.keys(n).forEach((function(t){e._vars[t]=n[t]})),N(),R()},e.completeSession=function(n){for(var t=e._onComplete,i=0;i<t.length;i++)t[i](n)},qe(),e._periodicTasks=[],e.sendHeroElement=function(e){O("2876",e)},e.getPageUrl=function(){return window.location.href},e._subpages={},e.makeSubPage=function(n,t){var i=e._subpages[n];e._subpages[n]=void 0===i?i=0:++i;var r=!1;return{689.2322:s(void 0!==t?t:e.getTime()),2924:n,2925:i,isCanceled:function(){return r},cancel:function(){r=!0}}},e.getTimeMarks=function(){return p},e.getDeltas=function(){return h},e.getVarsList=l,e.getResourceTimings=V,e.pushConnectionTypeTo=Q,e.pushTimingTo=g,e.normalize=s,e.sendCounter=j,e.sendDelta=A,e.onReady=c,e.getSelector=u,e.getSetting("disableCLS")||(e.finalizeLayoutShiftScore=ce),e.getSetting("disableLCP")||(e.finalizeLargestContentfulPaint=fe),e.sendTrafficData=ze,e._getCommonVars=F,e._addListener=o,e._observe=i,e._timeout=r,e.sendTTI=ke,e._getLongtasksStringValue=Pe,e.onQuietWindow=Le,e.sendBFCacheTimeMark=Xe}else e.getSetting=function(){return""},e.getVarsList=function(){return[]},e.getResourceTimings=e.completeSession=e.pushConnectionTypeTo=e.pushTimingTo=e.normalize=e.sendCounter=e.destroy=e.restart=e.setVars=e.completeSession=e.sendDelta=e.sendTimeMark=e.sendResTiming=e.sendTiming=e.sendTTI=e.makeSubPage=e.sendHeroElement=e.onReady=e.onQuietWindow=function(){}}();
!function(n){if(!n.Ya||!Ya.Rum)throw new Error("Rum: interface is not defined");var e=Ya.Rum;e.getSetting=function(n){var t=e._settings[n];return null===t?null:t||""}}("undefined"!=typeof self?self:window);
!function(e,r){var n={client:["690.2354",1e3,100,0],uncaught:["690.2361",100,10,0],external:["690.2854",100,10,0],script:["690.2609",100,10,0]},t={};r.ERROR_LEVEL={INFO:"info",DEBUG:"debug",WARN:"warn",ERROR:"error",FATAL:"fatal"},r._errorSettings={clck:"https://yandex.ru/clck/click",beacon:!0,project:"unknown",page:"",env:"",experiments:[],additional:{},platform:"",region:"",dc:"",host:"",service:"",level:"",version:"",yandexuid:"",loggedin:!1,coordinates_gp:"",referrer:!0,preventError:!1,unhandledRejection:!1,traceUnhandledRejection:!1,uncaughtException:!0,debug:!1,limits:{},silent:{},filters:{},pageMaxAge:864e6,initTimestamp:+new Date};var o=!1;function a(e,r){for(var n in r)r.hasOwnProperty(n)&&(e[n]=r[n]);return e}function i(e){return"boolean"==typeof e&&(e=+e),"number"==typeof e?e+"":null}r.initErrors=function(n){var t=a(r._errorSettings,n);o||(t.uncaughtException&&function(){var n=r._errorSettings;if(e.addEventListener)e.addEventListener("error",s),n.resourceFails&&e.addEventListener("error",l,!0),"Promise"in e&&n.unhandledRejection&&e.addEventListener("unhandledrejection",function(e){var n,t,o=e.reason,a={};o&&(o.stack&&o.message?(n=o.message,t=o.stack):(n=String(o),t=r._parseTraceablePromiseStack(e.promise),"[object Event]"===n?n="event.type: "+o.type:"[object Object]"===n&&(a.unhandledObject=o)),o.target&&o.target.src&&(a.src=o.target.src),s({message:"Unhandled rejection: "+n,stack:t,additional:a}))});else{var t=e.onerror;e.onerror=function(e,r,n,o,a){s({error:a||new Error(e||"Empty error"),message:e,lineno:n,colno:o,filename:r}),t&&t.apply(this,arguments)}}}(),t.unhandledRejection&&t.traceUnhandledRejection&&r._traceUnhandledRejection&&r._traceUnhandledRejection(),o=!0)},r.updateErrors=function(e){a(r._errorSettings,e)},r.updateAdditional=function(e){r._errorSettings.additional=a(r._errorSettings.additional||{},e)},r._handleError=function(e,o,i){var s,l,c=r._errorSettings;if(c.preventError&&e.preventDefault&&e.preventDefault(),o)s=e,l="client";else{s=r._normalizeError(e),l=s.type;var d=c.onError;"function"==typeof d&&d(s);var u=c.transform;if("function"==typeof u&&(s=u(s)),!s)return;s.settings&&(i=s.settings)}var g=+new Date,f=c.initTimestamp,p=c.pageMaxAge;if(!(-1!==p&&f&&f+p<g)){var m=n[l][1];"number"==typeof c.limits[l]&&(m=c.limits[l]);var v=n[l][2];"number"==typeof c.silent[l]&&(v=c.silent[l]);var h=n[l][3];if(h<m||-1===m){s.path=n[l][0];var E=r._getErrorData(s,{silent:h<v||-1===v?"no":"yes",isCustom:Boolean(o)},a(a({},c),i)),_=function(e){t[s.message]=!1,r._sendError(e.path,e.vars),n[l][3]++}.bind(this,E);if(void 0===c.throttleSend)_();else{if(t[s.message])return;t[s.message]=!0,setTimeout(_,c.throttleSend)}}}},r._getReferrer=function(r){var n=r.referrer,t=typeof n;return"function"===t?n():"string"===t&&n?n:!1!==n&&e.location?e.location.href:void 0},r.getErrorSetting=function(e){return r._errorSettings[e]},r._buildExperiments=function(e){return e instanceof Array?e.join(";"):""},r._buildAdditional=function(e,r){var n="";try{var t=a(a({},e),r);0!==Object.keys(t).length&&(n=JSON.stringify(t))}catch(e){}return n},r._getErrorData=function(n,t,o){t=t||{};var a=r._buildExperiments(o.experiments),s=r._buildAdditional(o.additional,n.additional),l={"-stack":n.stack,"-url":n.file,"-line":n.line,"-col":n.col,"-block":n.block,"-method":n.method,"-msg":n.message,"-env":o.env,"-external":n.external,"-externalCustom":n.externalCustom,"-project":o.project,"-service":n.service||o.service,"-page":n.page||o.page,"-platform":o.platform,"-level":n.level,"-experiments":a,"-version":o.version,"-region":o.region,"-dc":o.dc,"-host":o.host,"-yandexuid":o.yandexuid,"-loggedin":o.loggedin,"-coordinates_gp":n.coordinates_gp||o.coordinates_gp,"-referrer":r._getReferrer(o),"-source":n.source,"-sourceMethod":n.sourceMethod,"-type":t.isCustom?n.type:"","-additional":s,"-adb":i(Ya.blocker)||i(o.blocker),"-cdn":e.YaStaticRegion,"-ua":navigator.userAgent,"-silent":t.silent,"-ts":+new Date,"-init-ts":o.initTimestamp};return o.debug&&e.console&&console[console[n.level]?n.level:"error"]("[error-counter] "+n.message,l,n.stack),{path:n.path,vars:l}},r._baseNormalizeError=function(e){var r=(e=e||{}).error,n=e.filename||e.fileName||"",t=r&&r.stack||e.stack||"",o=e.message||"",a=r&&r.additional||e.additional;return{file:n,line:e.lineno||e.lineNumber,col:e.colno||e.colNumber,stack:t,message:o,additional:a}},r._normalizeError=function(e){var n=r._baseNormalizeError(e),t="uncaught",o=r._isExternalError(n.file,n.message,n.stack),a="",i="";return o.hasExternal?(t="external",a=o.common,i=o.custom):/^Script error\.?$/.test(n.message)&&(t="script"),n.external=a,n.externalCustom=i,n.type=t,n},r._createVarsString=function(e){var r=[];for(var n in e)e.hasOwnProperty(n)&&(e[n]||0===e[n])&&r.push(n+"="+encodeURIComponent(e[n]).replace(/\*/g,"%2A"));return r.join(",")},r._sendError=function(e,n){r.send(null,e,r._createVarsString(n),null,null,null,null)};var s=function(e){r._handleError(e,!1)},l=function(e){var n=e.target;if(n){var t=n.srcset||n.src;if(t||(t=n.href),t){var o=n.tagName||"UNKNOWN";r.logError({message:o+" load error",additional:{src:t}})}}};r._parseTraceablePromiseStack=function(){}}("undefined"!=typeof self?self:window,Ya.Rum);
!function(e){var r={url:{0:/(miscellaneous|extension)_bindings/,1:/^chrome:/,2:/kaspersky-labs\.com\//,3:/^(?:moz|chrome|safari)-extension:\/\//,4:/^file:/,5:/^resource:\/\//,6:/webnetc\.top/,7:/local\.adguard\.com/},message:{0:/__adgRemoveDirect/,1:/Content Security Policy/,2:/vid_mate_check/,3:/ucapi/,4:/Access is denied/i,5:/^Uncaught SecurityError/i,6:/__ybro/,7:/__show__deepen/,8:/ntp is not defined/,9:/Cannot set property 'install' of undefined/,10:/NS_ERROR/,11:/Error loading script/,12:/^TypeError: undefined is not a function$/,13:/__firefox__\.(?:favicons|metadata|reader|searchQueryForField|searchLoginField)/},stack:{0:/(?:moz|chrome|safari)-extension:\/\//,1:/adguard.*\.user\.js/i}};function n(e,r){if(e&&r){var n=[];for(var o in r)if(r.hasOwnProperty(o)){var i=r[o];"string"==typeof i&&(i=new RegExp(i)),i instanceof RegExp&&i.test(e)&&n.push(o)}return n.join("_")}}function o(e,o){var i,a=[];for(var t in r)r.hasOwnProperty(t)&&(i=n(e[t],o[t]))&&a.push(t+"~"+i);return a.join(";")}e._isExternalError=function(n,i,a){var t=e._errorSettings.filters||{},s={url:(n||"")+"",message:(i||"")+"",stack:(a||"")+""},c=o(s,r),u=o(s,t);return{common:c,custom:u,hasExternal:!(!c&&!u)}}}(Ya.Rum);
Ya.Rum.initErrors({
        reqid: '1768064963563285-10555172254180365541',
        project: 'education-web',
        env: 'production',
        page: window.location.pathname,
        version: 'undefined',
        platform: window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop'
    });</script><style></style><style></style><script nonce="" id="yandex-metrika">
  (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
  m[i].l=1*new Date();
  for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
  k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
  (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");
</script><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/0f6801932ea3fcf4-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/305b936a915bc48f-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/3fdc59da94114ecd-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/a853c69d3cf13b17-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/b4b0da158404816f-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/c8ae0fac15b37b16-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/cc87cb16fedd6384-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/dd32e121f6104240-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/e10f0a1f1c5bddfe-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/f2f0493f5123f937-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"><link rel="preload" href="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/f3f9c83d0bcb2176-s.p.woff2" as="font" crossorigin="" nonce="" type="font/woff2"></head><body class="utilityfocus HR-9-31-0 Theme Theme_color_hrLight Theme_root_hrLight Theme_controls_size_m styles_with-sticky-drawer__bAg_p" style="overflow: initial;"><style>.gdpr-popup-v3-hidden{display:none;}
.gdpr-popup-v3-button{background-color:#FFDD57;color:#2F3747;padding:8px 16px 8px 16px;border-radius:12px;border:none;cursor:pointer;display:inline-block;}.gdpr-popup-v3-button_theme_dark{background-color:rgba(176,189,214,0.2);color:white;}.gdpr-popup-v3-button_theme_transparent{background-color:transparent;color:white;border:#4C5264 solid 1px;}@media (max-width:576px){.gdpr-popup-v3-button{display:block;text-align:center;padding:1.5vh;border-radius:1.5vh;}}
.gdpr-popup-v3-checkbox{width:36px;height:20px;border-radius:18px;background-color:rgba(165,177,202,0.3);display:flex;cursor:pointer;}.gdpr-popup-v3-checkbox__check{width:16px;height:16px;border-radius:8px;background-color:white;margin:auto;transition:margin ease 1s;}.gdpr-popup-v3-checkbox_checked_yes{background-color:#FFDD57;}.gdpr-popup-v3-checkbox_disabled_yes{background-color:rgba(176,189,214,0.15);}.gdpr-popup-v3-checkbox_disabled_yes .gdpr-popup-v3-checkbox__check{background:rgba(255,255,255,0.5);}.gdpr-popup-v3-checkbox_checked_yes .gdpr-popup-v3-checkbox__check{float:right;margin-right:2px;}.gdpr-popup-v3-checkbox_checked_no .gdpr-popup-v3-checkbox__check{margin-left:2px;}@media (max-width:576px){.gdpr-popup-v3-checkbox{height:5vw;border-radius:2.5vw;width:10vw;}.gdpr-popup-v3-checkbox__check{height:calc(5vw - 4px);width:calc(5vw - 4px);border-radius:calc(2.5vw - 2px);}}
.gdpr-popup-v3-svg{margin:auto;}
.gdpr-popup-v3-spoiler{border-bottom:solid 1px #5F697F;}.gdpr-popup-v3-spoiler__header{display:flex;padding-top:12px;padding-bottom:12px;}.gdpr-popup-v3-spoiler__opener{margin:auto 8px auto 0;cursor:pointer;display:flex;min-height:14px;min-width:14px;}.gdpr-popup-v3-spoiler__opener{background-color:#FFDD57\0/;width:20px\0/;}.gdpr-popup-v3-spoiler__opener .gdpr-popup-v3-svg{margin-top:auto;margin-bottom:auto;}.gdpr-popup-v3-spoiler__label{color:white;font-weight:500;font-size:16px;line-height:24px;margin:0;}.gdpr-popup-v3-spoiler__checkbox-container{margin:auto 0 auto auto;}.gdpr-popup-v3-spoiler__content{margin-bottom:8px;padding-left:30px;}.gdpr-popup-v3-spoiler__description{margin-bottom:16px;line-height:20px;}@media (max-width:576px){.gdpr-popup-v3-spoiler{border-bottom:solid 1px #5F697F;}.gdpr-popup-v3-spoiler__header{height:9vw;padding-top:2vw;padding-bottom:2vw;}.gdpr-popup-v3-spoiler__opener{margin:auto 2vw auto 0;}.gdpr-popup-v3-spoiler__label{font-size:14px;margin-top:auto;margin-bottom:auto;}.gdpr-popup-v3-spoiler__content{margin-bottom:3vw;padding-left:7vw;}.gdpr-popup-v3-spoiler__description{margin-bottom:4vw;}}
.gdpr-popup-v3-main{background-color:#252A38;width:460px;padding:20px;color:#BEBFC3;font-family:'YS Text',Arial,Helvetica,"Arial Unicode MS",sans-serif;font-style:normal;font-size:14px;position:fixed;bottom:30px;right:20px;border-radius:14px;max-height:458px;overflow-y:scroll;z-index:10000;-webkit-text-size-adjust:100%;}.gdpr-popup-v3-main__main-description{line-height:20px;}.gdpr-popup-v3-main::-webkit-scrollbar{display:none;}.gdpr-popup-v3-main__spoilers{margin-top:8px;border-top:solid 1px #5F697F;}.gdpr-popup-v3-main__title{color:white;font-weight:500;font-size:16px;line-height:24px;margin-top:0;}.gdpr-popup-v3-main__buttons{margin-top:16px;display:flex;}.gdpr-popup-v3-main__buttons .gdpr-popup-v3-button{margin-right:8px;}.gdpr-popup-v3-main__buttons .gdpr-popup-v3-button_id_back{padding:8px 10px 8px 10px;font-size:18px;display:flex;}.gdpr-popup-v3-button_id_mobile-back{height:1em;display:none;}.gdpr-popup-v3-main__settings-description{line-height:20px;}.gdpr-popup-v3-main a{color:#FFD21F;text-decoration:none;cursor:pointer;}.gdpr-popup-v3-main a:visited{color:#FFD21F;text-decoration:none;}@media (max-width:576px){.gdpr-popup-v3-main{padding:2.3vh;width:calc(100% - 6.6vh);max-height:80vh;font-size:14px;border-radius:1.5vh;right:1vh;}.gdpr-popup-v3-main__spoilers{margin-top:2vw;border-top:solid 1px #5F697F;}.gdpr-popup-v3-main__title{font-size:16px;line-height:inherit;margin-bottom:0;}.gdpr-popup-v3-main__title span{margin:auto auto auto 16px;}.gdpr-popup-v3-main__title .gdpr-popup-v3-button{padding:8px;float:left;}.gdpr-popup-v3-main__buttons{margin-top:4vw;display:block;}.gdpr-popup-v3-main__buttons .gdpr-popup-v3-button{display:block;margin-right:0;margin-top:8px;}.gdpr-popup-v3-main__buttons .gdpr-popup-v3-button_id_back{display:none;}.gdpr-popup-v3-button_id_mobile-back{display:block;margin-right:10px;}.gdpr-popup-v3-main__settings-title{margin-bottom:8px;text-align:center;display:flex;align-items:center;}}@media (orientation:landscape) and (max-height:450px){.gdpr-popup-v3-main{max-height:70vh;}}</style><div hidden=""><!--$--><!--/$--></div><script id="hasOwnPolyfill" nonce="">('hasOwn' in Object) || (Object.hasOwn = Object.call.bind(Object.hasOwnProperty));</script><script id="UserAGentUADataPolyfill" nonce="">
  function applyUADataPolyfill(){
    function e(a){let b=/Windows NT (\d+(\.\d+)*)/.exec(a);if(!b)return null;let c={"6.1":"0.1","6.2":"0.2","6.3":"0.3","10.0":"10.0","11.0":"13.0"}[b[1]],d=c?q(c,3):"",e="",f="";return/\b(WOW64|Win64|x64)\b/.test(a)&&(e="x86",f="64"),{platform:"Windows",platformVersion:d,architecture:e,bitness:f}}
    function f(a,b){let c=/Android (\d+(\.\d+)*)/.exec(a);if(!c)return null;let d="",e="",f=/Linux (\w+)/.exec(b);if(f&&f[1]){let a=p(f[1]);d=a[0],e=a[1]}return{platform:"Android",platformVersion:q(c[1]),architecture:d,bitness:e}}
    function g(a){let b=/(iPhone|iPod touch); CPU iPhone OS (\d+(_\d+)*)/.exec(a),c=/(iPad); CPU OS (\d+(_\d+)*)/.exec(a),d=b||c;return d?{platform:"iOS",platformVersion:q(d[2].replace(/_/g,"."))}:null}
    function h(a){let b=/Macintosh; (Intel|\w+) Mac OS X (\d+([_.]\d+)*)/.exec(a);return b?{platform:"macOS",platformVersion:q(b[2].replace(/_/g,"."))}:null}
    function i(a){let b=/CrOS (\w+) (\d+(\.\d+)*)/.exec(a);if(!b)return null;let c=p(b[1]);return{platform:"Chrome OS",platformVersion:q(b[2]),architecture:c[0],bitness:c[1]}}
    function j(a,b){for(let c of[()=>e(a),()=>f(a,b),()=>g(a),()=>h(a),()=>i(a)]){let a=c();if(a)return a}return/Linux/.test(a)?{platform:"Linux",platformVersion:""}:{platform:"Unknown",platformVersion:""}}
    function k(a,b,c){let d=/Chrome\/(\d+(\.\d+)*)/.exec(a);if(!d||"Google Inc."!==c)return null;let e=[{brand:"Chromium",version:q(d[1],4)}],f=/(Edge?)\/(\d+(\.\d+)*)/.exec(a);if(f){let a={Edge:"Microsoft Edge",Edg:"Microsoft Edge"}[f[1]];e.push({brand:a,version:q(f[2],4)})}else e.push({brand:"Google Chrome",version:q(d[1],4)});return e}
    function l(a,b){let c=/AppleWebKit\/(\d+(\.\d+)*)/.exec(a);return c&&"Apple Computer, Inc."===b?[{brand:"WebKit",version:q(c[1])}]:null}
    function m(a){let b=/Firefox\/(\d+(\.\d+)*)/.exec(a);return b?[{brand:"Firefox",version:q(b[1])}]:null}
    function n(a,b,c,d,e){let f=!1,g=[];for(let h of[()=>k(a,b,c),()=>l(a,c),()=>m(a)]){let c=h();if(c){g=c,h===k&&/\bwv\b/.test(b)&&(f=!0);let i=/(CriOS|EdgiOS|FxiOS|Version)\/(\d+(\.\d+)*)/.exec(a);if(h===l&&"iOS"===d&&i){let a={CriOS:"Google Chrome",EdgiOS:"Microsoft Edge",FxiOS:"Mozilla Firefox",Version:"Apple Safari"}[i[1]];g.push({brand:a,version:q(i[2])}),e&&!e.some(a=>a.startsWith("Safari/"))&&(f=!0)}break}}return 0===g.length&&(g=[{brand:"Not;A Brand",version:"99.0.0.0"}]),{fullVersionList:g,webview:f}}
    function o(a){let{userAgent:b,platform:c,vendor:d}=a,e=b,f=!1,g=b.replace(/\(([^)]+)\)/g,(a,b)=>(f||(e=b,f=!0),"")),h=g.match(/(\S+)\/(\S+)/g),i=b.includes("Mobile"),k=j(e,c),{fullVersionList:l,webview:m}=n(g,e,d,k.platform,h),o=l.length>0?l[l.length-1].version:"",p=l.map(a=>{let b=a.version.indexOf("."),c=-1===b?a.version:a.version.slice(0,b);return{brand:a.brand,version:c}});return{mobile:i,platform:k.platform,brands:p,platformVersion:k.platformVersion,architecture:k.architecture||"",bitness:k.bitness||"",model:"",uaFullVersion:o,fullVersionList:l,webview:m}}
    function p(a){switch(a){case"x86_64":case"x64":return["x86","64"];case"x86_32":case"x86":return["x86",""];case"armv6l":case"armv7l":case"armv8l":return[a,""];case"aarch64":return["arm","64"];default:return["",""]}}
    function q(a,b=3){let c=a.split(".");if(c.length<b)for(;c.length<b;)c.push("0");return c.join(".")}
    class r{constructor(a){this._clientHints=o(a),Object.defineProperties(this,{_clientHints:{enumerable:!1}})}get mobile(){return this._clientHints.mobile}get platform(){return this._clientHints.platform}get brands(){return this._clientHints.brands}getHighEntropyValues(a){return new Promise(b=>{if(!Array.isArray(a))throw TypeError("Argument hints must be an array");let c=new Set(a),d=this._clientHints,e={mobile:d.mobile,platform:d.platform,brands:d.brands};c.has("architecture")&&(e.architecture=d.architecture),c.has("bitness")&&(e.bitness=d.bitness),c.has("model")&&(e.model=d.model),c.has("platformVersion")&&(e.platformVersion=d.platformVersion),c.has("uaFullVersion")&&(e.uaFullVersion=d.uaFullVersion),c.has("fullVersionList")&&(e.fullVersionList=d.fullVersionList),b(e)})}toJSON(){return{mobile:this._clientHints.mobile,brands:this._clientHints.brands}}}
    function(){if("https:"===location.protocol&&!navigator.userAgentData){Object.defineProperty(r.prototype,Symbol.toStringTag,{enumerable:!1,configurable:!0,writable:!1,value:"NavigatorUAData"});let a=new r(navigator);return Object.defineProperty(Navigator.prototype,"userAgentData",{enumerable:!0,configurable:!0,get:function(){return a}}),Object.defineProperty(window,"NavigatorUAData",{enumerable:!1,configurable:!0,writable:!0,value:r}),!0}return!1}
    polyfill();
  }
  applyUADataPolyfill();
</script><div class="styles_container__LRzqX"><div class="Toastify" id="defaultToastContainer"></div></div><div class="styles_container__LRzqX"><div class="Toastify" id="noCloseButtonToastContainer"></div></div><header class="styles_header__cN9zm styles_noContentWidth__pFPRk"><div class="styles_header__inset__mXfjd"><div class="styles_header__backdrop__ZkyIx"></div><section class="styles_root__IBJUG styles_header__container__J_Yo5" style="--gutter-size-xs:50px;--gutter-size-sm:50px;--gutter-size-lg:50px;--gutter-size-xl:50px"><div class="styles_header__row__nqo4h"><div class="styles_header__start__OHmLc"><div class="styles_header__logo__TIaUe"><div class="styles_logo-header_primary__Ezzwg"><a aria-label="Яндекс Образование" class="styles_logo-header__link__pDHX1 styles_logo-header__image__zuDpr" href="/"><img alt="Яндекс Образование" aria-hidden="true" class="styles_desktopOnly__j_5Ge" src="https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg"><img alt="Яндекс Образование" aria-hidden="true" class="styles_mobileOnly__3cLf9" src="https://yastatic.net/s3/education-portal/media/logo_mobile_8bc5eb38fb.svg"><h1 aria-hidden="true" class="Text Text_color_primary Text_typography_bodyS styles_logo-header__title__0_MiO styles_text__iQHEy">Яндекс Образование — Личный кабинет</h1></a></div></div></div><div aria-hidden="true" class="styles_header__controls-burger__WUP18"><div class="styles_header__swipe-element__q6aOO"></div><div class="styles_buttonsContainerMobile___WcOD"><div class="styles_header-buttons__WWn3B styles_header__buttons__z2b3Y"><a tabindex="-1" href="https://passport.yandex.ru/auth/?origin=education&amp;retpath=https%3A%2F%2Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis"><span class="Button Button_view_primary Button_size_s Button_isInteractive Theme_controls_size_s styles_header-buttons__item__DNLsb" autocomplete="off"><span class="Button-Content Theme_palette_inverse"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"></div><span class="Text Text_hideOverflow Button-Text" aria-describedby="7cb4d227-181b-4fa7-9067-2a9acdc2eda5" style="--text-max-visible-lines: 1;">Войти в ID</span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></span></a></div></div><nav aria-label="Навигация по сайту" class="styles_top-menu__37_My"><div class="styles_top-menu__links__vkwzq"><a title="Школьникам" class="styles_top-menu__link__joHz8" id="19" tabindex="-1" href="/pupils">Школьникам</a><a aria-disabled="false" title="Студентам" class="styles_top-menu__link__joHz8" id="4136" tabindex="-1" href="/students">Студентам</a><a aria-disabled="false" title="Абитуриентам" class="styles_top-menu__link__joHz8" id="4137" tabindex="-1" href="/university">Абитуриентам</a><a aria-disabled="false" title="Партнёрам" class="styles_top-menu__link__joHz8" id="4138" tabindex="-1" href="https://edumakers.yandex.ru/">Партнёрам</a><a aria-disabled="false" title="События" class="styles_top-menu__link__joHz8" id="4139" tabindex="-1" href="/handbook">Хендбуки</a><a aria-disabled="false" title="Журнал" class="styles_top-menu__link__joHz8" id="4140" tabindex="-1" href="/journal">Журнал</a><a aria-disabled="false" title="AI" class="styles_top-menu__link__joHz8" id="4141" tabindex="-1" href="https://education.yandex.ru/ai">AI</a></div></nav></div><div class="styles_sideGroup__gWR7N"><button class="Button Button_view_ghost Button_size_m Button_isInteractive Theme_controls_size_m styles_header-buttons__search__0Sfs6" aria-label="Поиск" autocomplete="off" type="button" data-testid="search-button"><span class="Button-Content"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_icon Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span class="Icon Icon_size_l Icon_hasGlyph_noFill Icon_oldFormat Icon_sizeManagement_self Icon_glyph_search Button-Icon styles_header-buttons__search-icon__6kEp0" aria-hidden="true" style="--hr-icon-color:var(--hr-color-text-secondary)"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11 5a6 6 0 1 0 0 12 6 6 0 0 0 0-12Zm-8 6a8 8 0 1 1 14.32 4.906l3.387 3.387a1 1 0 0 1-1.414 1.414l-3.387-3.387A8 8 0 0 1 3 11Z" fill="currentColor"></path></svg></span></span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_icon Button-Slot"></div><span class="Button-State"></span></span></button><div class="styles_buttonsContainerDesktop__W9OAy"><div class="styles_header-buttons__WWn3B styles_header__buttons__z2b3Y"><a tabindex="-1" href="https://passport.yandex.ru/auth/?origin=education&amp;retpath=https%3A%2F%2Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis"><span class="Button Button_view_primary Button_size_s Button_isInteractive Theme_controls_size_s styles_header-buttons__item__DNLsb" autocomplete="off"><span class="Button-Content Theme_palette_inverse"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text">Войти в ID</span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></span></a></div></div><div class="styles_header__toggle__xS3iB"><button class="Button Button_view_ghost Button_size_m Button_isInteractive Theme_controls_size_m styles_toggle__n99_Y" aria-label="Меню" autocomplete="off" type="button" data-testid="header-menu-button"><span class="Button-Content"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_icon Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span class="Icon Icon_size_l Icon_hasGlyph_noFill Icon_oldFormat Icon_sizeManagement_self Icon_glyph_menu Button-Icon styles_toggleIcon__SAvbq" aria-hidden="true" style="--hr-icon-color:var(--hr-color-text-secondary)"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a1 1 0 0 1 1-1h16a1 1 0 1 1 0 2H4a1 1 0 0 1-1-1ZM3 18a1 1 0 0 1 1-1h16a1 1 0 1 1 0 2H4a1 1 0 0 1-1-1ZM3 6a1 1 0 0 1 1-1h16a1 1 0 1 1 0 2H4a1 1 0 0 1-1-1Z" fill="currentColor"></path></svg></span></span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_icon Button-Slot"></div><span class="Button-State"></span></span></button></div></div></div></section></div></header><div class="TopDrawer_root__OZJVp TopDrawer_isOpening__po5Fn styles_drawer__QQFuJ" data-testid="TopDrawer" style="height: 812px;"><div class="TopDrawer_observer__KZrCY"><div class="TopDrawer_overlay__NhRuz"></div><div class="TopDrawer_curtain__RLie3"><div aria-label="Поиск" aria-modal="true" class="TopDrawer_content__qJybk" data-slot="content" role="dialog" tabindex="-1"><div class="styles_content__n9ucm"><button aria-label="Закрыть" class="styles_close__Tvrr3"><span class="Icon Icon_size_l Icon_hasGlyph_noFill Icon_oldFormat Icon_sizeManagement_self Icon_glyph_close" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.293 4.293a1 1 0 0 1 1.414 0L12 10.586l6.293-6.293a1 1 0 1 1 1.414 1.414L13.414 12l6.293 6.293a1 1 0 0 1-1.414 1.414L12 13.414l-6.293 6.293a1 1 0 0 1-1.414-1.414L10.586 12 4.293 5.707a1 1 0 0 1 0-1.414Z" fill="currentColor"></path></svg></span></button><form class="styles_search__3lV53"><div class="styles_root__Yoz99" data-testid="borderlessInput-root"><span aria-hidden="true" class="Icon Icon_size_l Icon_hasGlyph_noFill Icon_oldFormat Icon_sizeManagement_self Icon_glyph_search styles_searchIcon__Fnqiw"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11 5a6 6 0 1 0 0 12 6 6 0 0 0 0-12Zm-8 6a8 8 0 1 1 14.32 4.906l3.387 3.387a1 1 0 0 1-1.414 1.414l-3.387-3.387A8 8 0 0 1 3 11Z" fill="currentColor"></path></svg></span><input autofocus="" maxlength="300" placeholder="Поиск по сайту" class="styles_search_input__xpJY1 styles_input__1iRua" value=""></div></form></div></div></div></div></div><div class="hb"><div class="styles_article-side-panel__k1oHL"><div class="styles_drawer__zco4X"><div class="styles_drawerWrapper__yQ6Ju"><div class="styles_overlay__OqjzT"></div><div class="styles_drawerHeader__G3tPl"><div class="styles_panel-tabs__8BQ4K"><div class="styles_button-wrapper__A8ySg"><button class="Button Button_view_ghost Button_size_m Button_isInteractive Button_withoutSlots Theme_controls_size_m styles_button__b2Aik styles_button__slVGb" aria-label="Содержание" autocomplete="off" type="button" data-testid="panel-contents-tab"><span class="Button-Content"><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span data-icon="book" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__1BYeh" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 4.992a6.013 6.013 0 0 0-.107-.132C11.14 3.956 9.881 3 8 3H6a4 4 0 0 0-4 4v8a4 4 0 0 0 3.999 4H8c1.846 0 2.547.46 2.793.707C11.162 20.228 11.15 21 12 21c.85 0 .838-.771 1.207-1.293.246-.246.947-.707 2.793-.707h2.001A3.999 3.999 0 0 0 22 15V7a4 4 0 0 0-4-4h-2c-1.881 0-3.14.956-3.893 1.86a6.013 6.013 0 0 0-.107.132ZM6 5a2 2 0 0 0-2 2v8a2 2 0 0 0 1.999 2H8c1.28 0 2.259.19 3 .503V7.188a4.396 4.396 0 0 0-.643-1.048C9.86 5.544 9.119 5 8 5H6Zm7 12.503c.741-.313 1.72-.503 3-.503h2.001A1.999 1.999 0 0 0 20 15V7a2 2 0 0 0-2-2h-2c-1.119 0-1.86.544-2.357 1.14A4.396 4.396 0 0 0 13 7.188v10.315Z" fill="currentColor"></path></svg></span><span class="styles_text-wrapper__ap6Ri">Содержание</span></span><span class="Button-State"></span></span></button></div><div class="styles_button-wrapper__A8ySg"><button class="Button Button_view_ghost Button_size_m Button_isInteractive Button_withoutSlots Theme_controls_size_m styles_button__b2Aik styles_button__slVGb" aria-label="Заметки" autocomplete="off" type="button" data-testid="panel-notes-tab"><span class="Button-Content"><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span data-icon="bookmark" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__1BYeh" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4 6a4 4 0 0 1 4-4h8a4 4 0 0 1 4 4v12.942c0 1.67-1.923 2.604-3.236 1.573L12 16.772l-4.764 3.743C5.923 21.546 4 20.611 4 18.942V6Zm4-2a2 2 0 0 0-2 2v12.942l4.764-3.743a2 2 0 0 1 2.472 0L18 18.942V6a2 2 0 0 0-2-2H8Z" fill="currentColor"></path></svg></span><span class="styles_text-wrapper__ap6Ri">Заметки</span></span><span class="Button-State"></span></span></button></div></div></div><div class="styles_drawerBody__Xqvde" aria-hidden="true" tabindex="-1" hidden=""><div class="styles_header__RdXaV"><h2 class="styles_root__EmBCZ styles_title__w1r1k" data-variant="heading" data-weight="medium" data-color="primary">Содержание</h2><button class="Button Button_view_ghost Button_size_m Button_isInteractive Theme_controls_size_m styles_close__InH6S styles_button__slVGb" autocomplete="off" type="button" title="закрыть"><span class="Button-Content"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_icon Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span data-icon="close" class="Icon Icon_size_s Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat Button-Icon" aria-hidden="true" style="--hr-icon-color:var(--hr-color-text-secondary)"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.293 4.293a1 1 0 0 1 1.414 0L12 10.586l6.293-6.293a1 1 0 1 1 1.414 1.414L13.414 12l6.293 6.293a1 1 0 0 1-1.414 1.414L12 13.414l-6.293 6.293a1 1 0 0 1-1.414-1.414L10.586 12 4.293 5.707a1 1 0 0 1 0-1.414Z" fill="currentColor"></path></svg></span></span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_icon Button-Slot"></div><span class="Button-State"></span></span></button></div><div class="styles_content__rxXV3"><nav class="styles_menu__EGY9t" data-search-hidden="true" aria-label="Навигация по статье"><ul class="styles_menu-list__bqpuA"><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">1.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Введение</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->3</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">2.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Введение в теорию графов</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->5</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">3.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Основы математического анализа</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->3</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">4.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Линейная алгебра</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->14</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp styles_open__iwALZ" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div><ul class="styles_menu-block__items-list__ZrouL"><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.1</span><a class="styles_menu-item__link___ZydP" href="./chetupe-o-chyom-mi-pogovorim-v-etoi-glave"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">О чём мы поговорим в этой главе</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.2</span><a class="styles_menu-item__link___ZydP" href="./vektori"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Векторы</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.3</span><a class="styles_menu-item__link___ZydP" href="./matritsi"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Матрицы</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.4</span><a class="styles_menu-item__link___ZydP" href="./istemi-lineinikh-uravnenii-osnovi"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Системы линейных уравнений основы</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.5</span><a class="styles_menu-item__link___ZydP" href="./sistemi-lineinikh-uravnenii-prodvinutie-metodi-resheniia"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Системы линейных уравнений: продвинутые методы решения</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.6</span><a class="styles_menu-item__link___ZydP" href="./opredelitel"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Определитель</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.7</span><a class="styles_menu-item__link___ZydP" href="./geometriia-priznakovogo-prostranstva"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Геометрия признакового пространства: нормы и расстояния</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.8</span><a class="styles_menu-item__link___ZydP" href="./proektsii-ugli-i-ortogonalnost"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Проекции, углы и ортогональность</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.9</span><a class="styles_menu-item__link___ZydP" href="./spektralnie-metodi-i-matrichnie-razlozheniia"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Спектральные методы и матричные разложения</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.10</span><a class="styles_menu-item__link___ZydP" href="./snizhenie-razmernosti-i-latentnie-faktori"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Снижение размерности и латентные факторы</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.11</span><a class="styles_menu-item__link___ZydP" href="./lineinie-modeli-i-reguliarizatsiia"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Линейные модели и регуляризация</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.12</span><a class="styles_menu-item__link___ZydP" href="./svm-i-yadrovoi-triuk"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">SVM и ядровой трюк</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.13</span><a class="styles_menu-item__link___ZydP" href="./preprotsessing-priznakov"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Препроцессинг признаков</span><div class="styles_indicator__MC7hu styles_indicator__inactive__UTnKP" data-testid="sidepanel-indicator"></div></div></a></li><li class="styles_menu-item__LxPcN"><span class="styles_root__EmBCZ styles_menu-item__label__tKCIB" data-weight="regular" data-color="primary">4.14</span><span class="styles_menu-item__link___ZydP styles_menu-item__link_active__5CBIq"><div class="styles_menu-item__article-title__XdZVg"><span class="styles_root__EmBCZ styles_menu-item__active__a59KG styles_menu-item__article-title-text__BTrkr" data-weight="regular" data-color="primary">Чему вы научились</span><div class="styles_indicator__MC7hu styles_indicator__active__NjQNY" data-testid="sidepanel-indicator"></div></div><ul class="styles_sections__eCQId"></ul></span></li></ul></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">5.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Комбинаторика</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->6</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">6.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Теория вероятностей</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->8</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li><li class="styles_menu-block__WLHSg"><div class="styles_accordion__IP9mb" data-testid="sidepanel-accordion"><div class="styles_divider__vpyCs styles_divider__g8Mhh"></div><div class="styles_title__IKvyk" tabindex="0" role="button"><div class="styles_menu-block__chapter-title__juSkS"><h2 class="styles_root__EmBCZ styles_menu-block__title-number__Ru7_L" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">7.</h2><div class="styles_menu-block__progress__78kU4"><h2 class="styles_root__EmBCZ styles_menu-block__title__Msaqv" style="font-size:20px" data-variant="heading" data-weight="medium" data-color="primary">Продвинутый анализ графов</h2><span class="styles_root__EmBCZ" style="font-size:16px" data-weight="regular" data-color="secondary">Пройдено<!-- --> <!-- -->0<!-- -->/<!-- -->4</span></div></div><span data-icon="arrowShortRight" class="Icon Icon_size_m Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_icon__Ppqtp" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></div></li></ul></nav></div></div></div><div class="styles_children__KSNca"><meta property="og:type" content="article" itemprop=""><meta property="article:modified_time" content="2025-12-01T15:27:39.561Z" itemprop=""><meta property="article:section" content="Математика для анализа данных" itemprop=""><div class="styles_container__SBhDc"><div class="styles_progressContainer__1vGGr"><div class="styles_progressBar__XS_vE" style="width:11px"></div></div><div class="styles_articleCover__EKYZn"><div class="styles_root__jRQsx styles_breadcrumbs__nMPkF" data-search-hidden="true"><div class="styles_backArrow__WbYmN"><span data-icon="arrowShortRight" aria-hidden="true" class="Icon Icon_size_xs Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div><nav aria-label="Хлебные крошки" class="styles_root__vgBQ1" data-search-hidden="true"><ol class="styles_items__QJQzv" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="styles_item__uyDoI" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><a href="/" tabindex="0" data-react-aria-pressable="true" itemprop="item" class=""><span class="styles_root__EmBCZ" itemprop="name" data-weight="regular" data-color="inherit">Главная</span></a><meta itemprop="position" content="0"></li><li aria-hidden="true" class="styles_separator__tQEdo">/</li><li class="styles_item__uyDoI" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><a href="/handbook" tabindex="0" data-react-aria-pressable="true" itemprop="item" class=""><span class="styles_root__EmBCZ" itemprop="name" data-weight="regular" data-color="inherit">Хендбуки</span></a><meta itemprop="position" content="1"></li><li aria-hidden="true" class="styles_separator__tQEdo">/</li><li class="styles_item__uyDoI styles_showMobile__l2S6V" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><a href="/handbook/math" tabindex="0" data-react-aria-pressable="true" itemprop="item" class=""><span class="styles_root__EmBCZ" itemprop="name" data-weight="regular" data-color="inherit">Математика для анализа данных</span></a><meta itemprop="position" content="2"></li><li aria-hidden="true" class="styles_separator__tQEdo">/</li><li class="styles_item__uyDoI" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem"><span class="styles_root__EmBCZ" itemprop="name" data-weight="regular" data-color="inherit">Чему вы научились</span><meta itemprop="position" content="3"></li></ol></nav></div><h1 class="styles_root__EmBCZ styles_title__Ae0WW" data-search-hidden="true" data-variant="heading" data-weight="medium" data-color="primary">4.14 Чему вы научились</h1></div><main class="styles_root__R5rVX" data-testid="ArticleContent-root" data-ai-main-material="true"><div class="hljs_hljs-atelier-heath__2Efzm styles_content__jb6Og" id="article-content"><div class="styles_notes-from-selected__d23uG"><div><div class="styles_content__Lzr6L"><div id="wysiwyg-client-content"><p>В этой главе мы собрали воедино линейно-алгебраические инструменты, которые чаще всего встречаются в анализе данных и машинном обучении, и показали их на практических задачах.</p>
<ul>
<li>Освежили базу: векторы и матрицы, операции (сложение, умножение, транспонирование), свойства следа и их роль в регуляризации и упрощении выражений.</li>
<li>Научились решать системы линейных уравнений: метод Гаусса, LU-разложение (и узнали случаи, когда им пользоваться); обсудили смысл определителя.</li>
<li>Разобрали ортогонализацию и QR-разложение (включая Грама — Шмидта) и увидели, как через ортогональные проекции формулируется и решается МНК-регрессия.</li>
<li>Ввели нормы и расстояния, обсудили обусловленность и устойчивость вычислений и разобрали, почему масштаб признаков влияет на сходимость алгоритмов.</li>
<li>Перешли к спектральным методам: разобрали собственные значения/векторы и диагонализацию; сингулярное разложение (SVD) как основу низкоранговых приближений, сжатия и поиска структуры.</li>
<li>Показали, как из SVD получается метод главных компонент (PCA) для снижения размерности. На практике рассмотрели, как по спектру выбирать число компонент и что означает объяснённая дисперсия.</li>
<li>Рассмотрели разложения для текстов и скрытых тем: LSA и NMF, их интерпретируемость и ограничения.</li>
<li>Разобрали матричную факторизацию в рекомендательных системах: как разреженная матрица рейтингов сворачивается в общие латентные факторы пользователей и объектов</li>
<li>Рассмотрели работу SVM, вывели двойственную задачу и обсудили ядровой трюк, позволяющий работать в высокоразмерных пространствах без явного преобразования признаков.</li>
<li>Завершили предобработкой признаков: центрирование, стандартизация, робастное масштабирование и их влияние на устойчивость и качество моделей.</li>
</ul>
<p>Теперь у вас есть целостное представление о том, как методы линейной алгебры применяются в современных алгоритмах анализа данных, и вы умеете использовать эти инструменты для повышения устойчивости, интерпретируемости и эффективности моделей в практических задачах.</p>
<p>Освоив геометрию признакового пространства, вы научились работать с данными в непрерывных пространствах, представляя их векторами и находя в них геометрические структуры. Однако многие задачи в машинном обучении — от подбора признаков до настройки моделей — сводятся к работе с конечными наборами и выбору из огромного числа вариантов. Чтобы научиться оценивать сложность таких задач и понимать, почему полный перебор часто невозможен, понадобится аппарат комбинаторики.</p>
<p>Так что в следующей главе мы погрузимся в мир множеств, перестановок и сочетаний, чтобы разобраться, как принципы подсчёта лежат в основе настройки моделей и помогают осознать то самое «проклятие размерности».</p><div id="inline-hanbook-community-banner" class="styles_hanbookCommunityBannerInline__4E6Cj"><div class="styles_inlineBanner__eiuoo" data-testid="HanbookCommunityBanner-root"><div class="styles_inlineImageWrapper__rSMRU"><img aria-hidden="true" name="Poluchajte" hash="Poluchajte_obnovleniya_ot_obrazovaniya_30910b54df_61ea47b298" ext=".svg" mime="image/svg+xml" size="33.89" url="https://yastatic.net/s3/education-portal/media/Poluchajte_obnovleniya_ot_obrazovaniya_30910b54df_61ea47b298.svg" provider="strapi-plugin-yandexify" alt="" loading="lazy" width="237" height="124" decoding="async" data-nimg="1" class="styles_inlineImage__U1Yqp" src="https://yastatic.net/s3/education-portal/media/Poluchajte_obnovleniya_ot_obrazovaniya_30910b54df_61ea47b298.svg" style="color: transparent;"></div><div class="styles_inlineContent__d9XZM" data-test-inaccessible="true"><div class="styles_inlineTitleWrapper__6HFQs"><h3 class="styles_root__EmBCZ styles_inlineTitle__xD8ER" data-weight="medium" data-color="primary">Вступайте в&nbsp;сообщество хендбука</h3></div><div data-test-inaccessible="true"><span class="styles_root__EmBCZ styles_inlineDescription__6JoFe" data-weight="regular" data-color="primary">Здесь можно найти единомышленников, экспертов и&nbsp;просто интересных собеседников. А&nbsp;ещё&nbsp;— получить помощь или поделиться знаниями.</span></div></div><div class="styles_inlineJoinWrapper__ckZVg"><a target="_blank" rel="noopener noreferrer" title="Вступить" aria-disabled="false" id="5538" href="https://t.me/+bikU3_M1x0s0YzMy"><span class="Button Button_view_primary Button_size_l Button_isInteractive Theme_controls_size_l styles_popupJoinButton__KSomV styles_button__slVGb" autocomplete="off"><span class="Button-Content Theme_palette_inverse"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"></div><span class="Text Text_hideOverflow Button-Text" style="--text-max-visible-lines: 1;">Вступить</span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></span></a></div></div></div>
</div></div></div><div class="styles_action-wrapper__qGgRM"><span class="styles_visually-hidden__mtWeI">Чтобы добавить в заметки выделенный текст, нажмите <!-- -->Command<!-- --> + E</span><button class="Button Button_view_primary Button_size_m Button_isInteractive Theme_controls_size_m styles_button__slVGb" aria-label="Добавить в заметки" autocomplete="off" type="button" role="button"><span class="Button-Content Theme_palette_inverse"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"><div class="Slot Slot_gap_s Slot_padding_s"><span class="Slot-Item"><span data-icon="bookmark" class="Icon Icon_size_s Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat Button-Icon" aria-hidden="true"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4 6a4 4 0 0 1 4-4h8a4 4 0 0 1 4 4v12.942c0 1.67-1.923 2.604-3.236 1.573L12 16.772l-4.764 3.743C5.923 21.546 4 20.611 4 18.942V6Zm4-2a2 2 0 0 0-2 2v12.942l4.764-3.743a2 2 0 0 1 2.472 0L18 18.942V6a2 2 0 0 0-2-2H8Z" fill="currentColor"></path></svg></span></span></div></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text">Добавить в заметки</span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></button></div></div></div></main><div class="styles_articleProgress__u3phH"><div class="styles_root__jW9gl" data-testid="article-progress"><div class="styles_progress__YrkcK"><div class="Skeleton Skeleton_type_text Skeleton_size_l Skeleton_animation" style="width:100%"></div></div></div><div class="styles_errorLinkWrapper__IREsd"><a target="_blank" rel="noopener noreferrer" title="Сообщить об ошибке" aria-disabled="false" id="4399" class="styles_errorLink__LlWps" href="https://forms.yandex.ru/surveys/academy/?proekt=handbooks"><svg width="18" height="19" viewBox="0 0 18 19" fill="none" xmlns="http://www.w3.org/2000/svg" class="styles_icon__6R_y4" aria-hidden="true"><path d="M0.25 9.5C0.25 11.2306 0.763179 12.9223 1.72464 14.3612C2.6861 15.8002 4.05267 16.9217 5.65152 17.5839C7.25037 18.2462 9.00971 18.4195 10.707 18.0819C12.4044 17.7442 13.9635 16.9109 15.1872 15.6872C16.4109 14.4635 17.2442 12.9044 17.5819 11.207C17.9195 9.50971 17.7462 7.75037 17.0839 6.15152C16.4217 4.55267 15.3002 3.1861 13.8612 2.22464C12.4223 1.26318 10.7306 0.75 9 0.75C6.67936 0.75 4.45376 1.67187 2.81282 3.31282C1.17187 4.95376 0.25 7.17936 0.25 9.5ZM14.7188 14.3438L4.15625 3.78125C5.59415 2.58437 7.42745 1.96811 9.29636 2.05341C11.1653 2.13872 12.9348 2.91942 14.2577 4.24231C15.5806 5.5652 16.3613 7.33474 16.4466 9.20364C16.5319 11.0725 15.9156 12.9059 14.7188 14.3438ZM4.15 15.225C2.63645 13.9397 1.69438 12.1066 1.53035 10.1278C1.36632 8.14888 1.99373 6.18572 3.275 4.66875L13.8312 15.225C12.4776 16.3688 10.7628 16.9963 8.99062 16.9963C7.21849 16.9963 5.50361 16.3688 4.15 15.225Z" fill="#666666"></path></svg><span class="styles_root__EmBCZ" data-weight="regular" data-color="primary">Сообщить об ошибке</span></a></div></div><div class="styles_v2ContestQuiz__mOOqZ"></div><section class="styles_callToAction__XgopL styles_bookCTA__HRQex styles_noArticleAction__IC_rK"><div></div></section><div class="styles_errorLinkWrapper__pz4ZU"><a target="_blank" rel="noopener noreferrer" title="Сообщить об ошибке" aria-disabled="false" id="4399" class="styles_errorLink__e38cf" href="https://forms.yandex.ru/surveys/academy/?proekt=handbooks"><svg width="18" height="19" viewBox="0 0 18 19" fill="none" xmlns="http://www.w3.org/2000/svg" class="styles_icon__z8p3U" aria-hidden="true"><path d="M0.25 9.5C0.25 11.2306 0.763179 12.9223 1.72464 14.3612C2.6861 15.8002 4.05267 16.9217 5.65152 17.5839C7.25037 18.2462 9.00971 18.4195 10.707 18.0819C12.4044 17.7442 13.9635 16.9109 15.1872 15.6872C16.4109 14.4635 17.2442 12.9044 17.5819 11.207C17.9195 9.50971 17.7462 7.75037 17.0839 6.15152C16.4217 4.55267 15.3002 3.1861 13.8612 2.22464C12.4223 1.26318 10.7306 0.75 9 0.75C6.67936 0.75 4.45376 1.67187 2.81282 3.31282C1.17187 4.95376 0.25 7.17936 0.25 9.5ZM14.7188 14.3438L4.15625 3.78125C5.59415 2.58437 7.42745 1.96811 9.29636 2.05341C11.1653 2.13872 12.9348 2.91942 14.2577 4.24231C15.5806 5.5652 16.3613 7.33474 16.4466 9.20364C16.5319 11.0725 15.9156 12.9059 14.7188 14.3438ZM4.15 15.225C2.63645 13.9397 1.69438 12.1066 1.53035 10.1278C1.36632 8.14888 1.99373 6.18572 3.275 4.66875L13.8312 15.225C12.4776 16.3688 10.7628 16.9963 8.99062 16.9963C7.21849 16.9963 5.50361 16.3688 4.15 15.225Z" fill="#666666"></path></svg><span class="styles_root__EmBCZ" data-weight="regular" data-color="primary">Сообщить об ошибке</span></a></div><div class="styles_article-navigate__SKki7" data-testid="ArticleNavigate-root"><a class="styles_navigation-block__p937C" href="./preprotsessing-priznakov"><span class="styles_root__EmBCZ styles_label__QNj9q" data-weight="regular" data-color="primary">Предыдущий параграф</span><span class="styles_root__EmBCZ styles_title___sZy2" data-weight="medium" data-color="primary">4.13. Препроцессинг признаков</span><div class="styles_arrow-line__qoKkm"><span data-icon="arrowShortRight" aria-hidden="true" class="Icon Icon_size_xl Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_arrow__BhuvQ styles_back__1E0f1"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></a><a class="styles_navigation-block__p937C" href="./math-glava-pyat-o-chyom-mi-pogovorim-v-etoi-glave"><span class="styles_root__EmBCZ styles_label__QNj9q" data-weight="regular" data-color="primary">Следующий параграф</span><span class="styles_root__EmBCZ styles_title___sZy2" data-weight="medium" data-color="primary">5.1. О чём мы поговорим в этой главе</span><div class="styles_arrow-line__qoKkm"><span data-icon="arrowShortRight" aria-hidden="true" class="Icon Icon_size_xl Icon_sizeManagement_self Icon_hasGlyph Icon_newFormat styles_arrow__BhuvQ styles_forward__d5yy6"><svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.293 4.293a1 1 0 0 0 0 1.414L14.586 12l-6.293 6.293a1 1 0 1 0 1.414 1.414l7-7a1 1 0 0 0 0-1.414l-7-7a1 1 0 0 0-1.414 0Z" fill="currentColor"></path></svg></span></div></a></div></div></div></div></div><!--$--><!--/$--></div><footer md="0" data-testid="Footer-root" class="styles_root__IBJUG styles_gutter-top__lwtYx styles_footer___2teW" style="background-color:#FFFFFF;--gutter-size-xs:0px;--gutter-size-sm:0px;--gutter-size-lg:0px;--gutter-size-xl:0px"><div class="styles_root__IBJUG styles_inner__lwcI4" style="--gutter-size-xs:50px;--gutter-size-sm:50px;--gutter-size-lg:50px;--gutter-size-xl:50px"><div class="styles_logo__oFZ4z"><a aria-label="Яндекс Образование" href="/"><img alt="Яндекс Образование" aria-hidden="true" class="styles_desktopOnly__j_5Ge" src="https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg"><img alt="Яндекс Образование" aria-hidden="true" class="styles_mobileOnly__3cLf9" src="https://yastatic.net/s3/education-portal/media/edu_logo_simple_9790e70002.svg"></a></div><div class="styles_columns__n2ryY"><div class="styles_column__XW85v"><ul><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4460" href="https://education.yandex.ru/uchebnik/main"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Яндекс Учебник</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4458" href="https://lyceum.yandex.ru/"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Яндекс Лицей</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4459" href="https://practicum.yandex.ru/?utm_source=partners&amp;utm_medium=partners&amp;utm_campaign=yandexeducation_partners_RF_Common_Unde_b2c_Landing-page_None_None"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Яндекс Практикум</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4461" href="https://shad.yandex.ru/"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Школа анализа данных</span></a></li><li><a aria-disabled="false" id="4463" href="/university"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Программы в университетах</span></a></li></ul></div><div class="styles_column__XW85v"><ul><li><a aria-disabled="false" id="4462" href="/research"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Исследования</span></a></li><li><a aria-disabled="false" id="4464" href="/handbook"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Хендбуки</span></a></li><li><a aria-disabled="false" id="5881" href="/roadmap"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Карты IT-навыков</span></a></li><li><a aria-disabled="false" id="4466" href="/knowledge"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">База знаний</span></a></li><li><a aria-disabled="false" id="4465" href="/journal"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Журнал</span></a></li><li><a aria-disabled="false" id="4467" href="/events"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">События</span></a></li></ul></div><div class="styles_column__XW85v"><ul><li><a aria-disabled="false" id="4468" href="/about"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">О нас</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4469" href="https://forms.yandex.ru/surveys/13457493.e7112b8cdd8c782bfe6e4b1ab1b73f49438edacf/"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Обратная связь</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4470" href="https://yandex.ru/legal/education_termsofuse/ru/"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Пользовательское соглашение</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4471" href="https://yandex.ru/edtech"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Сайт образовательной организации</span></a></li><li><a aria-disabled="false" rel="noopener noreferrer" target="_blank" id="4472" href="https://yandex.ru/edtech/sveden"><span class="Text Text_color_primary Text_typography_bodyS styles_text__iQHEy">Сведения об образовательной организации</span></a></li></ul></div></div><ul class="styles_socials__WWBdN styles_socials__Is6By"><li class="styles_social__pAnot"><a title="Рассылка" class="Button Button_view_secondary Button_size_m Button_isInteractive Theme_controls_size_m styles_button___Ftu4" aria-label="Рассылка" autocomplete="off" href="/subscribe"><span class="Button-Content"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span aria-hidden="true">Рассылка</span></span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></a></li><li class="styles_social__pAnot"><a title="Бот" class="Button Button_view_secondary Button_size_m Button_isInteractive Theme_controls_size_m styles_button___Ftu4" aria-label="Бот" autocomplete="off" href="https://t.me/yaeducation_bot?start=n_113083__c_7628"><span class="Button-Content"><div class="Slot-Wrapper Slot-Wrapper_place_left Slot-Wrapper_type_action Button-Slot"></div><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><span aria-hidden="true">Бот</span></span><div class="Slot-Wrapper Slot-Wrapper_place_right Slot-Wrapper_type_action Button-Slot"></div><span class="Button-State"></span></span></a></li><li class="styles_social__pAnot"><a title="ВКонтакте" class="Button Button_view_secondary Button_size_m Button_isInteractive Button_withoutSlots Theme_controls_size_m styles_button___Ftu4" aria-label="ВКонтакте" autocomplete="off" href="https://vk.com/yandex_education"><span class="Button-Content"><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><img alt="ВКонтакте" aria-hidden="true" src="https://yastatic.net/s3/education-portal/media/social_icon_vk_97bf858cd5.svg" color="secondary" class="Button-Icon"></span><span class="Button-State"></span></span></a></li><li class="styles_social__pAnot"><a title="YouTube" class="Button Button_view_secondary Button_size_m Button_isInteractive Button_withoutSlots Theme_controls_size_m styles_button___Ftu4" aria-label="YouTube" autocomplete="off" href="https://www.youtube.com/@Education_Yandex"><span class="Button-Content"><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><img alt="YouTube" aria-hidden="true" src="https://yastatic.net/s3/education-portal/media/social_icon_yt_d20daea655.svg" color="secondary" class="Button-Icon"></span><span class="Button-State"></span></span></a></li><li class="styles_social__pAnot"><a title="Telegram" class="Button Button_view_secondary Button_size_m Button_isInteractive Button_withoutSlots Theme_controls_size_m styles_button___Ftu4" aria-label="Telegram" autocomplete="off" href="https://t.me/education_yandex"><span class="Button-Content"><span style="--text-max-visible-lines:1" class="Text Text_hideOverflow Button-Text"><img alt="Telegram" aria-hidden="true" src="https://yastatic.net/s3/education-portal/media/social_icon_tg_9faafb663e.svg" color="secondary" class="Button-Icon"></span><span class="Button-State"></span></span></a></li></ul><p class="Text Text_color_primary Text_typography_bodyS styles_disclaimer__ga29X styles_text__iQHEy">Образовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»
на&nbsp;основании <a rel="noopener noreferrer" target="_self" href="https://yastatic.net/s3/academy/docs/license-ysda.pdf">Лицензии № Л035-01298-77/00185314</a> от 24 марта 2015 года.<!-- -->
<!-- -->© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».</p></div></footer><script nonce="">(self.__next_s=self.__next_s||[]).push([0,{"nonce":"99442c83-224a-4e1c-a471-dfa207be1201","children":"!function(e,t){if(e.Ya=e.Ya||{},Ya.Rum)throw new Error(\"Rum: interface is already defined\");var n=e.performance,i=n&&n.timing&&n.timing.navigationStart||Ya.startPageLoad||+new Date,s=e.requestAnimationFrame,r=function(){},a=Ya.Rum={_defTimes:[],_defRes:[],_countersToExposeAsEvents:[\"2325\",\"2616.85.1928\",\"react.inited\"],enabled:!!n,version:\"6.1.21\",vsStart:document.visibilityState,vsChanged:!1,vsChangeTime:1/0,_deltaMarks:{},_markListeners:{},_onComplete:[],_onInit:[],_unsubscribers:[],_eventLisneters:{},_settings:{},_vars:{},init:function(e,t){a._settings=e,a._vars=t},getTime:n&&n.now?function(){return n.now()}:Date.now?function(){return Date.now()-i}:function(){return new Date-i},time:function(e){a._deltaMarks[e]=[a.getTime()]},timeEnd:function(e,t){var n=a._deltaMarks[e];n&&0!==n.length&&n.push(a.getTime(),t)},sendTimeMark:function(e,t,n,i){void 0===t&&(t=a.getTime()),a.emit({metricName:\"defTimes\",data:[e,t,i]}),a.mark(e,t)},sendDelta:function(e,t,n,i){var s,r=a._deltaMarks;r[e]||void 0===t||(s=i&&i.originalEndTime?i.originalEndTime:a.getTime(),r[e]=[s-t,s,n])},sendResTiming:function(e,t){a.emit({metricName:\"defRes\",data:[e,t]})},sendRaf:function(e){var t=a.getSetting(\"forcePaintTimeSending\");if(s&&(t||a.isTimeAfterPageShow(a.getTime()))){var n=\"2616.\"+e;s((function(){a.getSetting(\"sendFirstRaf\")&&a.sendTimeMark(n+\".205\"),s((function(){a.sendTimeMark(n+\".1928\")}))}))}},isVisibilityChanged:function(){return a.vsStart&&(\"visible\"!==a.vsStart||a.vsChanged)},isTimeAfterPageShow:function(e){return\"visible\"===a.vsStart||a.vsChangeTime<e},mark:n&&n.mark?function(e,t){n.mark(e+(t?\": \"+t:\"\"))}:function(){},getSetting:function(e){var t=a._settings[e];return null===t?null:t||\"\"},on:function(e,t){if(\"function\"==typeof t)return(a._markListeners[e]=a._markListeners[e]||[]).push(t),function(){if(a._markListeners[e]){var n=a._markListeners[e].indexOf(t);n>-1&&a._markListeners[e].splice(n,1)}}},noop:r,sendTrafficData:r,finalizeLayoutShiftScore:r,finalizeLargestContentfulPaint:r,getLCPAdditionalParams:r,getCLSAdditionalParams:r,getINPAdditionalParams:r,getImageGoodnessAdditionalParams:r,_eventListeners:{},_eventsBuffer:{},subscribe:function(e,t){if(!a.getSetting(\"noEvents\"))return this._eventLisneters[e]=this._eventLisneters[e]||new Set,this._eventLisneters[e].add(t),function(){this.unsubscribe(e,t)}.bind(this)},unsubscribe:function(e,t){this._eventLisneters[e].delete(t)},emit:function(e){if(!a.getSetting(\"noEvents\")){var t=a.getSetting(\"eventsLimits\")&&a.getSetting(\"eventsLimits\")[e.metricName]||20;this._eventLisneters[e.metricName]&&this._eventLisneters[e.metricName].forEach((function(t){t(e)})),this._eventsBuffer[e.metricName]=this._eventsBuffer[e.metricName]||[],this._eventsBuffer[e.metricName].push(e),this._eventsBuffer[e.metricName].length>t&&(this._eventsBuffer[e.metricName].length=Math.floor(t/2))}},getBufferedEvents:function(e){var t=this._eventsBuffer,n={};return Object.keys(t).forEach((function(i){-1!==e.indexOf(i)&&(n[i]=t[i])})),n},clearEvents:function(e){this._eventsBuffer[e]&&(this._eventsBuffer[e].length=0)}};function f(){Ya.Rum.vsChanged=!0,Ya.Rum.vsChangeTime=a.getTime(),removeEventListener(\"visibilitychange\",f)}addEventListener(\"visibilitychange\",f),a._onVisibilityChange=f}(window);\n!function(){if(window.PerformanceLongTaskTiming){var e=function(e,n){return(e=e.concat(n)).length>300&&(e=e.slice(e.length-300)),e},n=\"undefined\"!=typeof PerformanceLongAnimationFrameTiming,t=n?[\"longtask\",\"long-animation-frame\"]:[\"longtask\"];function r(){var r=Ya.Rum._tti={events:[],loafEvents:n?[]:void 0,eventsAfterTTI:[],fired:!1,observer:new PerformanceObserver((function(t){var o=t.getEntriesByType(\"longtask\"),s=t.getEntriesByType(\"long-animation-frame\");r.events=e(r.events,o),n&&(r.loafEvents=e(r.loafEvents,s)),r.fired&&(r.eventsAfterTTI=e(r.eventsAfterTTI,o))}))};r.observer.observe({entryTypes:t}),Ya.Rum._unsubscribers&&Ya.Rum._unsubscribers.push((function(){r.observer.disconnect()}))}r(),Ya.Rum._onInit.push(r)}}();\nYa.Rum.observeDOMNode=window.IntersectionObserver?function(e,i,n){var t=this,o=Ya.Rum.getSetting(\"forcePaintTimeSending\");!function r(){if(o||!t.isVisibilityChanged()){var s=\"string\"==typeof i?document.querySelector(i):i;s?new IntersectionObserver((function(i,n){!o&&t.isVisibilityChanged()||(Ya.Rum.sendTimeMark(e),n.unobserve(s))}),n).observe(s):setTimeout(r,100)}}()}:function(){};\nvar rum_platform = window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop';\n    var rum_segment = window.location.pathname.replace(/^\\//, \"\").replace(/\\/.*/);\n    \n    if ([\"knowledge\", \"journal\", \"profile\", \"handbook\"].indexOf(rum_segment) === -1) {\n      rum_segment = \"portal\";\n    }\n\n    Ya.Rum.init({ beacon: true, clck: 'https://yandex.ru/clck/click', reqid: '1768064963563285-10555172254180365541'},\n    {\n        rum_id: 'ru.education.' + rum_platform + '.' + rum_segment,\n        '-env': 'production',\n        '-project': 'education-web',\n        '-page': window.location.pathname,\n        '-version': 'undefined',\n        '-platform': rum_platform\n    });Ya.Rum.observeDOMNode('2876', 'body');!function(){var e,t,n,i=Ya.Rum,o=42,r=4e4,g=15,a=[],s=\"\\r\\n\",l=i.getSetting(\"countersInitialDelay\")||0;if(l){var c;function u(){removeEventListener(\"visibilitychange\",h),clearTimeout(c),l=0,f()}function h(){document.hidden&&u()}c=setTimeout(u,l),addEventListener(\"visibilitychange\",h)}function f(){if(t&&a.length){for(var n=0,i=0,l=0;i<a.length&&l<=r&&n<o;i++)(l+=(i?s.length:0)+a[i].length)<=r&&n++;var c=a.splice(0,n);d(t,c.join(s)),a.length&&(e=setTimeout(f,g))}else a.length=0}function d(e,t){if(!(navigator.sendBeacon&&n&&navigator.sendBeacon(e,t))){var o=Boolean(i.getSetting(\"sendCookie\")),r=new XMLHttpRequest;r.open(\"POST\",e),r.withCredentials=o,r.send(t)}}i.send=function(c,u,h,d,v,m,S,p){t=i.getSetting(\"clck\"),n=i.getSetting(\"beacon\"),o=i.getSetting(\"maxBatchCounters\")||o,r=i.getSetting(\"maxBatchLength\")||r,g=i.getSetting(\"countersBatchTimeout\")||g,function(t,n,i,c,u,h,d,v,m,S){clearTimeout(e);var p=[t?\"/reqid=\"+t:\"\",n?\"/\"+n.join(\"/\"):\"\",i?\"/path=\"+i:\"\",c?\"/events=\"+c:\"\",u?\"/slots=\"+u.join(\";\"):\"\",h?\"/experiments=\"+h.join(\";\"):\"\",d?\"/vars=\"+d:\"\",\"/cts=\"+(new Date).getTime(),\"\",\"/*\"].join(\"\");p.length>r?\"undefined\"!=typeof console&&console.error&&console.error(\"Counter length \"+p.length+\" is more than allowed \"+r,p):(a.push(p),l||(function(){if(a.length>=o)return!0;for(var e=0,t=0;t<a.length;t++)if((e+=(t?s.length:0)+a[t].length)>=r)return!0;return!1}()?f():e=setTimeout(f,g)))}(i.getSetting(\"reqid\"),S,u,v,i.getSetting(\"slots\"),i.getSetting(\"experiments\"),h)}}();\n!function(){var e=Ya.Rum,n=!window.BigInt||!(\"PerformanceObserver\"in window);function t(n){e._unsubscribers.push(n)}function i(e,i,o){if(!n){var a=o||{};if(e){a.type=e,a.hasOwnProperty(\"buffered\")||(a.buffered=!0);var s=new PerformanceObserver((function(e,n){return i(e.getEntries(),n)}));return r((function(){try{s.observe(a)}catch(e){return void console.error(e.message)}t((function(){s.disconnect()}))}),0),s}throw new Error(\"PO without type field is forbidden\")}}function r(e,n){var i=setTimeout(e,n);return t((function(){clearInterval(i)})),i}function o(e,n,i){addEventListener(e,n,i),t((function(){removeEventListener(e,n,i)}))}function a(e,n,t){o(\"visibilitychange\",(function i(){if(\"hidden\"===document.visibilityState){try{t||(removeEventListener(\"visibilitychange\",i),e.disconnect())}catch(e){}n()}})),o(\"beforeunload\",n)}function s(e,n){return\"string\"==typeof e?encodeURIComponent(e):Math.round(1e3*(e-(n||0)))/1e3}function u(e){if(!e)return\"\";var n=(e.tagName||\"\").toLowerCase(),t=e.className&&void 0!==e.className.baseVal?e.className.baseVal:e.className;return n+(t?(\" \"+t).replace(/\\s+/g,\".\"):\"\")}function c(e){function n(){removeEventListener(\"DOMContentLoaded\",n),removeEventListener(\"load\",n),e()}\"loading\"===document.readyState?(o(\"DOMContentLoaded\",n),o(\"load\",n)):e()}function d(n){e._onComplete.push(n)}function f(){return e._periodicTasks}function l(){var n=e._vars;return Object.keys(n).map((function(e){return e+\"=\"+encodeURIComponent(n[e]).replace(/\\*/g,\"%2A\")}))}var m={connectEnd:2116,connectStart:2114,decodedBodySize:2886,domComplete:2124,domContentLoadedEventEnd:2131,domContentLoadedEventStart:2123,domInteractive:2770,domLoading:2769,domainLookupEnd:2113,domainLookupStart:2112,duration:2136,encodedBodySize:2887,entryType:2888,fetchStart:2111,initiatorType:2889,loadEventEnd:2126,loadEventStart:2125,nextHopProtocol:2890,redirectCount:1385,redirectEnd:2110,redirectStart:2109,requestStart:2117,responseEnd:2120,responseStart:2119,secureConnectionStart:2115,startTime:2322,transferSize:2323,type:76,unloadEventEnd:2128,unloadEventStart:2127,workerStart:2137},v=625;function g(n,t){Object.keys(m).forEach((function(e){if(e in t){var i=t[e];(i||0===i)&&n.push(m[e]+\"=\"+s(i))}})),n.push(\"\".concat(v,\"=\").concat(e.version))}var p,h,y,S,T,b=\"690.2096.2877\",w=\"690.2096.207\",E=\"690.2096.2044\",C=3,k=e.getSetting(\"savedDeltasLimit\")||0,L=document.createElement(\"link\"),P=window.performance||{},M=\"function\"==typeof P.getEntriesByType,_=0;function O(n,t,i,r,o){void 0===t&&(t=e.getTime()),void 0!==i&&!0!==i||e.mark(n,t);var a=I(n);if(a.push(\"207=\"+s(t)),x(a,r)){j(w,a,o&&o.force),p[n]=p[n]||[],p[n].push(t);var u=e._markListeners[n];u&&u.length&&u.forEach((function(e){e(t)})),e.emit({metricName:n,value:t,params:r})}}function I(n){return T.concat([e.isVisibilityChanged()?\"-vsChanged=1\":\"\",\"1701=\"+n,e.ajaxStart&&\"1201.2154=\"+s(e.ajaxStart),e.ajaxComplete&&\"1201.2052=\"+s(e.ajaxComplete)])}function N(){S=l(),e.getSetting(\"sendClientUa\")&&S.push(\"1042=\"+encodeURIComponent(navigator.userAgent))}function R(){var e=window.performance&&window.performance.timing&&window.performance.timing.navigationStart;T=S.concat([\"143.2129=\"+e])}function x(e,n){if(n){if(n.isCanceled&&n.isCanceled())return!1;var t=e.reduce((function(e,n,t){return\"string\"==typeof n&&(e[n.split(\"=\")[0]]=t),e}),{});Object.keys(n).forEach((function(i){if(\"function\"!=typeof n[i]){var r=t[i],o=i+\"=\"+n[i];void 0===r?e.push(o):e[r]=o}}))}return!0}function j(n,t,i){var r=encodeURIComponent(window.YaStaticRegion||\"unknown\");t.push(\"-cdn=\"+r);var o=t.filter(Boolean).join(\",\");e.send(null,n,o,void 0,void 0,void 0,void 0,i)}function z(e,n,t){j(e,F().concat(n),t)}function B(n,t){var i=y[n];i&&0!==i.length&&(i.push(e.getTime(),t),A(n))}function A(n,t,i,r){var o,a,u,c=y[n];if(void 0!==t?o=(a=r&&r.originalEndTime?r.originalEndTime:e.getTime())-t:c&&(o=c[0],a=c[1],u=c[2]),void 0!==o&&void 0!==a){var d=I(n);d.push(\"207.2154=\"+s(o),\"207.1428=\"+s(a),\"2877=\"+s(a-o)),x(d,i)&&x(d,u)&&(j(b,d,r&&r.force),_<k&&(h[n]=h[n]||[],h[n].push(a-o),_++),e.emit({metricName:n,value:a-o,params:{start:o,end:a}}),delete y[n])}}function V(e,n){if(!M)return n(null);L.href=e;var t=0,i=100,o=L.href;r((function e(){var a=P.getEntriesByName(o);if(a.length)return n(a);t++<C?(r(e,i),i+=i):n(null)}),0)}function U(e,n,t){V(n,(function(i){i&&D(e,i[i.length-1],n,t)}))}function D(n,t,i,r){var o=I(n);e.getSetting(\"sendUrlInResTiming\")&&o.push(\"13=\"+encodeURIComponent(i)),g(o,t),x(o,r),j(E,o)}function F(){return S}var W={bluetooth:2064,cellular:2065,ethernet:2066,none:1229,wifi:2067,wimax:2068,other:861,unknown:836,0:836,1:2066,2:2067,3:2070,4:2071,5:2768},H=navigator.connection;function Q(e){if(H){var n=W[H.type];e.push(\"2437=\"+(n||2771),void 0!==H.downlinkMax&&\"2439=\"+H.downlinkMax,H.effectiveType&&\"2870=\"+H.effectiveType,void 0!==H.rtt&&\"rtt=\"+H.rtt,void 0!==H.downlink&&\"dwl=\"+H.downlink,!n&&\"rawType=\"+H.type)}}var Y,q,G,J,$,K,X,Z,ee=\"690.2096.4004\",ne=!1,te=1/0,ie=1/0,re=(\"layout-shift\",Boolean(window.PerformanceObserver&&window.PerformanceObserver.supportedEntryTypes&&-1!==window.PerformanceObserver.supportedEntryTypes.indexOf(\"layout-shift\"))?0:null);function oe(){$>q&&(q=$,G=J,e.emit({metricName:\"cls-debug\",value:q,params:{clsEntries:G,target:ae(G)}}))}function ae(e){var n;if(!e)return null;var t=null;if((n=e.reduce((function(e,n){return e&&e.value>n.value?e:n})))&&n.sources&&n.sources.length){for(var i=0;i<n.sources.length;i++){var r=n.sources[i];if(r.node&&1===r.node.nodeType){t=r;break}}t=t||n.sources[0]}return t}function se(e){null==q&&(q=0);for(var n=0;n<e.length;n++){var t=e[n];t.hadRecentInput||($&&t.startTime-J[J.length-1].startTime<te&&t.startTime-J[0].startTime<ie?($+=t.value,J.push(t)):(oe(),$=t.value,J=[t]))}oe()}function ue(){q=re,Y=void 0,G=null,J=null,$=null,ne=!1}function ce(n){if(null!=q&&!ne){var t=Math.round(1e6*q)/1e6;if(Y!==t){Y=t,e.getSetting(\"enableContinuousCollection\")||(ne=!0);var i=ae(G),r=[\"s=\"+t];r.push(\"target=\"+u(i&&i.node));var o=e.getCLSAdditionalParams(i);o&&x(r,o),z(ee,r,n),e.emit({metricName:\"cls-debug\",value:q,params:{clsEntries:J,target:i,isFinalized:ne}})}}}function de(n){var t=n[n.length-1];K=t.renderTime||t.loadTime,X=t,e.emit({metricName:\"largest-contentful-paint-debug\",value:K,params:{entry:t}}),Z||(O(\"largest-loading-elem-paint\",K),Z=!0)}function fe(n){if(null!=K){var t=e.getLCPAdditionalParams(X);O(\"largest-contentful-paint\",K,!1,t,n&&{force:!0}),e.emit({metricName:\"largest-contentful-paint-debug\",value:K,params:{additionalParams:t,entry:X,isFinalized:!0}}),K=null,X=null}}e.getLCPAdditionalParams===e.noop&&(e.getLCPAdditionalParams=function(){var n={},t=X.element;if(t){n[\"-className\"]=e.getSelector(t),n[\"-tagName\"]=t.tagName.toLowerCase();var i=t.getBoundingClientRect();n[\"-width\"]=i.width,n[\"-height\"]=i.height}return X.size&&(n[\"-size\"]=X.size),n});var le={\"first-paint\":2793,\"first-contentful-paint\":2794},me=Object.keys(le).length,ve={},ge=window.performance||{},pe=\"function\"==typeof ge.getEntriesByType,he=0;function ye(){if(pe&&(e.getSetting(\"forcePaintTimeSending\")||!e.isVisibilityChanged()))for(var n=ge.getEntriesByType(\"paint\"),t=0;t<n.length;t++){var i=n[t],r=le[i.name];r&&!ve[i.name]&&(ve[i.name]=!0,he++,O(\"1926.\"+r,i.startTime))}}var Se=3e3,Te=1;function be(){return e._tti.events||[]}function we(){return e._tti.loafEvents}function Ee(){return e._tti}function Ce(n){return n?n===e.getPageUrl()?\"<page>\":n.replace(/\\?.*$/,\"\"):n}function ke(n,t,i){if(Ee()){var r=e.getTime(),o=\"undefined\"!=typeof PerformanceLongAnimationFrameTiming&&e.getSetting(\"sendLongAnimationFrames\");Le((function(a){var u,c={2796.2797:Pe(be(),t),689.2322:s(r)};if(o){var d=function(e){var n=we();if(n)return e?n.filter((function(n){return n.startTime+n.duration>=e})):n}(t);d&&(c[\"loaf.2797\"]=Pe(d,void 0,{useName:!1}),1===e.getSetting(\"longAnimationFramesMode\")&&(c[\"-additional\"]=encodeURIComponent(JSON.stringify({loaf:(u=d,u.map(Me))}))))}i&&Object.keys(i).forEach((function(e){c[e]=i[e]})),O(n||\"2795\",a,!0,c,{force:Boolean(o)}),e._tti.fired=!0}),t)}}function Le(n,t){var i=(arguments.length>2&&void 0!==arguments[2]?arguments[2]:{}).mode,o=void 0===i?Te:i;Ee()&&(t||(t=e.getTime()),function i(){var a,s=t,u=e.getTime(),c=o===Te?be():we()||[],d=c.length;0!==d&&(a=c[d-1],s=Math.max(s,Math.floor(a.startTime+a.duration))),u-s>=Se?n(s):r(i,1e3)}())}function Pe(e,n){var t=(arguments.length>2&&void 0!==arguments[2]?arguments[2]:{}).useName,i=void 0===t||t;return n=n||0,(e=e||[]).filter((function(e){return n-e.startTime<=50})).map((function(e){var n=Math.floor(e.startTime),t=Math.floor(n+e.duration);return i?(e.name?e.name.split(\"-\").map((function(e){return e[0]})).join(\"\"):\"u\")+\"-\"+n+\"-\"+t:n+\"-\"+t})).join(\".\")}function Me(e){var n=e.blockingDuration,t=e.duration,i=e.firstUIEventTimestamp,r=e.renderStart,o=e.scripts,a=e.startTime,s=e.styleAndLayoutStart;return[Math.round(a),Math.round(t),o.map(Oe),Math.round(n),Math.round(i),Math.round(r),Math.round(s)]}function _e(e){return{\"user-callback\":1,\"event-listener\":2,\"resolve-promise\":3,\"reject-promise\":4,\"classic-script\":5,\"module-script\":6}[e]||0}function Oe(e){var n=e.invoker,t=e.sourceURL,i=e.sourceFunctionName,r=e.sourceCharPosition,o=e.startTime,a=e.duration,s=e.windowAttribution,u=e.executionStart,c=e.forcedStyleAndLayoutDuration,d=e.pauseDuration,f=e.invokerType;return[Ce(n),Ce(t),i,r,Math.round(o),Math.round(a),s,Math.round(u),Math.round(c),Math.round(d),_e(f)]}var Ie=\"690.2096.361\",Ne=document.createElement(\"a\"),Re=0,xe={};function je(e){var n=e.transferSize;if(null!=n){Ne.href=e.name;var t=Ne.pathname;if(0!==t.indexOf(\"/clck\")){var i=t.lastIndexOf(\".\"),r=\"\";return-1!==i&&t.lastIndexOf(\"/\")<i&&t.length-i<=5&&(r=t.slice(i+1)),{size:n,domain:Ne.hostname,extension:r}}}}function ze(){var n=e.getSetting(\"maxTrafficCounters\")||250;if(Re>=n)return!1;for(var t=Object.keys(xe),i=\"\",r=0;r<t.length;r++){var o=t[r],a=xe[o];i+=encodeURIComponent(o)+\"!\"+a.count+\"!\"+a.size+\";\"}return i.length&&(Re++,z(Ie,[\"d=\"+i,\"t=\"+s(e.getTime())])),xe={},Re<n}d(ze);var Be=\"690.1033\",Ae={visible:1,hidden:2,prerender:3},Ve=window.performance||{},Ue=Ve.navigation||{},De=Ve.timing||{},Fe=De.navigationStart;function We(){var n=De.domContentLoadedEventStart,t=De.domContentLoadedEventEnd;if(0!==n||0!==t){var i=0===De.responseStart?Fe:De.responseStart,o=0===De.domainLookupStart?Fe:De.domainLookupStart,a=[\"2129=\"+Fe,\"1036=\"+(o-Fe),\"1037=\"+(De.domainLookupEnd-De.domainLookupStart),\"1038=\"+(De.connectEnd-De.connectStart),De.secureConnectionStart&&\"1383=\"+(De.connectEnd-De.secureConnectionStart),\"1039=\"+(De.responseStart-De.connectEnd),\"1040=\"+(De.responseEnd-i),\"1040.906=\"+(De.responseEnd-o),\"1310.2084=\"+(De.domLoading-i),\"1310.2085=\"+(De.domInteractive-i),\"1310.1309=\"+(t-n),\"1310.1007=\"+(n-i),navigator.deviceMemory&&\"3140=\"+navigator.deviceMemory,navigator.hardwareConcurrency&&\"3141=\"+navigator.hardwareConcurrency];Object.keys(m).forEach((function(e){e in De&&De[e]&&a.push(m[e]+\"=\"+s(De[e],Fe))})),e.vsStart?(a.push(\"1484=\"+(Ae[e.vsStart]||2771)),e.vsChanged&&a.push(\"1484.719=1\")):a.push(\"1484=\"+Ae.visible),Ue&&(Ue.redirectCount&&a.push(\"1384.1385=\"+Ue.redirectCount),1!==Ue.type&&2!==Ue.type||a.push(\"770.76=\"+Ue.type)),Q(a),z(Be,a)}else r(We,50)}var He=\"690.2096.2892\",Qe=window.performance||{},Ye=\"function\"==typeof Qe.getEntriesByType;if(!e)throw new Error(\"Rum: interface is not included\");if(e.enabled){function qe(){p={},h={},_=0,y=e._deltaMarks,N(),R(),e.ajaxStart=0,e.ajaxComplete=0,c(Je)}function Ge(){var n;e.sendTimeMark=O,e.sendResTiming=U,e.sendTiming=D,e.timeEnd=B;var t=(e.getBufferedEvents([\"defRes\"]).defRes||[]).map((function(e){return e.data}));for(n=0;n<t.length;n++)U(t[n][0],t[n][1]);e.clearEvents(\"defRes\");var i=(e.getBufferedEvents([\"defTimes\"]).defTimes||[]).map((function(e){return e.data}));for(n=0;n<i.length;n++)O(i[n][0],i[n][1],!1,i[n][2]);e.clearEvents(\"defTimes\"),Object.keys(y).forEach((function(e){A(e)}))}function Je(){var n=window.performance&&window.performance.timing&&window.performance.timing.navigationStart,t=e.getSetting(\"skipTiming\"),a=e.getSetting(\"techParamsByVisible\");n&&(a&&addEventListener(\"visibilitychange\",(function n(){\"visible\"!==e.vsStart?\"visible\"===document.visibilityState&&(e.vsStart=\"visible\",removeEventListener(\"visibilitychange\",n),We()):removeEventListener(\"visibilitychange\",n)})),r((function(){Ge(),(!t&&!a||a&&\"visible\"===e.vsStart)&&We(),e.getSetting(\"disableFCP\")||(ye(),he<me&&i(\"paint\",(function(e,n){ye(),n&&he>=me&&n.disconnect()}),{buffered:!0})),e.getSetting(\"sendAutoElementTiming\")&&(!window.PerformanceObserver||!e.getSetting(\"forcePaintTimeSending\")&&e.isVisibilityChanged()||i(\"element\",(function(e){for(var n=0;n<e.length;n++){var t=e[n];O(\"element-timing.\"+t.identifier,t.startTime)}}))),o(\"pageshow\",Xe),ke(),\"complete\"===document.readyState?$e({skipTimingApi:t}):o(\"load\",$e.bind(void 0,{skipTimingApi:t}))}),0))}function $e(n){var r,s;e.getSetting(\"disableOnLoadTasks\")||(removeEventListener(\"load\",$e),n.skipTimingApi||function(){if(Ye){var e=Qe.getEntriesByType(\"navigation\")[0];if(e){var n=[];g(n,e),Q(n);var t=Qe.getEntriesByName(\"yndxNavigationSource\")[0];t&&n.push(\"2091.186=\"+t.value);var i=Qe.getEntriesByName(\"yndxNavigationToken\",\"yndxEntry\")[0];i&&n.push(\"2091.3649=\"+i.value),z(He,n)}}}(),(s=e.getSetting(\"periodicStatsIntervalMs\"))||null===s||(s=15e3),s&&(r=setInterval(Ze,s),t((function(){clearInterval(r)})),Ke=r),o(\"beforeunload\",Ze),function(){if(window.PerformanceObserver){xe={},Re=0;var e=function(e){!function(e){if(e&&e.length)for(var n=xe,t=0;t<e.length;t++){var i=je(e[t]);if(i){var r=i.domain+\"-\"+i.extension,o=n[r]=n[r]||{count:0,size:0};o.count++,o.size+=i.size}}}(e)};i(\"resource\",e),i(\"navigation\",e),f().push(ze)}}(),e.getSetting(\"disableFID\")||i(\"first-input\",(function(n,t){var i=n[0];if(i){var r=i.processingStart,o={duration:i.duration,js:i.processingEnd-r,name:i.name};i.target&&(o.target=u(i.target));var a=r-i.startTime;A(\"first-input\",a,o),e.emit({metricName:\"first-input-debug\",value:a,params:{entry:i,additionalParams:o}}),t.disconnect()}}),{buffered:!0}),e.getSetting(\"disableCLS\")||window.PerformanceObserver&&(d(ce),d(ue),q=re,G=null,J=null,$=null,te=e.getSetting(\"clsWindowGap\")||te,ie=e.getSetting(\"clsWindowSize\")||ie,a(i(\"layout-shift\",se),(function(){return ce(!0)}),!0)),e.getSetting(\"disableLCP\")||!window.PerformanceObserver||!e.getSetting(\"forcePaintTimeSending\")&&e.isVisibilityChanged()||(d(fe),K=null,X=null,Z=!1,a(i(\"largest-contentful-paint\",de),(function(){return fe(!0)}),!1)))}var Ke;function Xe(e){e.persisted&&O(\"bfcache\")}function Ze(){var e=!1;f().forEach((function(n){n()&&(e=!0)})),e||clearInterval(Ke)}d(Ge),e.destroy=function(n){var t=e._unsubscribers;n.shouldComplete&&e.completeSession(!0),e._onComplete=[];for(var i=0;i<t.length;i++)t[i]();removeEventListener(\"visibilitychange\",e._onVisibilityChange),e._unsubscribers=[],e._periodicTasks=[],e._markListeners={},e._deltaMarks={}},e.restart=function(n,t,i){e.destroy({shouldComplete:i}),e.init(n,t),addEventListener(\"visibilitychange\",e._onVisibilityChange),qe(),function(){for(var n=0;n<e._onInit.length;n++)e._onInit[n]()}()},e.setVars=function(n){Object.keys(n).forEach((function(t){e._vars[t]=n[t]})),N(),R()},e.completeSession=function(n){for(var t=e._onComplete,i=0;i<t.length;i++)t[i](n)},qe(),e._periodicTasks=[],e.sendHeroElement=function(e){O(\"2876\",e)},e.getPageUrl=function(){return window.location.href},e._subpages={},e.makeSubPage=function(n,t){var i=e._subpages[n];e._subpages[n]=void 0===i?i=0:++i;var r=!1;return{689.2322:s(void 0!==t?t:e.getTime()),2924:n,2925:i,isCanceled:function(){return r},cancel:function(){r=!0}}},e.getTimeMarks=function(){return p},e.getDeltas=function(){return h},e.getVarsList=l,e.getResourceTimings=V,e.pushConnectionTypeTo=Q,e.pushTimingTo=g,e.normalize=s,e.sendCounter=j,e.sendDelta=A,e.onReady=c,e.getSelector=u,e.getSetting(\"disableCLS\")||(e.finalizeLayoutShiftScore=ce),e.getSetting(\"disableLCP\")||(e.finalizeLargestContentfulPaint=fe),e.sendTrafficData=ze,e._getCommonVars=F,e._addListener=o,e._observe=i,e._timeout=r,e.sendTTI=ke,e._getLongtasksStringValue=Pe,e.onQuietWindow=Le,e.sendBFCacheTimeMark=Xe}else e.getSetting=function(){return\"\"},e.getVarsList=function(){return[]},e.getResourceTimings=e.completeSession=e.pushConnectionTypeTo=e.pushTimingTo=e.normalize=e.sendCounter=e.destroy=e.restart=e.setVars=e.completeSession=e.sendDelta=e.sendTimeMark=e.sendResTiming=e.sendTiming=e.sendTTI=e.makeSubPage=e.sendHeroElement=e.onReady=e.onQuietWindow=function(){}}();\n!function(n){if(!n.Ya||!Ya.Rum)throw new Error(\"Rum: interface is not defined\");var e=Ya.Rum;e.getSetting=function(n){var t=e._settings[n];return null===t?null:t||\"\"}}(\"undefined\"!=typeof self?self:window);\n!function(e,r){var n={client:[\"690.2354\",1e3,100,0],uncaught:[\"690.2361\",100,10,0],external:[\"690.2854\",100,10,0],script:[\"690.2609\",100,10,0]},t={};r.ERROR_LEVEL={INFO:\"info\",DEBUG:\"debug\",WARN:\"warn\",ERROR:\"error\",FATAL:\"fatal\"},r._errorSettings={clck:\"https://yandex.ru/clck/click\",beacon:!0,project:\"unknown\",page:\"\",env:\"\",experiments:[],additional:{},platform:\"\",region:\"\",dc:\"\",host:\"\",service:\"\",level:\"\",version:\"\",yandexuid:\"\",loggedin:!1,coordinates_gp:\"\",referrer:!0,preventError:!1,unhandledRejection:!1,traceUnhandledRejection:!1,uncaughtException:!0,debug:!1,limits:{},silent:{},filters:{},pageMaxAge:864e6,initTimestamp:+new Date};var o=!1;function a(e,r){for(var n in r)r.hasOwnProperty(n)&&(e[n]=r[n]);return e}function i(e){return\"boolean\"==typeof e&&(e=+e),\"number\"==typeof e?e+\"\":null}r.initErrors=function(n){var t=a(r._errorSettings,n);o||(t.uncaughtException&&function(){var n=r._errorSettings;if(e.addEventListener)e.addEventListener(\"error\",s),n.resourceFails&&e.addEventListener(\"error\",l,!0),\"Promise\"in e&&n.unhandledRejection&&e.addEventListener(\"unhandledrejection\",function(e){var n,t,o=e.reason,a={};o&&(o.stack&&o.message?(n=o.message,t=o.stack):(n=String(o),t=r._parseTraceablePromiseStack(e.promise),\"[object Event]\"===n?n=\"event.type: \"+o.type:\"[object Object]\"===n&&(a.unhandledObject=o)),o.target&&o.target.src&&(a.src=o.target.src),s({message:\"Unhandled rejection: \"+n,stack:t,additional:a}))});else{var t=e.onerror;e.onerror=function(e,r,n,o,a){s({error:a||new Error(e||\"Empty error\"),message:e,lineno:n,colno:o,filename:r}),t&&t.apply(this,arguments)}}}(),t.unhandledRejection&&t.traceUnhandledRejection&&r._traceUnhandledRejection&&r._traceUnhandledRejection(),o=!0)},r.updateErrors=function(e){a(r._errorSettings,e)},r.updateAdditional=function(e){r._errorSettings.additional=a(r._errorSettings.additional||{},e)},r._handleError=function(e,o,i){var s,l,c=r._errorSettings;if(c.preventError&&e.preventDefault&&e.preventDefault(),o)s=e,l=\"client\";else{s=r._normalizeError(e),l=s.type;var d=c.onError;\"function\"==typeof d&&d(s);var u=c.transform;if(\"function\"==typeof u&&(s=u(s)),!s)return;s.settings&&(i=s.settings)}var g=+new Date,f=c.initTimestamp,p=c.pageMaxAge;if(!(-1!==p&&f&&f+p<g)){var m=n[l][1];\"number\"==typeof c.limits[l]&&(m=c.limits[l]);var v=n[l][2];\"number\"==typeof c.silent[l]&&(v=c.silent[l]);var h=n[l][3];if(h<m||-1===m){s.path=n[l][0];var E=r._getErrorData(s,{silent:h<v||-1===v?\"no\":\"yes\",isCustom:Boolean(o)},a(a({},c),i)),_=function(e){t[s.message]=!1,r._sendError(e.path,e.vars),n[l][3]++}.bind(this,E);if(void 0===c.throttleSend)_();else{if(t[s.message])return;t[s.message]=!0,setTimeout(_,c.throttleSend)}}}},r._getReferrer=function(r){var n=r.referrer,t=typeof n;return\"function\"===t?n():\"string\"===t&&n?n:!1!==n&&e.location?e.location.href:void 0},r.getErrorSetting=function(e){return r._errorSettings[e]},r._buildExperiments=function(e){return e instanceof Array?e.join(\";\"):\"\"},r._buildAdditional=function(e,r){var n=\"\";try{var t=a(a({},e),r);0!==Object.keys(t).length&&(n=JSON.stringify(t))}catch(e){}return n},r._getErrorData=function(n,t,o){t=t||{};var a=r._buildExperiments(o.experiments),s=r._buildAdditional(o.additional,n.additional),l={\"-stack\":n.stack,\"-url\":n.file,\"-line\":n.line,\"-col\":n.col,\"-block\":n.block,\"-method\":n.method,\"-msg\":n.message,\"-env\":o.env,\"-external\":n.external,\"-externalCustom\":n.externalCustom,\"-project\":o.project,\"-service\":n.service||o.service,\"-page\":n.page||o.page,\"-platform\":o.platform,\"-level\":n.level,\"-experiments\":a,\"-version\":o.version,\"-region\":o.region,\"-dc\":o.dc,\"-host\":o.host,\"-yandexuid\":o.yandexuid,\"-loggedin\":o.loggedin,\"-coordinates_gp\":n.coordinates_gp||o.coordinates_gp,\"-referrer\":r._getReferrer(o),\"-source\":n.source,\"-sourceMethod\":n.sourceMethod,\"-type\":t.isCustom?n.type:\"\",\"-additional\":s,\"-adb\":i(Ya.blocker)||i(o.blocker),\"-cdn\":e.YaStaticRegion,\"-ua\":navigator.userAgent,\"-silent\":t.silent,\"-ts\":+new Date,\"-init-ts\":o.initTimestamp};return o.debug&&e.console&&console[console[n.level]?n.level:\"error\"](\"[error-counter] \"+n.message,l,n.stack),{path:n.path,vars:l}},r._baseNormalizeError=function(e){var r=(e=e||{}).error,n=e.filename||e.fileName||\"\",t=r&&r.stack||e.stack||\"\",o=e.message||\"\",a=r&&r.additional||e.additional;return{file:n,line:e.lineno||e.lineNumber,col:e.colno||e.colNumber,stack:t,message:o,additional:a}},r._normalizeError=function(e){var n=r._baseNormalizeError(e),t=\"uncaught\",o=r._isExternalError(n.file,n.message,n.stack),a=\"\",i=\"\";return o.hasExternal?(t=\"external\",a=o.common,i=o.custom):/^Script error\\.?$/.test(n.message)&&(t=\"script\"),n.external=a,n.externalCustom=i,n.type=t,n},r._createVarsString=function(e){var r=[];for(var n in e)e.hasOwnProperty(n)&&(e[n]||0===e[n])&&r.push(n+\"=\"+encodeURIComponent(e[n]).replace(/\\*/g,\"%2A\"));return r.join(\",\")},r._sendError=function(e,n){r.send(null,e,r._createVarsString(n),null,null,null,null)};var s=function(e){r._handleError(e,!1)},l=function(e){var n=e.target;if(n){var t=n.srcset||n.src;if(t||(t=n.href),t){var o=n.tagName||\"UNKNOWN\";r.logError({message:o+\" load error\",additional:{src:t}})}}};r._parseTraceablePromiseStack=function(){}}(\"undefined\"!=typeof self?self:window,Ya.Rum);\n!function(e){var r={url:{0:/(miscellaneous|extension)_bindings/,1:/^chrome:/,2:/kaspersky-labs\\.com\\//,3:/^(?:moz|chrome|safari)-extension:\\/\\//,4:/^file:/,5:/^resource:\\/\\//,6:/webnetc\\.top/,7:/local\\.adguard\\.com/},message:{0:/__adgRemoveDirect/,1:/Content Security Policy/,2:/vid_mate_check/,3:/ucapi/,4:/Access is denied/i,5:/^Uncaught SecurityError/i,6:/__ybro/,7:/__show__deepen/,8:/ntp is not defined/,9:/Cannot set property 'install' of undefined/,10:/NS_ERROR/,11:/Error loading script/,12:/^TypeError: undefined is not a function$/,13:/__firefox__\\.(?:favicons|metadata|reader|searchQueryForField|searchLoginField)/},stack:{0:/(?:moz|chrome|safari)-extension:\\/\\//,1:/adguard.*\\.user\\.js/i}};function n(e,r){if(e&&r){var n=[];for(var o in r)if(r.hasOwnProperty(o)){var i=r[o];\"string\"==typeof i&&(i=new RegExp(i)),i instanceof RegExp&&i.test(e)&&n.push(o)}return n.join(\"_\")}}function o(e,o){var i,a=[];for(var t in r)r.hasOwnProperty(t)&&(i=n(e[t],o[t]))&&a.push(t+\"~\"+i);return a.join(\";\")}e._isExternalError=function(n,i,a){var t=e._errorSettings.filters||{},s={url:(n||\"\")+\"\",message:(i||\"\")+\"\",stack:(a||\"\")+\"\"},c=o(s,r),u=o(s,t);return{common:c,custom:u,hasExternal:!(!c&&!u)}}}(Ya.Rum);\nYa.Rum.initErrors({\n        reqid: '1768064963563285-10555172254180365541',\n        project: 'education-web',\n        env: 'production',\n        page: window.location.pathname,\n        version: 'undefined',\n        platform: window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop'\n    });","id":"rum-error"}])</script><script nonce="">(self.__next_s=self.__next_s||[]).push([0,{"nonce":"99442c83-224a-4e1c-a471-dfa207be1201","children":"\n  (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};\n  m[i].l=1*new Date();\n  for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}\n  k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})\n  (window, document, \"script\", \"https://mc.yandex.ru/metrika/tag.js\", \"ym\");\n","id":"yandex-metrika"}])</script><noscript><div><img loading="lazy" src="https://mc.yandex.ru/watch/26760489" style="position:absolute;left:-9999px" alt=""/></div></noscript><script src="https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/chunks/webpack-05f11ea662126375.js" nonce="" crossorigin="" id="_R_" async=""></script><script nonce="">(self.__next_f=self.__next_f||[]).push([0])</script><script nonce="">self.__next_f.push([1,"1:\"$Sreact.fragment\"\n3:I[78919,[],\"\"]\n4:I[74619,[],\"\"]\n1a:I[34733,[],\"\"]\n1d:I[7918,[],\"OutletBoundary\"]\n1f:I[33443,[],\"AsyncMetadataOutlet\"]\n21:I[7918,[],\"ViewportBoundary\"]\n23:I[7918,[],\"MetadataBoundary\"]\n24:\"$Sreact.suspense\"\n26:I[71311,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"\"]\n28:I[45866,[\"1666\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/layout-e3a20ad13303521f.js\"],\"UseLatexRuntime\"]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/0f6801932ea3fcf4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/305b936a915bc48f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/3fdc59da94114ecd-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/a853c69d3cf13b17-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/b4b0da158404816f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-"])</script><script nonce="">self.__next_f.push([1,"portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/c8ae0fac15b37b16-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/cc87cb16fedd6384-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/dd32e121f6104240-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/e10f0a1f1c5bddfe-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/f2f0493f5123f937-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/media/f3f9c83d0bcb2176-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"type\":\"font/woff2\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/00e25bd25bbd0437.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb46cd5c40721db6.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/85781da132a474d7.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]"])</script><script nonce="">self.__next_f.push([1,"\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/df0a82ffea795a48.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/c800df164b7b8c22.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/033fa3e38f305e9b.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/80e3798aa7f5abc0.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/991aec17f3093e7d.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5f16a3a32691d755.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dbd806e03d28c8cc.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5dfa285aa1529eec.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/051319dc975c787e.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/17fd"])</script><script nonce="">self.__next_f.push([1,"f933d7378e89.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a2f4cf3f3bb12d85.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d844ab516aaccc48.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb0e02f100b08559.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/381835674ab7c3f7.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/8268a37c4890f71e.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/e2bd809ab2e171e9.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/84f1d4eada6960dd.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cee7c32dde4c9323.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ef486034b69d75eb.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/"])</script><script nonce="">self.__next_f.push([1,"education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/4453bad77d6636b4.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dfbd22c99f7f399e.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5c966d7ef241f1d1.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b1ea991da29cd10f.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cb7ec9243c9cb1bf.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ca947b8fe9734df5.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a412a0891b49ea16.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ddb8c975b9776637.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9425ba6f0e81e31a.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/feca8288b0586eb3.css\",\"style\",{\"cr"])</script><script nonce="">self.__next_f.push([1,"ossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d238d176bd8ae895.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9fc77bcede8e7238.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/daf421f1dcbde59d.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/bd270492e7e016dc.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/500049d7c719a090.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/3e9ccf27ab1f71e9.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b6dc735f8f16428c.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n:HL[\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/094d7d6ad311c845.css\",\"style\",{\"crossOrigin\":\"\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n"])</script><script nonce="">self.__next_f.push([1,"0:{\"P\":null,\"b\":\"xZcdS5roGxWU4G4BMA8rx\",\"p\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e\",\"c\":[\"\",\"handbook\",\"math\",\"article\",\"math-glava-chetire-chemu-vi-nauchilis\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"handbook\",{\"children\":[[\"book\",\"math\",\"d\"],{\"children\":[\"article\",{\"children\":[[\"article\",\"math-glava-chetire-chemu-vi-nauchilis\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/00e25bd25bbd0437.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb46cd5c40721db6.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/85781da132a474d7.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/df0a82ffea795a48.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/c800df164b7b8c22.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/033fa3e38f305e9b.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"6\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/80e3798aa7f5abc0.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]],\"$L2\"]}],{\"children\":[\"handbook\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"book\",\"math\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/991aec17f3093e7d.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5f16a3a32691d755.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dbd806e03d28c8cc.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5dfa285aa1529eec.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/051319dc975c787e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],\"$L5\",\"$L6\",\"$L7\",\"$L8\",\"$L9\",\"$La\",\"$Lb\",\"$Lc\",\"$Ld\",\"$Le\",\"$Lf\",\"$L10\",\"$L11\",\"$L12\",\"$L13\",\"$L14\"],\"$L15\"]}],{\"children\":[\"article\",\"$L16\",{\"children\":[[\"article\",\"math-glava-chetire-chemu-vi-nauchilis\",\"d\"],\"$L17\",{\"children\":[\"__PAGE__\",\"$L18\",{},null,false]},null,false]},null,false]},null,false]},null,false]},null,false],\"$L19\",false]],\"m\":\"$undefined\",\"G\":[\"$1a\",[]],\"s\":false,\"S\":false}\n"])</script><script nonce="">self.__next_f.push([1,"5:[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/17fdf933d7378e89.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n6:[\"$\",\"link\",\"6\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a2f4cf3f3bb12d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n7:[\"$\",\"link\",\"7\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d844ab516aaccc48.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n8:[\"$\",\"link\",\"8\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb0e02f100b08559.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n9:[\"$\",\"link\",\"9\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/381835674ab7c3f7.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\na:[\"$\",\"link\",\"10\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/8268a37c4890f71e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\nb:[\"$\",\"link\",\"11\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/e2bd809ab2e171e9.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\nc:[\"$\",\"link\",\"12\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00"])</script><script nonce="">self.__next_f.push([1,"b204e9800998ecf8427e/_next/static/css/84f1d4eada6960dd.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\nd:[\"$\",\"link\",\"13\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cee7c32dde4c9323.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\ne:[\"$\",\"link\",\"14\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ef486034b69d75eb.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\nf:[\"$\",\"link\",\"15\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/4453bad77d6636b4.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n10:[\"$\",\"link\",\"16\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dfbd22c99f7f399e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n11:[\"$\",\"link\",\"17\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5c966d7ef241f1d1.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n12:[\"$\",\"link\",\"18\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b1ea991da29cd10f.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n13:[\"$\",\"link\",\"19\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cb7ec9243c9cb1bf.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce"])</script><script nonce="">self.__next_f.push([1,"\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n14:[\"$\",\"link\",\"20\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ca947b8fe9734df5.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n16:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\n17:[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L1b\"]}]\n"])</script><script nonce="">self.__next_f.push([1,"18:[\"$\",\"$1\",\"c\",{\"children\":[\"$L1c\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a412a0891b49ea16.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ddb8c975b9776637.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9425ba6f0e81e31a.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/feca8288b0586eb3.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d238d176bd8ae895.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/9fc77bcede8e7238.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"6\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/daf421f1dcbde59d.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"7\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/bd270492e7e016dc.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"8\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/500049d7c719a090.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"9\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/3e9ccf27ab1f71e9.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"10\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b6dc735f8f16428c.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"11\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/094d7d6ad311c845.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]],[\"$\",\"$L1d\",null,{\"children\":[\"$L1e\",[\"$\",\"$L1f\",null,{\"promise\":\"$@20\"}]]}]]}]\n"])</script><script nonce="">self.__next_f.push([1,"19:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L21\",null,{\"children\":\"$L22\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L23\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$24\",null,{\"fallback\":null,\"children\":\"$L25\"}]}]}]]}]\n1b:[[\"$\",\"$L26\",null,{\"src\":\"https://yastatic.net/s3/cloud/forms/_/embed.js\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],\"$@27\",[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L28\",null,{}]]\n27:null\n22:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"#a48eef\"}]]\n1e:null\n"])</script><script nonce="">self.__next_f.push([1,"15:[\"$L29\",[\"$\",\"div\",null,{\"className\":\"hb\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],\"$L2a\"]\n"])</script><script nonce="">self.__next_f.push([1,"2c:I[75076,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"FooterPlain\"]\n29:[[\"$\",\"$1\",\"web-page.block-0\",{\"children\":[\"$L2b\"]}]]\n"])</script><script nonce="">self.__next_f.push([1,"2a:[[\"$\",\"$1\",\"web-page.block-0\",{\"children\":[[\"$\",\"$L2c\",\"web-page.footer-plain\",{\"id\":2,\"Anchor\":null,\"Theme\":\"#FFFFFF\",\"Disclaimer\":\"Образовательные услуги оказываются АНО ДПО «Образовательные технологии Яндекса»\\nна\u0026nbsp;основании [Лицензии № Л035-01298-77/00185314](https://yastatic.net/s3/academy/docs/license-ysda.pdf) от 24 марта 2015 года.\\n\\n© 2025 Яндекс, АНО ДПО «Образовательные технологии Яндекса».\",\"Columns\":[{\"id\":8,\"Name\":null,\"Links\":[{\"id\":4460,\"Text\":\"Яндекс Учебник\",\"URL\":\"https://education.yandex.ru/uchebnik/main\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4458,\"Text\":\"Яндекс Лицей\",\"URL\":\"https://lyceum.yandex.ru/\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4459,\"Text\":\"Яндекс Практикум\",\"URL\":\"https://practicum.yandex.ru/?utm_source=partners\u0026utm_medium=partners\u0026utm_campaign=yandexeducation_partners_RF_Common_Unde_b2c_Landing-page_None_None\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4461,\"Text\":\"Школа анализа данных\",\"URL\":\"https://shad.yandex.ru/\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4463,\"Text\":\"Программы в университетах\",\"URL\":\"/university\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null}]},{\"id\":9,\"Name\":null,\"Links\":[{\"id\":4462,\"Text\":\"Исследования\",\"URL\":\"/research\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4464,\"Text\":\"Хендбуки\",\"URL\":\"/handbook\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":5881,\"Text\":\"Карты IT-навыков\",\"URL\":\"/roadmap\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4466,\"Text\":\"База знаний\",\"URL\":\"/knowledge\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4465,\"Text\":\"Журнал\",\"URL\":\"/journal\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4467,\"Text\":\"События\",\"URL\":\"/events\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null}]},{\"id\":10,\"Name\":null,\"Links\":[{\"id\":4468,\"Text\":\"О нас\",\"URL\":\"/about\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4469,\"Text\":\"Обратная связь\",\"URL\":\"https://forms.yandex.ru/surveys/13457493.e7112b8cdd8c782bfe6e4b1ab1b73f49438edacf/\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4470,\"Text\":\"Пользовательское соглашение\",\"URL\":\"https://yandex.ru/legal/education_termsofuse/ru/\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4471,\"Text\":\"Сайт образовательной организации\",\"URL\":\"https://yandex.ru/edtech\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4472,\"Text\":\"Сведения об образовательной организации\",\"URL\":\"https://yandex.ru/edtech/sveden\",\"Title\":null,\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null}]}],\"Socials\":[{\"id\":4454,\"Text\":\"Рассылка\",\"URL\":\"/subscribe\",\"Title\":\"Рассылка\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null,\"Image\":{\"data\":null}},{\"id\":4473,\"Text\":\"Бот\",\"URL\":\"https://t.me/yaeducation_bot?start=n_113083__c_7628\",\"Title\":\"Бот\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null,\"Image\":{\"data\":null}},{\"id\":4455,\"Text\":\"ВКонтакте\",\"URL\":\"https://vk.com/yandex_education\",\"Title\":\"ВКонтакте\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null,\"Image\":{\"data\":{\"id\":5,\"attributes\":{\"name\":\"social_icon_vk.svg\",\"alternativeText\":\"\",\"caption\":\"\",\"width\":40,\"height\":40,\"formats\":null,\"hash\":\"social_icon_vk_97bf858cd5\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":0.49,\"url\":\"https://yastatic.net/s3/education-portal/media/social_icon_vk_97bf858cd5.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-01T08:03:45.007Z\",\"updatedAt\":\"2024-02-09T11:21:16.497Z\"}}}},{\"id\":4456,\"Text\":\"YouTube\",\"URL\":\"https://www.youtube.com/@Education_Yandex\",\"Title\":\"YouTube\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null,\"Image\":{\"data\":{\"id\":6,\"attributes\":{\"name\":\"social_icon_yt.svg\",\"alternativeText\":\"\",\"caption\":\"\",\"width\":40,\"height\":40,\"formats\":null,\"hash\":\"social_icon_yt_d20daea655\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":0.77,\"url\":\"https://yastatic.net/s3/education-portal/media/social_icon_yt_d20daea655.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-01T08:03:45.307Z\",\"updatedAt\":\"2024-02-01T08:03:45.307Z\"}}}},{\"id\":4457,\"Text\":\"Telegram\",\"URL\":\"https://t.me/education_yandex\",\"Title\":\"Telegram\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null,\"Image\":{\"data\":{\"id\":7,\"attributes\":{\"name\":\"social_icon_tg.svg\",\"alternativeText\":\"\",\"caption\":\"\",\"width\":40,\"height\":40,\"formats\":null,\"hash\":\"social_icon_tg_9faafb663e\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":1.04,\"url\":\"https://yastatic.net/s3/education-portal/media/social_icon_tg_9faafb663e.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-01T08:03:45.537Z\",\"updatedAt\":\"2024-02-01T08:03:45.537Z\"}}}}],\"Gutter\":{\"id\":569,\"xs\":0,\"sm\":0,\"md\":0,\"lg\":0,\"xl\":0},\"Logo\":{\"id\":77,\"URL\":\"/\",\"Title\":\"Яндекс Образование\",\"Image\":{\"data\":{\"id\":8,\"attributes\":{\"name\":\"edu_logo.svg\",\"alternativeText\":null,\"caption\":null,\"width\":233,\"height\":24,\"formats\":null,\"hash\":\"edu_logo_9eee76ff17\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":13.44,\"url\":\"https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-09T11:00:59.235Z\",\"updatedAt\":\"2024-12-04T04:36:42.717Z\"}}},\"ImageMobile\":{\"data\":{\"id\":3190,\"attributes\":{\"name\":\"edu-logo-simple.svg\",\"alternativeText\":\"edu-logo-simple.svg\",\"caption\":\"edu-logo-simple.svg\",\"width\":204,\"height\":21,\"formats\":null,\"hash\":\"edu_logo_simple_9790e70002\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":7.25,\"url\":\"https://yastatic.net/s3/education-portal/media/edu_logo_simple_9790e70002.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-20T11:27:57.767Z\",\"updatedAt\":\"2024-12-04T12:30:44.339Z\"}}}}}]]}]]\n"])</script><script nonce="">self.__next_f.push([1,"2d:I[93982,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"default\"]\n"])</script><script nonce="">self.__next_f.push([1,"2b:[\"$\",\"$L2d\",\"web-page.header-block\",{\"id\":2,\"Authorization\":true,\"Search\":true,\"ProfileUrl\":\"/profile\",\"Wide\":null,\"Notifications\":true,\"Gutter\":null,\"Buttons\":[],\"Menu\":[{\"id\":19,\"Text\":\"Школьникам\",\"URL\":\"/pupils\",\"Title\":\"Школьникам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_school\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":null,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4136,\"Text\":\"Студентам\",\"URL\":\"/students\",\"Title\":\"Студентам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_students\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4137,\"Text\":\"Абитуриентам\",\"URL\":\"/university\",\"Title\":\"Абитуриентам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_abit\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4138,\"Text\":\"Партнёрам\",\"URL\":\"https://edumakers.yandex.ru/\",\"Title\":\"Партнёрам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_partner\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4139,\"Text\":\"Хендбуки\",\"URL\":\"/handbook\",\"Title\":\"События\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4140,\"Text\":\"Журнал\",\"URL\":\"/journal\",\"Title\":\"Журнал\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_journal\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4141,\"Text\":\"AI\",\"URL\":\"https://education.yandex.ru/ai\",\"Title\":\"AI\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null}],\"Logo\":{\"id\":3,\"URL\":\"/\",\"Title\":\"Яндекс Образование\",\"Image\":{\"data\":{\"id\":8,\"attributes\":{\"name\":\"edu_logo.svg\",\"alternativeText\":null,\"caption\":null,\"width\":233,\"height\":24,\"formats\":null,\"hash\":\"edu_logo_9eee76ff17\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":13.44,\"url\":\"https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-09T11:00:59.235Z\",\"updatedAt\":\"2024-12-04T04:36:42.717Z\"}}},\"ImageMobile\":{\"data\":{\"id\":10764,\"attributes\":{\"name\":\"logo_mobile.svg\",\"alternativeText\":null,\"caption\":null,\"width\":179,\"height\":25,\"formats\":null,\"hash\":\"logo_mobile_8bc5eb38fb\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":18.25,\"url\":\"https://yastatic.net/s3/education-portal/media/logo_mobile_8bc5eb38fb.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-07-17T12:53:33.633Z\",\"updatedAt\":\"2024-12-04T04:35:29.061Z\"}}}},\"isLoggedIn\":false,\"passportUrl\":\"https://passport.yandex.ru/auth/?origin=education\u0026retpath=https%3A%2F%2Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis\",\"passportEnv\":\"prod\",\"passportUserData\":{\"avatarId\":\"$undefined\",\"plus\":false,\"name\":\"$undefined\"}}]\n"])</script><script nonce="">self.__next_f.push([1,"2e:T1248,"])</script><script nonce="">self.__next_f.push([1,"\n  function applyUADataPolyfill(){\n    function e(a){let b=/Windows NT (\\d+(\\.\\d+)*)/.exec(a);if(!b)return null;let c={\"6.1\":\"0.1\",\"6.2\":\"0.2\",\"6.3\":\"0.3\",\"10.0\":\"10.0\",\"11.0\":\"13.0\"}[b[1]],d=c?q(c,3):\"\",e=\"\",f=\"\";return/\\b(WOW64|Win64|x64)\\b/.test(a)\u0026\u0026(e=\"x86\",f=\"64\"),{platform:\"Windows\",platformVersion:d,architecture:e,bitness:f}}\n    function f(a,b){let c=/Android (\\d+(\\.\\d+)*)/.exec(a);if(!c)return null;let d=\"\",e=\"\",f=/Linux (\\w+)/.exec(b);if(f\u0026\u0026f[1]){let a=p(f[1]);d=a[0],e=a[1]}return{platform:\"Android\",platformVersion:q(c[1]),architecture:d,bitness:e}}\n    function g(a){let b=/(iPhone|iPod touch); CPU iPhone OS (\\d+(_\\d+)*)/.exec(a),c=/(iPad); CPU OS (\\d+(_\\d+)*)/.exec(a),d=b||c;return d?{platform:\"iOS\",platformVersion:q(d[2].replace(/_/g,\".\"))}:null}\n    function h(a){let b=/Macintosh; (Intel|\\w+) Mac OS X (\\d+([_.]\\d+)*)/.exec(a);return b?{platform:\"macOS\",platformVersion:q(b[2].replace(/_/g,\".\"))}:null}\n    function i(a){let b=/CrOS (\\w+) (\\d+(\\.\\d+)*)/.exec(a);if(!b)return null;let c=p(b[1]);return{platform:\"Chrome OS\",platformVersion:q(b[2]),architecture:c[0],bitness:c[1]}}\n    function j(a,b){for(let c of[()=\u003ee(a),()=\u003ef(a,b),()=\u003eg(a),()=\u003eh(a),()=\u003ei(a)]){let a=c();if(a)return a}return/Linux/.test(a)?{platform:\"Linux\",platformVersion:\"\"}:{platform:\"Unknown\",platformVersion:\"\"}}\n    function k(a,b,c){let d=/Chrome\\/(\\d+(\\.\\d+)*)/.exec(a);if(!d||\"Google Inc.\"!==c)return null;let e=[{brand:\"Chromium\",version:q(d[1],4)}],f=/(Edge?)\\/(\\d+(\\.\\d+)*)/.exec(a);if(f){let a={Edge:\"Microsoft Edge\",Edg:\"Microsoft Edge\"}[f[1]];e.push({brand:a,version:q(f[2],4)})}else e.push({brand:\"Google Chrome\",version:q(d[1],4)});return e}\n    function l(a,b){let c=/AppleWebKit\\/(\\d+(\\.\\d+)*)/.exec(a);return c\u0026\u0026\"Apple Computer, Inc.\"===b?[{brand:\"WebKit\",version:q(c[1])}]:null}\n    function m(a){let b=/Firefox\\/(\\d+(\\.\\d+)*)/.exec(a);return b?[{brand:\"Firefox\",version:q(b[1])}]:null}\n    function n(a,b,c,d,e){let f=!1,g=[];for(let h of[()=\u003ek(a,b,c),()=\u003el(a,c),()=\u003em(a)]){let c=h();if(c){g=c,h===k\u0026\u0026/\\bwv\\b/.test(b)\u0026\u0026(f=!0);let i=/(CriOS|EdgiOS|FxiOS|Version)\\/(\\d+(\\.\\d+)*)/.exec(a);if(h===l\u0026\u0026\"iOS\"===d\u0026\u0026i){let a={CriOS:\"Google Chrome\",EdgiOS:\"Microsoft Edge\",FxiOS:\"Mozilla Firefox\",Version:\"Apple Safari\"}[i[1]];g.push({brand:a,version:q(i[2])}),e\u0026\u0026!e.some(a=\u003ea.startsWith(\"Safari/\"))\u0026\u0026(f=!0)}break}}return 0===g.length\u0026\u0026(g=[{brand:\"Not;A Brand\",version:\"99.0.0.0\"}]),{fullVersionList:g,webview:f}}\n    function o(a){let{userAgent:b,platform:c,vendor:d}=a,e=b,f=!1,g=b.replace(/\\(([^)]+)\\)/g,(a,b)=\u003e(f||(e=b,f=!0),\"\")),h=g.match(/(\\S+)\\/(\\S+)/g),i=b.includes(\"Mobile\"),k=j(e,c),{fullVersionList:l,webview:m}=n(g,e,d,k.platform,h),o=l.length\u003e0?l[l.length-1].version:\"\",p=l.map(a=\u003e{let b=a.version.indexOf(\".\"),c=-1===b?a.version:a.version.slice(0,b);return{brand:a.brand,version:c}});return{mobile:i,platform:k.platform,brands:p,platformVersion:k.platformVersion,architecture:k.architecture||\"\",bitness:k.bitness||\"\",model:\"\",uaFullVersion:o,fullVersionList:l,webview:m}}\n    function p(a){switch(a){case\"x86_64\":case\"x64\":return[\"x86\",\"64\"];case\"x86_32\":case\"x86\":return[\"x86\",\"\"];case\"armv6l\":case\"armv7l\":case\"armv8l\":return[a,\"\"];case\"aarch64\":return[\"arm\",\"64\"];default:return[\"\",\"\"]}}\n    function q(a,b=3){let c=a.split(\".\");if(c.length\u003cb)for(;c.length\u003cb;)c.push(\"0\");return c.join(\".\")}\n    class r{constructor(a){this._clientHints=o(a),Object.defineProperties(this,{_clientHints:{enumerable:!1}})}get mobile(){return this._clientHints.mobile}get platform(){return this._clientHints.platform}get brands(){return this._clientHints.brands}getHighEntropyValues(a){return new Promise(b=\u003e{if(!Array.isArray(a))throw TypeError(\"Argument hints must be an array\");let c=new Set(a),d=this._clientHints,e={mobile:d.mobile,platform:d.platform,brands:d.brands};c.has(\"architecture\")\u0026\u0026(e.architecture=d.architecture),c.has(\"bitness\")\u0026\u0026(e.bitness=d.bitness),c.has(\"model\")\u0026\u0026(e.model=d.model),c.has(\"platformVersion\")\u0026\u0026(e.platformVersion=d.platformVersion),c.has(\"uaFullVersion\")\u0026\u0026(e.uaFullVersion=d.uaFullVersion),c.has(\"fullVersionList\")\u0026\u0026(e.fullVersionList=d.fullVersionList),b(e)})}toJSON(){return{mobile:this._clientHints.mobile,brands:this._clientHints.brands}}}\n    function(){if(\"https:\"===location.protocol\u0026\u0026!navigator.userAgentData){Object.defineProperty(r.prototype,Symbol.toStringTag,{enumerable:!1,configurable:!0,writable:!1,value:\"NavigatorUAData\"});let a=new r(navigator);return Object.defineProperty(Navigator.prototype,\"userAgentData\",{enumerable:!0,configurable:!0,get:function(){return a}}),Object.defineProperty(window,\"NavigatorUAData\",{enumerable:!1,configurable:!0,writable:!0,value:r}),!0}return!1}\n    polyfill();\n  }\n  applyUADataPolyfill();\n"])</script><script nonce="">self.__next_f.push([1,"2:[\"$\",\"html\",null,{\"lang\":\"ru\",\"className\":\"__className_a39d3e __variable_a39d3e __variable_5a49b6 __variable_2ac229 HR-9-31-0\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"https://yastatic.net/s3/education-portal/web/favicon.ico\",\"sizes\":\"any\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"https://yastatic.net/s3/education-portal/web/icon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"href\":\"https://yastatic.net/s3/education-portal/web/apple-touch-icon.png\"}],[\"$\",\"script\",null,{\"id\":\"ab-test-data\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"dangerouslySetInnerHTML\":{\"__html\":\"window.__AB_TEST_DATA = {\\\"flags\\\":{},\\\"experiments\\\":\\\"P54lnS9LcLo,\\\"};\"},\"suppressHydrationWarning\":true}]]}],[\"$\",\"body\",null,{\"className\":\"Theme Theme_color_hrLight Theme_root_hrLight utilityfocus\",\"children\":[[[\"$\",\"script\",null,{\"id\":\"hasOwnPolyfill\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"suppressHydrationWarning\":true,\"children\":\"('hasOwn' in Object) || (Object.hasOwn = Object.call.bind(Object.hasOwnProperty));\"}],[\"$\",\"script\",null,{\"id\":\"UserAGentUADataPolyfill\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"suppressHydrationWarning\":true,\"children\":\"$2e\"}]],\"$L2f\",\"$L30\",\"$L31\",\"$L32\",\"$L33\"]}]]}]\n"])</script><script nonce="">self.__next_f.push([1,"34:I[87609,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"ErrorBoundary\"]\n35:I[46262,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"ToastContainer\"]\n36:I[85137,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"Theme\"]\n37:I[12665,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"2665\",\"static/chunks/2665-c77f7b9517a535fe.js\",\"8039\",\"static/chunks/app/error-da5ebf8e69abf3a6.js\"],\"default\"]\n46:I[79949,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"PortalMetrika\"]\n47:I[46678,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"YandexMetrikaCounter\"]\n4a:I[15552,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"ScrollToTopOnPathnameChange\"]\n4b:I[78705,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"7177\",\"static/chunks/app/layout-e78d4be001a57a28.js\"],\"SW\"]\n2f:[\"$\",\"$L34\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L35\",null,{\"containerId\":\"defaultToastContainer\",\"enableMultiContainer\":true}],[\"$\",\"$L35\",null,{\"containerId\":\"noCloseButtonToastContainer\",\"closeButton\":false,\"enableMultiContain"])</script><script nonce="">self.__next_f.push([1,"er\":true}]]}]\n30:[\"$\",\"$L36\",null,{}]\n"])</script><script nonce="">self.__next_f.push([1,"31:[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$37\",\"errorStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5dfa285aa1529eec.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/991aec17f3093e7d.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5f16a3a32691d755.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dbd806e03d28c8cc.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/051319dc975c787e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]],\"errorScripts\":[],\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L38\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/991aec17f3093e7d.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5f16a3a32691d755.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dbd806e03d28c8cc.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5dfa285aa1529eec.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/051319dc975c787e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/17fdf933d7378e89.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"6\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/a2f4cf3f3bb12d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"7\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/d844ab516aaccc48.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],[\"$\",\"link\",\"8\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/fb0e02f100b08559.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}],\"$L39\",\"$L3a\",\"$L3b\",\"$L3c\",\"$L3d\",\"$L3e\",\"$L3f\",\"$L40\",\"$L41\",\"$L42\",\"$L43\"]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]\n"])</script><script nonce="">self.__next_f.push([1,"32:[\"$\",\"$L34\",null,{\"fallback\":null,\"children\":[\"$L44\",[\"$L45\",[\"$\",\"$L46\",null,{}],[\"$\",\"$L47\",null,{\"id\":26760489,\"settings\":\"defer=1,webvisor=1,hitOnInit=1,fullUrl=1\",\"firstPartyParams\":{},\"experiments\":\"P54lnS9LcLo,\"}],\"$undefined\",\"$L48\",\"$L49\"]]}]\n33:[[\"$\",\"$L34\",null,{\"fallback\":null,\"children\":[\"$\",\"$L4a\",null,{}]}],[\"$\",\"$L34\",null,{\"fallback\":null,\"children\":[\"$\",\"$L4b\",null,{}]}]]\n"])</script><script nonce="">self.__next_f.push([1,"39:[\"$\",\"link\",\"9\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/381835674ab7c3f7.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3a:[\"$\",\"link\",\"10\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/8268a37c4890f71e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3b:[\"$\",\"link\",\"11\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/e2bd809ab2e171e9.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3c:[\"$\",\"link\",\"12\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/84f1d4eada6960dd.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3d:[\"$\",\"link\",\"13\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cee7c32dde4c9323.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3e:[\"$\",\"link\",\"14\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/ef486034b69d75eb.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n3f:[\"$\",\"link\",\"15\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/4453bad77d6636b4.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n40:[\"$\",\"link\",\"16\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-"])</script><script nonce="">self.__next_f.push([1,"d41d8cd98f00b204e9800998ecf8427e/_next/static/css/dfbd22c99f7f399e.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n41:[\"$\",\"link\",\"17\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/b1ea991da29cd10f.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n42:[\"$\",\"link\",\"18\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/cb7ec9243c9cb1bf.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n43:[\"$\",\"link\",\"19\",{\"rel\":\"stylesheet\",\"href\":\"https://yastatic.net/s3/education-portal/web/3250da4636185ea5-d41d8cd98f00b204e9800998ecf8427e/_next/static/css/5c966d7ef241f1d1.css\",\"precedence\":\"next\",\"crossOrigin\":\"anonymous\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\"}]\n45:[\"$\",\"$L26\",null,{\"id\":\"yandex-metrika\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"strategy\":\"beforeInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"\\n  (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};\\n  m[i].l=1*new Date();\\n  for (var j = 0; j \u003c document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}\\n  k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})\\n  (window, document, \\\"script\\\", \\\"https://mc.yandex.ru/metrika/tag.js\\\", \\\"ym\\\");\\n\"}}]\n48:null\n49:null\n4c:T74ea,"])</script><script nonce="">self.__next_f.push([1,"!function(e,t){if(e.Ya=e.Ya||{},Ya.Rum)throw new Error(\"Rum: interface is already defined\");var n=e.performance,i=n\u0026\u0026n.timing\u0026\u0026n.timing.navigationStart||Ya.startPageLoad||+new Date,s=e.requestAnimationFrame,r=function(){},a=Ya.Rum={_defTimes:[],_defRes:[],_countersToExposeAsEvents:[\"2325\",\"2616.85.1928\",\"react.inited\"],enabled:!!n,version:\"6.1.21\",vsStart:document.visibilityState,vsChanged:!1,vsChangeTime:1/0,_deltaMarks:{},_markListeners:{},_onComplete:[],_onInit:[],_unsubscribers:[],_eventLisneters:{},_settings:{},_vars:{},init:function(e,t){a._settings=e,a._vars=t},getTime:n\u0026\u0026n.now?function(){return n.now()}:Date.now?function(){return Date.now()-i}:function(){return new Date-i},time:function(e){a._deltaMarks[e]=[a.getTime()]},timeEnd:function(e,t){var n=a._deltaMarks[e];n\u0026\u00260!==n.length\u0026\u0026n.push(a.getTime(),t)},sendTimeMark:function(e,t,n,i){void 0===t\u0026\u0026(t=a.getTime()),a.emit({metricName:\"defTimes\",data:[e,t,i]}),a.mark(e,t)},sendDelta:function(e,t,n,i){var s,r=a._deltaMarks;r[e]||void 0===t||(s=i\u0026\u0026i.originalEndTime?i.originalEndTime:a.getTime(),r[e]=[s-t,s,n])},sendResTiming:function(e,t){a.emit({metricName:\"defRes\",data:[e,t]})},sendRaf:function(e){var t=a.getSetting(\"forcePaintTimeSending\");if(s\u0026\u0026(t||a.isTimeAfterPageShow(a.getTime()))){var n=\"2616.\"+e;s((function(){a.getSetting(\"sendFirstRaf\")\u0026\u0026a.sendTimeMark(n+\".205\"),s((function(){a.sendTimeMark(n+\".1928\")}))}))}},isVisibilityChanged:function(){return a.vsStart\u0026\u0026(\"visible\"!==a.vsStart||a.vsChanged)},isTimeAfterPageShow:function(e){return\"visible\"===a.vsStart||a.vsChangeTime\u003ce},mark:n\u0026\u0026n.mark?function(e,t){n.mark(e+(t?\": \"+t:\"\"))}:function(){},getSetting:function(e){var t=a._settings[e];return null===t?null:t||\"\"},on:function(e,t){if(\"function\"==typeof t)return(a._markListeners[e]=a._markListeners[e]||[]).push(t),function(){if(a._markListeners[e]){var n=a._markListeners[e].indexOf(t);n\u003e-1\u0026\u0026a._markListeners[e].splice(n,1)}}},noop:r,sendTrafficData:r,finalizeLayoutShiftScore:r,finalizeLargestContentfulPaint:r,getLCPAdditionalParams:r,getCLSAdditionalParams:r,getINPAdditionalParams:r,getImageGoodnessAdditionalParams:r,_eventListeners:{},_eventsBuffer:{},subscribe:function(e,t){if(!a.getSetting(\"noEvents\"))return this._eventLisneters[e]=this._eventLisneters[e]||new Set,this._eventLisneters[e].add(t),function(){this.unsubscribe(e,t)}.bind(this)},unsubscribe:function(e,t){this._eventLisneters[e].delete(t)},emit:function(e){if(!a.getSetting(\"noEvents\")){var t=a.getSetting(\"eventsLimits\")\u0026\u0026a.getSetting(\"eventsLimits\")[e.metricName]||20;this._eventLisneters[e.metricName]\u0026\u0026this._eventLisneters[e.metricName].forEach((function(t){t(e)})),this._eventsBuffer[e.metricName]=this._eventsBuffer[e.metricName]||[],this._eventsBuffer[e.metricName].push(e),this._eventsBuffer[e.metricName].length\u003et\u0026\u0026(this._eventsBuffer[e.metricName].length=Math.floor(t/2))}},getBufferedEvents:function(e){var t=this._eventsBuffer,n={};return Object.keys(t).forEach((function(i){-1!==e.indexOf(i)\u0026\u0026(n[i]=t[i])})),n},clearEvents:function(e){this._eventsBuffer[e]\u0026\u0026(this._eventsBuffer[e].length=0)}};function f(){Ya.Rum.vsChanged=!0,Ya.Rum.vsChangeTime=a.getTime(),removeEventListener(\"visibilitychange\",f)}addEventListener(\"visibilitychange\",f),a._onVisibilityChange=f}(window);\n!function(){if(window.PerformanceLongTaskTiming){var e=function(e,n){return(e=e.concat(n)).length\u003e300\u0026\u0026(e=e.slice(e.length-300)),e},n=\"undefined\"!=typeof PerformanceLongAnimationFrameTiming,t=n?[\"longtask\",\"long-animation-frame\"]:[\"longtask\"];function r(){var r=Ya.Rum._tti={events:[],loafEvents:n?[]:void 0,eventsAfterTTI:[],fired:!1,observer:new PerformanceObserver((function(t){var o=t.getEntriesByType(\"longtask\"),s=t.getEntriesByType(\"long-animation-frame\");r.events=e(r.events,o),n\u0026\u0026(r.loafEvents=e(r.loafEvents,s)),r.fired\u0026\u0026(r.eventsAfterTTI=e(r.eventsAfterTTI,o))}))};r.observer.observe({entryTypes:t}),Ya.Rum._unsubscribers\u0026\u0026Ya.Rum._unsubscribers.push((function(){r.observer.disconnect()}))}r(),Ya.Rum._onInit.push(r)}}();\nYa.Rum.observeDOMNode=window.IntersectionObserver?function(e,i,n){var t=this,o=Ya.Rum.getSetting(\"forcePaintTimeSending\");!function r(){if(o||!t.isVisibilityChanged()){var s=\"string\"==typeof i?document.querySelector(i):i;s?new IntersectionObserver((function(i,n){!o\u0026\u0026t.isVisibilityChanged()||(Ya.Rum.sendTimeMark(e),n.unobserve(s))}),n).observe(s):setTimeout(r,100)}}()}:function(){};\nvar rum_platform = window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop';\n    var rum_segment = window.location.pathname.replace(/^\\//, \"\").replace(/\\/.*/);\n    \n    if ([\"knowledge\", \"journal\", \"profile\", \"handbook\"].indexOf(rum_segment) === -1) {\n      rum_segment = \"portal\";\n    }\n\n    Ya.Rum.init({ beacon: true, clck: 'https://yandex.ru/clck/click', reqid: '1768064963563285-10555172254180365541'},\n    {\n        rum_id: 'ru.education.' + rum_platform + '.' + rum_segment,\n        '-env': 'production',\n        '-project': 'education-web',\n        '-page': window.location.pathname,\n        '-version': 'undefined',\n        '-platform': rum_platform\n    });Ya.Rum.observeDOMNode('2876', 'body');!function(){var e,t,n,i=Ya.Rum,o=42,r=4e4,g=15,a=[],s=\"\\r\\n\",l=i.getSetting(\"countersInitialDelay\")||0;if(l){var c;function u(){removeEventListener(\"visibilitychange\",h),clearTimeout(c),l=0,f()}function h(){document.hidden\u0026\u0026u()}c=setTimeout(u,l),addEventListener(\"visibilitychange\",h)}function f(){if(t\u0026\u0026a.length){for(var n=0,i=0,l=0;i\u003ca.length\u0026\u0026l\u003c=r\u0026\u0026n\u003co;i++)(l+=(i?s.length:0)+a[i].length)\u003c=r\u0026\u0026n++;var c=a.splice(0,n);d(t,c.join(s)),a.length\u0026\u0026(e=setTimeout(f,g))}else a.length=0}function d(e,t){if(!(navigator.sendBeacon\u0026\u0026n\u0026\u0026navigator.sendBeacon(e,t))){var o=Boolean(i.getSetting(\"sendCookie\")),r=new XMLHttpRequest;r.open(\"POST\",e),r.withCredentials=o,r.send(t)}}i.send=function(c,u,h,d,v,m,S,p){t=i.getSetting(\"clck\"),n=i.getSetting(\"beacon\"),o=i.getSetting(\"maxBatchCounters\")||o,r=i.getSetting(\"maxBatchLength\")||r,g=i.getSetting(\"countersBatchTimeout\")||g,function(t,n,i,c,u,h,d,v,m,S){clearTimeout(e);var p=[t?\"/reqid=\"+t:\"\",n?\"/\"+n.join(\"/\"):\"\",i?\"/path=\"+i:\"\",c?\"/events=\"+c:\"\",u?\"/slots=\"+u.join(\";\"):\"\",h?\"/experiments=\"+h.join(\";\"):\"\",d?\"/vars=\"+d:\"\",\"/cts=\"+(new Date).getTime(),\"\",\"/*\"].join(\"\");p.length\u003er?\"undefined\"!=typeof console\u0026\u0026console.error\u0026\u0026console.error(\"Counter length \"+p.length+\" is more than allowed \"+r,p):(a.push(p),l||(function(){if(a.length\u003e=o)return!0;for(var e=0,t=0;t\u003ca.length;t++)if((e+=(t?s.length:0)+a[t].length)\u003e=r)return!0;return!1}()?f():e=setTimeout(f,g)))}(i.getSetting(\"reqid\"),S,u,v,i.getSetting(\"slots\"),i.getSetting(\"experiments\"),h)}}();\n!function(){var e=Ya.Rum,n=!window.BigInt||!(\"PerformanceObserver\"in window);function t(n){e._unsubscribers.push(n)}function i(e,i,o){if(!n){var a=o||{};if(e){a.type=e,a.hasOwnProperty(\"buffered\")||(a.buffered=!0);var s=new PerformanceObserver((function(e,n){return i(e.getEntries(),n)}));return r((function(){try{s.observe(a)}catch(e){return void console.error(e.message)}t((function(){s.disconnect()}))}),0),s}throw new Error(\"PO without type field is forbidden\")}}function r(e,n){var i=setTimeout(e,n);return t((function(){clearInterval(i)})),i}function o(e,n,i){addEventListener(e,n,i),t((function(){removeEventListener(e,n,i)}))}function a(e,n,t){o(\"visibilitychange\",(function i(){if(\"hidden\"===document.visibilityState){try{t||(removeEventListener(\"visibilitychange\",i),e.disconnect())}catch(e){}n()}})),o(\"beforeunload\",n)}function s(e,n){return\"string\"==typeof e?encodeURIComponent(e):Math.round(1e3*(e-(n||0)))/1e3}function u(e){if(!e)return\"\";var n=(e.tagName||\"\").toLowerCase(),t=e.className\u0026\u0026void 0!==e.className.baseVal?e.className.baseVal:e.className;return n+(t?(\" \"+t).replace(/\\s+/g,\".\"):\"\")}function c(e){function n(){removeEventListener(\"DOMContentLoaded\",n),removeEventListener(\"load\",n),e()}\"loading\"===document.readyState?(o(\"DOMContentLoaded\",n),o(\"load\",n)):e()}function d(n){e._onComplete.push(n)}function f(){return e._periodicTasks}function l(){var n=e._vars;return Object.keys(n).map((function(e){return e+\"=\"+encodeURIComponent(n[e]).replace(/\\*/g,\"%2A\")}))}var m={connectEnd:2116,connectStart:2114,decodedBodySize:2886,domComplete:2124,domContentLoadedEventEnd:2131,domContentLoadedEventStart:2123,domInteractive:2770,domLoading:2769,domainLookupEnd:2113,domainLookupStart:2112,duration:2136,encodedBodySize:2887,entryType:2888,fetchStart:2111,initiatorType:2889,loadEventEnd:2126,loadEventStart:2125,nextHopProtocol:2890,redirectCount:1385,redirectEnd:2110,redirectStart:2109,requestStart:2117,responseEnd:2120,responseStart:2119,secureConnectionStart:2115,startTime:2322,transferSize:2323,type:76,unloadEventEnd:2128,unloadEventStart:2127,workerStart:2137},v=625;function g(n,t){Object.keys(m).forEach((function(e){if(e in t){var i=t[e];(i||0===i)\u0026\u0026n.push(m[e]+\"=\"+s(i))}})),n.push(\"\".concat(v,\"=\").concat(e.version))}var p,h,y,S,T,b=\"690.2096.2877\",w=\"690.2096.207\",E=\"690.2096.2044\",C=3,k=e.getSetting(\"savedDeltasLimit\")||0,L=document.createElement(\"link\"),P=window.performance||{},M=\"function\"==typeof P.getEntriesByType,_=0;function O(n,t,i,r,o){void 0===t\u0026\u0026(t=e.getTime()),void 0!==i\u0026\u0026!0!==i||e.mark(n,t);var a=I(n);if(a.push(\"207=\"+s(t)),x(a,r)){j(w,a,o\u0026\u0026o.force),p[n]=p[n]||[],p[n].push(t);var u=e._markListeners[n];u\u0026\u0026u.length\u0026\u0026u.forEach((function(e){e(t)})),e.emit({metricName:n,value:t,params:r})}}function I(n){return T.concat([e.isVisibilityChanged()?\"-vsChanged=1\":\"\",\"1701=\"+n,e.ajaxStart\u0026\u0026\"1201.2154=\"+s(e.ajaxStart),e.ajaxComplete\u0026\u0026\"1201.2052=\"+s(e.ajaxComplete)])}function N(){S=l(),e.getSetting(\"sendClientUa\")\u0026\u0026S.push(\"1042=\"+encodeURIComponent(navigator.userAgent))}function R(){var e=window.performance\u0026\u0026window.performance.timing\u0026\u0026window.performance.timing.navigationStart;T=S.concat([\"143.2129=\"+e])}function x(e,n){if(n){if(n.isCanceled\u0026\u0026n.isCanceled())return!1;var t=e.reduce((function(e,n,t){return\"string\"==typeof n\u0026\u0026(e[n.split(\"=\")[0]]=t),e}),{});Object.keys(n).forEach((function(i){if(\"function\"!=typeof n[i]){var r=t[i],o=i+\"=\"+n[i];void 0===r?e.push(o):e[r]=o}}))}return!0}function j(n,t,i){var r=encodeURIComponent(window.YaStaticRegion||\"unknown\");t.push(\"-cdn=\"+r);var o=t.filter(Boolean).join(\",\");e.send(null,n,o,void 0,void 0,void 0,void 0,i)}function z(e,n,t){j(e,F().concat(n),t)}function B(n,t){var i=y[n];i\u0026\u00260!==i.length\u0026\u0026(i.push(e.getTime(),t),A(n))}function A(n,t,i,r){var o,a,u,c=y[n];if(void 0!==t?o=(a=r\u0026\u0026r.originalEndTime?r.originalEndTime:e.getTime())-t:c\u0026\u0026(o=c[0],a=c[1],u=c[2]),void 0!==o\u0026\u0026void 0!==a){var d=I(n);d.push(\"207.2154=\"+s(o),\"207.1428=\"+s(a),\"2877=\"+s(a-o)),x(d,i)\u0026\u0026x(d,u)\u0026\u0026(j(b,d,r\u0026\u0026r.force),_\u003ck\u0026\u0026(h[n]=h[n]||[],h[n].push(a-o),_++),e.emit({metricName:n,value:a-o,params:{start:o,end:a}}),delete y[n])}}function V(e,n){if(!M)return n(null);L.href=e;var t=0,i=100,o=L.href;r((function e(){var a=P.getEntriesByName(o);if(a.length)return n(a);t++\u003cC?(r(e,i),i+=i):n(null)}),0)}function U(e,n,t){V(n,(function(i){i\u0026\u0026D(e,i[i.length-1],n,t)}))}function D(n,t,i,r){var o=I(n);e.getSetting(\"sendUrlInResTiming\")\u0026\u0026o.push(\"13=\"+encodeURIComponent(i)),g(o,t),x(o,r),j(E,o)}function F(){return S}var W={bluetooth:2064,cellular:2065,ethernet:2066,none:1229,wifi:2067,wimax:2068,other:861,unknown:836,0:836,1:2066,2:2067,3:2070,4:2071,5:2768},H=navigator.connection;function Q(e){if(H){var n=W[H.type];e.push(\"2437=\"+(n||2771),void 0!==H.downlinkMax\u0026\u0026\"2439=\"+H.downlinkMax,H.effectiveType\u0026\u0026\"2870=\"+H.effectiveType,void 0!==H.rtt\u0026\u0026\"rtt=\"+H.rtt,void 0!==H.downlink\u0026\u0026\"dwl=\"+H.downlink,!n\u0026\u0026\"rawType=\"+H.type)}}var Y,q,G,J,$,K,X,Z,ee=\"690.2096.4004\",ne=!1,te=1/0,ie=1/0,re=(\"layout-shift\",Boolean(window.PerformanceObserver\u0026\u0026window.PerformanceObserver.supportedEntryTypes\u0026\u0026-1!==window.PerformanceObserver.supportedEntryTypes.indexOf(\"layout-shift\"))?0:null);function oe(){$\u003eq\u0026\u0026(q=$,G=J,e.emit({metricName:\"cls-debug\",value:q,params:{clsEntries:G,target:ae(G)}}))}function ae(e){var n;if(!e)return null;var t=null;if((n=e.reduce((function(e,n){return e\u0026\u0026e.value\u003en.value?e:n})))\u0026\u0026n.sources\u0026\u0026n.sources.length){for(var i=0;i\u003cn.sources.length;i++){var r=n.sources[i];if(r.node\u0026\u00261===r.node.nodeType){t=r;break}}t=t||n.sources[0]}return t}function se(e){null==q\u0026\u0026(q=0);for(var n=0;n\u003ce.length;n++){var t=e[n];t.hadRecentInput||($\u0026\u0026t.startTime-J[J.length-1].startTime\u003cte\u0026\u0026t.startTime-J[0].startTime\u003cie?($+=t.value,J.push(t)):(oe(),$=t.value,J=[t]))}oe()}function ue(){q=re,Y=void 0,G=null,J=null,$=null,ne=!1}function ce(n){if(null!=q\u0026\u0026!ne){var t=Math.round(1e6*q)/1e6;if(Y!==t){Y=t,e.getSetting(\"enableContinuousCollection\")||(ne=!0);var i=ae(G),r=[\"s=\"+t];r.push(\"target=\"+u(i\u0026\u0026i.node));var o=e.getCLSAdditionalParams(i);o\u0026\u0026x(r,o),z(ee,r,n),e.emit({metricName:\"cls-debug\",value:q,params:{clsEntries:J,target:i,isFinalized:ne}})}}}function de(n){var t=n[n.length-1];K=t.renderTime||t.loadTime,X=t,e.emit({metricName:\"largest-contentful-paint-debug\",value:K,params:{entry:t}}),Z||(O(\"largest-loading-elem-paint\",K),Z=!0)}function fe(n){if(null!=K){var t=e.getLCPAdditionalParams(X);O(\"largest-contentful-paint\",K,!1,t,n\u0026\u0026{force:!0}),e.emit({metricName:\"largest-contentful-paint-debug\",value:K,params:{additionalParams:t,entry:X,isFinalized:!0}}),K=null,X=null}}e.getLCPAdditionalParams===e.noop\u0026\u0026(e.getLCPAdditionalParams=function(){var n={},t=X.element;if(t){n[\"-className\"]=e.getSelector(t),n[\"-tagName\"]=t.tagName.toLowerCase();var i=t.getBoundingClientRect();n[\"-width\"]=i.width,n[\"-height\"]=i.height}return X.size\u0026\u0026(n[\"-size\"]=X.size),n});var le={\"first-paint\":2793,\"first-contentful-paint\":2794},me=Object.keys(le).length,ve={},ge=window.performance||{},pe=\"function\"==typeof ge.getEntriesByType,he=0;function ye(){if(pe\u0026\u0026(e.getSetting(\"forcePaintTimeSending\")||!e.isVisibilityChanged()))for(var n=ge.getEntriesByType(\"paint\"),t=0;t\u003cn.length;t++){var i=n[t],r=le[i.name];r\u0026\u0026!ve[i.name]\u0026\u0026(ve[i.name]=!0,he++,O(\"1926.\"+r,i.startTime))}}var Se=3e3,Te=1;function be(){return e._tti.events||[]}function we(){return e._tti.loafEvents}function Ee(){return e._tti}function Ce(n){return n?n===e.getPageUrl()?\"\u003cpage\u003e\":n.replace(/\\?.*$/,\"\"):n}function ke(n,t,i){if(Ee()){var r=e.getTime(),o=\"undefined\"!=typeof PerformanceLongAnimationFrameTiming\u0026\u0026e.getSetting(\"sendLongAnimationFrames\");Le((function(a){var u,c={2796.2797:Pe(be(),t),689.2322:s(r)};if(o){var d=function(e){var n=we();if(n)return e?n.filter((function(n){return n.startTime+n.duration\u003e=e})):n}(t);d\u0026\u0026(c[\"loaf.2797\"]=Pe(d,void 0,{useName:!1}),1===e.getSetting(\"longAnimationFramesMode\")\u0026\u0026(c[\"-additional\"]=encodeURIComponent(JSON.stringify({loaf:(u=d,u.map(Me))}))))}i\u0026\u0026Object.keys(i).forEach((function(e){c[e]=i[e]})),O(n||\"2795\",a,!0,c,{force:Boolean(o)}),e._tti.fired=!0}),t)}}function Le(n,t){var i=(arguments.length\u003e2\u0026\u0026void 0!==arguments[2]?arguments[2]:{}).mode,o=void 0===i?Te:i;Ee()\u0026\u0026(t||(t=e.getTime()),function i(){var a,s=t,u=e.getTime(),c=o===Te?be():we()||[],d=c.length;0!==d\u0026\u0026(a=c[d-1],s=Math.max(s,Math.floor(a.startTime+a.duration))),u-s\u003e=Se?n(s):r(i,1e3)}())}function Pe(e,n){var t=(arguments.length\u003e2\u0026\u0026void 0!==arguments[2]?arguments[2]:{}).useName,i=void 0===t||t;return n=n||0,(e=e||[]).filter((function(e){return n-e.startTime\u003c=50})).map((function(e){var n=Math.floor(e.startTime),t=Math.floor(n+e.duration);return i?(e.name?e.name.split(\"-\").map((function(e){return e[0]})).join(\"\"):\"u\")+\"-\"+n+\"-\"+t:n+\"-\"+t})).join(\".\")}function Me(e){var n=e.blockingDuration,t=e.duration,i=e.firstUIEventTimestamp,r=e.renderStart,o=e.scripts,a=e.startTime,s=e.styleAndLayoutStart;return[Math.round(a),Math.round(t),o.map(Oe),Math.round(n),Math.round(i),Math.round(r),Math.round(s)]}function _e(e){return{\"user-callback\":1,\"event-listener\":2,\"resolve-promise\":3,\"reject-promise\":4,\"classic-script\":5,\"module-script\":6}[e]||0}function Oe(e){var n=e.invoker,t=e.sourceURL,i=e.sourceFunctionName,r=e.sourceCharPosition,o=e.startTime,a=e.duration,s=e.windowAttribution,u=e.executionStart,c=e.forcedStyleAndLayoutDuration,d=e.pauseDuration,f=e.invokerType;return[Ce(n),Ce(t),i,r,Math.round(o),Math.round(a),s,Math.round(u),Math.round(c),Math.round(d),_e(f)]}var Ie=\"690.2096.361\",Ne=document.createElement(\"a\"),Re=0,xe={};function je(e){var n=e.transferSize;if(null!=n){Ne.href=e.name;var t=Ne.pathname;if(0!==t.indexOf(\"/clck\")){var i=t.lastIndexOf(\".\"),r=\"\";return-1!==i\u0026\u0026t.lastIndexOf(\"/\")\u003ci\u0026\u0026t.length-i\u003c=5\u0026\u0026(r=t.slice(i+1)),{size:n,domain:Ne.hostname,extension:r}}}}function ze(){var n=e.getSetting(\"maxTrafficCounters\")||250;if(Re\u003e=n)return!1;for(var t=Object.keys(xe),i=\"\",r=0;r\u003ct.length;r++){var o=t[r],a=xe[o];i+=encodeURIComponent(o)+\"!\"+a.count+\"!\"+a.size+\";\"}return i.length\u0026\u0026(Re++,z(Ie,[\"d=\"+i,\"t=\"+s(e.getTime())])),xe={},Re\u003cn}d(ze);var Be=\"690.1033\",Ae={visible:1,hidden:2,prerender:3},Ve=window.performance||{},Ue=Ve.navigation||{},De=Ve.timing||{},Fe=De.navigationStart;function We(){var n=De.domContentLoadedEventStart,t=De.domContentLoadedEventEnd;if(0!==n||0!==t){var i=0===De.responseStart?Fe:De.responseStart,o=0===De.domainLookupStart?Fe:De.domainLookupStart,a=[\"2129=\"+Fe,\"1036=\"+(o-Fe),\"1037=\"+(De.domainLookupEnd-De.domainLookupStart),\"1038=\"+(De.connectEnd-De.connectStart),De.secureConnectionStart\u0026\u0026\"1383=\"+(De.connectEnd-De.secureConnectionStart),\"1039=\"+(De.responseStart-De.connectEnd),\"1040=\"+(De.responseEnd-i),\"1040.906=\"+(De.responseEnd-o),\"1310.2084=\"+(De.domLoading-i),\"1310.2085=\"+(De.domInteractive-i),\"1310.1309=\"+(t-n),\"1310.1007=\"+(n-i),navigator.deviceMemory\u0026\u0026\"3140=\"+navigator.deviceMemory,navigator.hardwareConcurrency\u0026\u0026\"3141=\"+navigator.hardwareConcurrency];Object.keys(m).forEach((function(e){e in De\u0026\u0026De[e]\u0026\u0026a.push(m[e]+\"=\"+s(De[e],Fe))})),e.vsStart?(a.push(\"1484=\"+(Ae[e.vsStart]||2771)),e.vsChanged\u0026\u0026a.push(\"1484.719=1\")):a.push(\"1484=\"+Ae.visible),Ue\u0026\u0026(Ue.redirectCount\u0026\u0026a.push(\"1384.1385=\"+Ue.redirectCount),1!==Ue.type\u0026\u00262!==Ue.type||a.push(\"770.76=\"+Ue.type)),Q(a),z(Be,a)}else r(We,50)}var He=\"690.2096.2892\",Qe=window.performance||{},Ye=\"function\"==typeof Qe.getEntriesByType;if(!e)throw new Error(\"Rum: interface is not included\");if(e.enabled){function qe(){p={},h={},_=0,y=e._deltaMarks,N(),R(),e.ajaxStart=0,e.ajaxComplete=0,c(Je)}function Ge(){var n;e.sendTimeMark=O,e.sendResTiming=U,e.sendTiming=D,e.timeEnd=B;var t=(e.getBufferedEvents([\"defRes\"]).defRes||[]).map((function(e){return e.data}));for(n=0;n\u003ct.length;n++)U(t[n][0],t[n][1]);e.clearEvents(\"defRes\");var i=(e.getBufferedEvents([\"defTimes\"]).defTimes||[]).map((function(e){return e.data}));for(n=0;n\u003ci.length;n++)O(i[n][0],i[n][1],!1,i[n][2]);e.clearEvents(\"defTimes\"),Object.keys(y).forEach((function(e){A(e)}))}function Je(){var n=window.performance\u0026\u0026window.performance.timing\u0026\u0026window.performance.timing.navigationStart,t=e.getSetting(\"skipTiming\"),a=e.getSetting(\"techParamsByVisible\");n\u0026\u0026(a\u0026\u0026addEventListener(\"visibilitychange\",(function n(){\"visible\"!==e.vsStart?\"visible\"===document.visibilityState\u0026\u0026(e.vsStart=\"visible\",removeEventListener(\"visibilitychange\",n),We()):removeEventListener(\"visibilitychange\",n)})),r((function(){Ge(),(!t\u0026\u0026!a||a\u0026\u0026\"visible\"===e.vsStart)\u0026\u0026We(),e.getSetting(\"disableFCP\")||(ye(),he\u003cme\u0026\u0026i(\"paint\",(function(e,n){ye(),n\u0026\u0026he\u003e=me\u0026\u0026n.disconnect()}),{buffered:!0})),e.getSetting(\"sendAutoElementTiming\")\u0026\u0026(!window.PerformanceObserver||!e.getSetting(\"forcePaintTimeSending\")\u0026\u0026e.isVisibilityChanged()||i(\"element\",(function(e){for(var n=0;n\u003ce.length;n++){var t=e[n];O(\"element-timing.\"+t.identifier,t.startTime)}}))),o(\"pageshow\",Xe),ke(),\"complete\"===document.readyState?$e({skipTimingApi:t}):o(\"load\",$e.bind(void 0,{skipTimingApi:t}))}),0))}function $e(n){var r,s;e.getSetting(\"disableOnLoadTasks\")||(removeEventListener(\"load\",$e),n.skipTimingApi||function(){if(Ye){var e=Qe.getEntriesByType(\"navigation\")[0];if(e){var n=[];g(n,e),Q(n);var t=Qe.getEntriesByName(\"yndxNavigationSource\")[0];t\u0026\u0026n.push(\"2091.186=\"+t.value);var i=Qe.getEntriesByName(\"yndxNavigationToken\",\"yndxEntry\")[0];i\u0026\u0026n.push(\"2091.3649=\"+i.value),z(He,n)}}}(),(s=e.getSetting(\"periodicStatsIntervalMs\"))||null===s||(s=15e3),s\u0026\u0026(r=setInterval(Ze,s),t((function(){clearInterval(r)})),Ke=r),o(\"beforeunload\",Ze),function(){if(window.PerformanceObserver){xe={},Re=0;var e=function(e){!function(e){if(e\u0026\u0026e.length)for(var n=xe,t=0;t\u003ce.length;t++){var i=je(e[t]);if(i){var r=i.domain+\"-\"+i.extension,o=n[r]=n[r]||{count:0,size:0};o.count++,o.size+=i.size}}}(e)};i(\"resource\",e),i(\"navigation\",e),f().push(ze)}}(),e.getSetting(\"disableFID\")||i(\"first-input\",(function(n,t){var i=n[0];if(i){var r=i.processingStart,o={duration:i.duration,js:i.processingEnd-r,name:i.name};i.target\u0026\u0026(o.target=u(i.target));var a=r-i.startTime;A(\"first-input\",a,o),e.emit({metricName:\"first-input-debug\",value:a,params:{entry:i,additionalParams:o}}),t.disconnect()}}),{buffered:!0}),e.getSetting(\"disableCLS\")||window.PerformanceObserver\u0026\u0026(d(ce),d(ue),q=re,G=null,J=null,$=null,te=e.getSetting(\"clsWindowGap\")||te,ie=e.getSetting(\"clsWindowSize\")||ie,a(i(\"layout-shift\",se),(function(){return ce(!0)}),!0)),e.getSetting(\"disableLCP\")||!window.PerformanceObserver||!e.getSetting(\"forcePaintTimeSending\")\u0026\u0026e.isVisibilityChanged()||(d(fe),K=null,X=null,Z=!1,a(i(\"largest-contentful-paint\",de),(function(){return fe(!0)}),!1)))}var Ke;function Xe(e){e.persisted\u0026\u0026O(\"bfcache\")}function Ze(){var e=!1;f().forEach((function(n){n()\u0026\u0026(e=!0)})),e||clearInterval(Ke)}d(Ge),e.destroy=function(n){var t=e._unsubscribers;n.shouldComplete\u0026\u0026e.completeSession(!0),e._onComplete=[];for(var i=0;i\u003ct.length;i++)t[i]();removeEventListener(\"visibilitychange\",e._onVisibilityChange),e._unsubscribers=[],e._periodicTasks=[],e._markListeners={},e._deltaMarks={}},e.restart=function(n,t,i){e.destroy({shouldComplete:i}),e.init(n,t),addEventListener(\"visibilitychange\",e._onVisibilityChange),qe(),function(){for(var n=0;n\u003ce._onInit.length;n++)e._onInit[n]()}()},e.setVars=function(n){Object.keys(n).forEach((function(t){e._vars[t]=n[t]})),N(),R()},e.completeSession=function(n){for(var t=e._onComplete,i=0;i\u003ct.length;i++)t[i](n)},qe(),e._periodicTasks=[],e.sendHeroElement=function(e){O(\"2876\",e)},e.getPageUrl=function(){return window.location.href},e._subpages={},e.makeSubPage=function(n,t){var i=e._subpages[n];e._subpages[n]=void 0===i?i=0:++i;var r=!1;return{689.2322:s(void 0!==t?t:e.getTime()),2924:n,2925:i,isCanceled:function(){return r},cancel:function(){r=!0}}},e.getTimeMarks=function(){return p},e.getDeltas=function(){return h},e.getVarsList=l,e.getResourceTimings=V,e.pushConnectionTypeTo=Q,e.pushTimingTo=g,e.normalize=s,e.sendCounter=j,e.sendDelta=A,e.onReady=c,e.getSelector=u,e.getSetting(\"disableCLS\")||(e.finalizeLayoutShiftScore=ce),e.getSetting(\"disableLCP\")||(e.finalizeLargestContentfulPaint=fe),e.sendTrafficData=ze,e._getCommonVars=F,e._addListener=o,e._observe=i,e._timeout=r,e.sendTTI=ke,e._getLongtasksStringValue=Pe,e.onQuietWindow=Le,e.sendBFCacheTimeMark=Xe}else e.getSetting=function(){return\"\"},e.getVarsList=function(){return[]},e.getResourceTimings=e.completeSession=e.pushConnectionTypeTo=e.pushTimingTo=e.normalize=e.sendCounter=e.destroy=e.restart=e.setVars=e.completeSession=e.sendDelta=e.sendTimeMark=e.sendResTiming=e.sendTiming=e.sendTTI=e.makeSubPage=e.sendHeroElement=e.onReady=e.onQuietWindow=function(){}}();\n!function(n){if(!n.Ya||!Ya.Rum)throw new Error(\"Rum: interface is not defined\");var e=Ya.Rum;e.getSetting=function(n){var t=e._settings[n];return null===t?null:t||\"\"}}(\"undefined\"!=typeof self?self:window);\n!function(e,r){var n={client:[\"690.2354\",1e3,100,0],uncaught:[\"690.2361\",100,10,0],external:[\"690.2854\",100,10,0],script:[\"690.2609\",100,10,0]},t={};r.ERROR_LEVEL={INFO:\"info\",DEBUG:\"debug\",WARN:\"warn\",ERROR:\"error\",FATAL:\"fatal\"},r._errorSettings={clck:\"https://yandex.ru/clck/click\",beacon:!0,project:\"unknown\",page:\"\",env:\"\",experiments:[],additional:{},platform:\"\",region:\"\",dc:\"\",host:\"\",service:\"\",level:\"\",version:\"\",yandexuid:\"\",loggedin:!1,coordinates_gp:\"\",referrer:!0,preventError:!1,unhandledRejection:!1,traceUnhandledRejection:!1,uncaughtException:!0,debug:!1,limits:{},silent:{},filters:{},pageMaxAge:864e6,initTimestamp:+new Date};var o=!1;function a(e,r){for(var n in r)r.hasOwnProperty(n)\u0026\u0026(e[n]=r[n]);return e}function i(e){return\"boolean\"==typeof e\u0026\u0026(e=+e),\"number\"==typeof e?e+\"\":null}r.initErrors=function(n){var t=a(r._errorSettings,n);o||(t.uncaughtException\u0026\u0026function(){var n=r._errorSettings;if(e.addEventListener)e.addEventListener(\"error\",s),n.resourceFails\u0026\u0026e.addEventListener(\"error\",l,!0),\"Promise\"in e\u0026\u0026n.unhandledRejection\u0026\u0026e.addEventListener(\"unhandledrejection\",function(e){var n,t,o=e.reason,a={};o\u0026\u0026(o.stack\u0026\u0026o.message?(n=o.message,t=o.stack):(n=String(o),t=r._parseTraceablePromiseStack(e.promise),\"[object Event]\"===n?n=\"event.type: \"+o.type:\"[object Object]\"===n\u0026\u0026(a.unhandledObject=o)),o.target\u0026\u0026o.target.src\u0026\u0026(a.src=o.target.src),s({message:\"Unhandled rejection: \"+n,stack:t,additional:a}))});else{var t=e.onerror;e.onerror=function(e,r,n,o,a){s({error:a||new Error(e||\"Empty error\"),message:e,lineno:n,colno:o,filename:r}),t\u0026\u0026t.apply(this,arguments)}}}(),t.unhandledRejection\u0026\u0026t.traceUnhandledRejection\u0026\u0026r._traceUnhandledRejection\u0026\u0026r._traceUnhandledRejection(),o=!0)},r.updateErrors=function(e){a(r._errorSettings,e)},r.updateAdditional=function(e){r._errorSettings.additional=a(r._errorSettings.additional||{},e)},r._handleError=function(e,o,i){var s,l,c=r._errorSettings;if(c.preventError\u0026\u0026e.preventDefault\u0026\u0026e.preventDefault(),o)s=e,l=\"client\";else{s=r._normalizeError(e),l=s.type;var d=c.onError;\"function\"==typeof d\u0026\u0026d(s);var u=c.transform;if(\"function\"==typeof u\u0026\u0026(s=u(s)),!s)return;s.settings\u0026\u0026(i=s.settings)}var g=+new Date,f=c.initTimestamp,p=c.pageMaxAge;if(!(-1!==p\u0026\u0026f\u0026\u0026f+p\u003cg)){var m=n[l][1];\"number\"==typeof c.limits[l]\u0026\u0026(m=c.limits[l]);var v=n[l][2];\"number\"==typeof c.silent[l]\u0026\u0026(v=c.silent[l]);var h=n[l][3];if(h\u003cm||-1===m){s.path=n[l][0];var E=r._getErrorData(s,{silent:h\u003cv||-1===v?\"no\":\"yes\",isCustom:Boolean(o)},a(a({},c),i)),_=function(e){t[s.message]=!1,r._sendError(e.path,e.vars),n[l][3]++}.bind(this,E);if(void 0===c.throttleSend)_();else{if(t[s.message])return;t[s.message]=!0,setTimeout(_,c.throttleSend)}}}},r._getReferrer=function(r){var n=r.referrer,t=typeof n;return\"function\"===t?n():\"string\"===t\u0026\u0026n?n:!1!==n\u0026\u0026e.location?e.location.href:void 0},r.getErrorSetting=function(e){return r._errorSettings[e]},r._buildExperiments=function(e){return e instanceof Array?e.join(\";\"):\"\"},r._buildAdditional=function(e,r){var n=\"\";try{var t=a(a({},e),r);0!==Object.keys(t).length\u0026\u0026(n=JSON.stringify(t))}catch(e){}return n},r._getErrorData=function(n,t,o){t=t||{};var a=r._buildExperiments(o.experiments),s=r._buildAdditional(o.additional,n.additional),l={\"-stack\":n.stack,\"-url\":n.file,\"-line\":n.line,\"-col\":n.col,\"-block\":n.block,\"-method\":n.method,\"-msg\":n.message,\"-env\":o.env,\"-external\":n.external,\"-externalCustom\":n.externalCustom,\"-project\":o.project,\"-service\":n.service||o.service,\"-page\":n.page||o.page,\"-platform\":o.platform,\"-level\":n.level,\"-experiments\":a,\"-version\":o.version,\"-region\":o.region,\"-dc\":o.dc,\"-host\":o.host,\"-yandexuid\":o.yandexuid,\"-loggedin\":o.loggedin,\"-coordinates_gp\":n.coordinates_gp||o.coordinates_gp,\"-referrer\":r._getReferrer(o),\"-source\":n.source,\"-sourceMethod\":n.sourceMethod,\"-type\":t.isCustom?n.type:\"\",\"-additional\":s,\"-adb\":i(Ya.blocker)||i(o.blocker),\"-cdn\":e.YaStaticRegion,\"-ua\":navigator.userAgent,\"-silent\":t.silent,\"-ts\":+new Date,\"-init-ts\":o.initTimestamp};return o.debug\u0026\u0026e.console\u0026\u0026console[console[n.level]?n.level:\"error\"](\"[error-counter] \"+n.message,l,n.stack),{path:n.path,vars:l}},r._baseNormalizeError=function(e){var r=(e=e||{}).error,n=e.filename||e.fileName||\"\",t=r\u0026\u0026r.stack||e.stack||\"\",o=e.message||\"\",a=r\u0026\u0026r.additional||e.additional;return{file:n,line:e.lineno||e.lineNumber,col:e.colno||e.colNumber,stack:t,message:o,additional:a}},r._normalizeError=function(e){var n=r._baseNormalizeError(e),t=\"uncaught\",o=r._isExternalError(n.file,n.message,n.stack),a=\"\",i=\"\";return o.hasExternal?(t=\"external\",a=o.common,i=o.custom):/^Script error\\.?$/.test(n.message)\u0026\u0026(t=\"script\"),n.external=a,n.externalCustom=i,n.type=t,n},r._createVarsString=function(e){var r=[];for(var n in e)e.hasOwnProperty(n)\u0026\u0026(e[n]||0===e[n])\u0026\u0026r.push(n+\"=\"+encodeURIComponent(e[n]).replace(/\\*/g,\"%2A\"));return r.join(\",\")},r._sendError=function(e,n){r.send(null,e,r._createVarsString(n),null,null,null,null)};var s=function(e){r._handleError(e,!1)},l=function(e){var n=e.target;if(n){var t=n.srcset||n.src;if(t||(t=n.href),t){var o=n.tagName||\"UNKNOWN\";r.logError({message:o+\" load error\",additional:{src:t}})}}};r._parseTraceablePromiseStack=function(){}}(\"undefined\"!=typeof self?self:window,Ya.Rum);\n!function(e){var r={url:{0:/(miscellaneous|extension)_bindings/,1:/^chrome:/,2:/kaspersky-labs\\.com\\//,3:/^(?:moz|chrome|safari)-extension:\\/\\//,4:/^file:/,5:/^resource:\\/\\//,6:/webnetc\\.top/,7:/local\\.adguard\\.com/},message:{0:/__adgRemoveDirect/,1:/Content Security Policy/,2:/vid_mate_check/,3:/ucapi/,4:/Access is denied/i,5:/^Uncaught SecurityError/i,6:/__ybro/,7:/__show__deepen/,8:/ntp is not defined/,9:/Cannot set property 'install' of undefined/,10:/NS_ERROR/,11:/Error loading script/,12:/^TypeError: undefined is not a function$/,13:/__firefox__\\.(?:favicons|metadata|reader|searchQueryForField|searchLoginField)/},stack:{0:/(?:moz|chrome|safari)-extension:\\/\\//,1:/adguard.*\\.user\\.js/i}};function n(e,r){if(e\u0026\u0026r){var n=[];for(var o in r)if(r.hasOwnProperty(o)){var i=r[o];\"string\"==typeof i\u0026\u0026(i=new RegExp(i)),i instanceof RegExp\u0026\u0026i.test(e)\u0026\u0026n.push(o)}return n.join(\"_\")}}function o(e,o){var i,a=[];for(var t in r)r.hasOwnProperty(t)\u0026\u0026(i=n(e[t],o[t]))\u0026\u0026a.push(t+\"~\"+i);return a.join(\";\")}e._isExternalError=function(n,i,a){var t=e._errorSettings.filters||{},s={url:(n||\"\")+\"\",message:(i||\"\")+\"\",stack:(a||\"\")+\"\"},c=o(s,r),u=o(s,t);return{common:c,custom:u,hasExternal:!(!c\u0026\u0026!u)}}}(Ya.Rum);\nYa.Rum.initErrors({\n        reqid: '1768064963563285-10555172254180365541',\n        project: 'education-web',\n        env: 'production',\n        page: window.location.pathname,\n        version: 'undefined',\n        platform: window.matchMedia('(max-width: 767px)').matches ? 'touch' : 'desktop'\n    });"])</script><script nonce="">self.__next_f.push([1,"44:[\"$\",\"$L26\",null,{\"id\":\"rum-error\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"strategy\":\"beforeInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"$4c\"}}]\n"])</script><script nonce="">self.__next_f.push([1,"4d:T1248,"])</script><script nonce="">self.__next_f.push([1,"\n  function applyUADataPolyfill(){\n    function e(a){let b=/Windows NT (\\d+(\\.\\d+)*)/.exec(a);if(!b)return null;let c={\"6.1\":\"0.1\",\"6.2\":\"0.2\",\"6.3\":\"0.3\",\"10.0\":\"10.0\",\"11.0\":\"13.0\"}[b[1]],d=c?q(c,3):\"\",e=\"\",f=\"\";return/\\b(WOW64|Win64|x64)\\b/.test(a)\u0026\u0026(e=\"x86\",f=\"64\"),{platform:\"Windows\",platformVersion:d,architecture:e,bitness:f}}\n    function f(a,b){let c=/Android (\\d+(\\.\\d+)*)/.exec(a);if(!c)return null;let d=\"\",e=\"\",f=/Linux (\\w+)/.exec(b);if(f\u0026\u0026f[1]){let a=p(f[1]);d=a[0],e=a[1]}return{platform:\"Android\",platformVersion:q(c[1]),architecture:d,bitness:e}}\n    function g(a){let b=/(iPhone|iPod touch); CPU iPhone OS (\\d+(_\\d+)*)/.exec(a),c=/(iPad); CPU OS (\\d+(_\\d+)*)/.exec(a),d=b||c;return d?{platform:\"iOS\",platformVersion:q(d[2].replace(/_/g,\".\"))}:null}\n    function h(a){let b=/Macintosh; (Intel|\\w+) Mac OS X (\\d+([_.]\\d+)*)/.exec(a);return b?{platform:\"macOS\",platformVersion:q(b[2].replace(/_/g,\".\"))}:null}\n    function i(a){let b=/CrOS (\\w+) (\\d+(\\.\\d+)*)/.exec(a);if(!b)return null;let c=p(b[1]);return{platform:\"Chrome OS\",platformVersion:q(b[2]),architecture:c[0],bitness:c[1]}}\n    function j(a,b){for(let c of[()=\u003ee(a),()=\u003ef(a,b),()=\u003eg(a),()=\u003eh(a),()=\u003ei(a)]){let a=c();if(a)return a}return/Linux/.test(a)?{platform:\"Linux\",platformVersion:\"\"}:{platform:\"Unknown\",platformVersion:\"\"}}\n    function k(a,b,c){let d=/Chrome\\/(\\d+(\\.\\d+)*)/.exec(a);if(!d||\"Google Inc.\"!==c)return null;let e=[{brand:\"Chromium\",version:q(d[1],4)}],f=/(Edge?)\\/(\\d+(\\.\\d+)*)/.exec(a);if(f){let a={Edge:\"Microsoft Edge\",Edg:\"Microsoft Edge\"}[f[1]];e.push({brand:a,version:q(f[2],4)})}else e.push({brand:\"Google Chrome\",version:q(d[1],4)});return e}\n    function l(a,b){let c=/AppleWebKit\\/(\\d+(\\.\\d+)*)/.exec(a);return c\u0026\u0026\"Apple Computer, Inc.\"===b?[{brand:\"WebKit\",version:q(c[1])}]:null}\n    function m(a){let b=/Firefox\\/(\\d+(\\.\\d+)*)/.exec(a);return b?[{brand:\"Firefox\",version:q(b[1])}]:null}\n    function n(a,b,c,d,e){let f=!1,g=[];for(let h of[()=\u003ek(a,b,c),()=\u003el(a,c),()=\u003em(a)]){let c=h();if(c){g=c,h===k\u0026\u0026/\\bwv\\b/.test(b)\u0026\u0026(f=!0);let i=/(CriOS|EdgiOS|FxiOS|Version)\\/(\\d+(\\.\\d+)*)/.exec(a);if(h===l\u0026\u0026\"iOS\"===d\u0026\u0026i){let a={CriOS:\"Google Chrome\",EdgiOS:\"Microsoft Edge\",FxiOS:\"Mozilla Firefox\",Version:\"Apple Safari\"}[i[1]];g.push({brand:a,version:q(i[2])}),e\u0026\u0026!e.some(a=\u003ea.startsWith(\"Safari/\"))\u0026\u0026(f=!0)}break}}return 0===g.length\u0026\u0026(g=[{brand:\"Not;A Brand\",version:\"99.0.0.0\"}]),{fullVersionList:g,webview:f}}\n    function o(a){let{userAgent:b,platform:c,vendor:d}=a,e=b,f=!1,g=b.replace(/\\(([^)]+)\\)/g,(a,b)=\u003e(f||(e=b,f=!0),\"\")),h=g.match(/(\\S+)\\/(\\S+)/g),i=b.includes(\"Mobile\"),k=j(e,c),{fullVersionList:l,webview:m}=n(g,e,d,k.platform,h),o=l.length\u003e0?l[l.length-1].version:\"\",p=l.map(a=\u003e{let b=a.version.indexOf(\".\"),c=-1===b?a.version:a.version.slice(0,b);return{brand:a.brand,version:c}});return{mobile:i,platform:k.platform,brands:p,platformVersion:k.platformVersion,architecture:k.architecture||\"\",bitness:k.bitness||\"\",model:\"\",uaFullVersion:o,fullVersionList:l,webview:m}}\n    function p(a){switch(a){case\"x86_64\":case\"x64\":return[\"x86\",\"64\"];case\"x86_32\":case\"x86\":return[\"x86\",\"\"];case\"armv6l\":case\"armv7l\":case\"armv8l\":return[a,\"\"];case\"aarch64\":return[\"arm\",\"64\"];default:return[\"\",\"\"]}}\n    function q(a,b=3){let c=a.split(\".\");if(c.length\u003cb)for(;c.length\u003cb;)c.push(\"0\");return c.join(\".\")}\n    class r{constructor(a){this._clientHints=o(a),Object.defineProperties(this,{_clientHints:{enumerable:!1}})}get mobile(){return this._clientHints.mobile}get platform(){return this._clientHints.platform}get brands(){return this._clientHints.brands}getHighEntropyValues(a){return new Promise(b=\u003e{if(!Array.isArray(a))throw TypeError(\"Argument hints must be an array\");let c=new Set(a),d=this._clientHints,e={mobile:d.mobile,platform:d.platform,brands:d.brands};c.has(\"architecture\")\u0026\u0026(e.architecture=d.architecture),c.has(\"bitness\")\u0026\u0026(e.bitness=d.bitness),c.has(\"model\")\u0026\u0026(e.model=d.model),c.has(\"platformVersion\")\u0026\u0026(e.platformVersion=d.platformVersion),c.has(\"uaFullVersion\")\u0026\u0026(e.uaFullVersion=d.uaFullVersion),c.has(\"fullVersionList\")\u0026\u0026(e.fullVersionList=d.fullVersionList),b(e)})}toJSON(){return{mobile:this._clientHints.mobile,brands:this._clientHints.brands}}}\n    function(){if(\"https:\"===location.protocol\u0026\u0026!navigator.userAgentData){Object.defineProperty(r.prototype,Symbol.toStringTag,{enumerable:!1,configurable:!0,writable:!1,value:\"NavigatorUAData\"});let a=new r(navigator);return Object.defineProperty(Navigator.prototype,\"userAgentData\",{enumerable:!0,configurable:!0,get:function(){return a}}),Object.defineProperty(window,\"NavigatorUAData\",{enumerable:!1,configurable:!0,writable:!0,value:r}),!0}return!1}\n    polyfill();\n  }\n  applyUADataPolyfill();\n"])</script><script nonce="">self.__next_f.push([1,"38:[[[\"$\",\"script\",null,{\"id\":\"hasOwnPolyfill\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"suppressHydrationWarning\":true,\"children\":\"('hasOwn' in Object) || (Object.hasOwn = Object.call.bind(Object.hasOwnProperty));\"}],[\"$\",\"script\",null,{\"id\":\"UserAGentUADataPolyfill\",\"nonce\":\"99442c83-224a-4e1c-a471-dfa207be1201\",\"suppressHydrationWarning\":true,\"children\":\"$4d\"}]],\"$L4e\",\"$L4f\",\"$L50\",\"$L51\",\"$L52\"]\n"])</script><script nonce="">self.__next_f.push([1,"53:I[40678,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"Link\"]\n54:I[18694,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"Button\"]\n"])</script><script nonce="">self.__next_f.push([1,"4f:[\"$\",\"section\",null,{\"id\":\"error\",\"className\":\"styles_root__IBJUG styles_gutter-bottom__5KdTv styles_root__nqTMc\",\"style\":{\"--gutter-size-xs\":\"140px\",\"--gutter-size-sm\":\"140px\",\"--gutter-size-lg\":\"400px\",\"--gutter-size-xl\":\"400px\"},\"children\":[[\"$\",\"div\",null,{\"className\":\"styles_background-image__Etqcy\",\"style\":{\"backgroundImage\":\"url(https://yastatic.net/s3/education-portal/media/404_63c36c7c45.webp)\"}}],[\"$\",\"section\",null,{\"className\":\"styles_root__IBJUG styles_container__q8aSe\",\"style\":{\"--gutter-size-xs\":\"50px\",\"--gutter-size-sm\":\"50px\",\"--gutter-size-lg\":\"50px\",\"--gutter-size-xl\":\"50px\"},\"children\":[null,[\"$\",\"span\",null,{\"ref\":\"$undefined\",\"className\":\"styles_root__EmBCZ styles_description__ZE0yq\",\"style\":{},\"children\":\"Такой страницы нет,\\nвернитесь на главную\",\"data-variant\":\"$undefined\",\"data-weight\":\"regular\",\"data-color\":\"primary\"}],[\"$\",\"$L53\",null,{\"Text\":\"Вернуться на главную\",\"URL\":\"/\",\"Variant\":\"primary\",\"children\":[\"$\",\"$L54\",null,{\"as\":\"span\",\"view\":\"primary\",\"children\":\"Вернуться на главную\"}]}]]}]]}]\n"])</script><script nonce="">self.__next_f.push([1,"52:[\"$\",\"title\",null,{\"children\":\"Страница не найдена - 404 Not Found\"}]\n"])</script><script nonce="">self.__next_f.push([1,"4e:[[\"$\",\"$1\",\"web-page.block-0\",{\"children\":[\"$L55\"]}]]\n50:null\n51:null\n"])</script><script nonce="">self.__next_f.push([1,"55:[\"$\",\"$L2d\",\"web-page.header-block\",{\"id\":2,\"Authorization\":true,\"Search\":true,\"ProfileUrl\":\"/profile\",\"Wide\":null,\"Notifications\":true,\"Gutter\":null,\"Buttons\":[],\"Menu\":[{\"id\":19,\"Text\":\"Школьникам\",\"URL\":\"/pupils\",\"Title\":\"Школьникам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_school\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":null,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4136,\"Text\":\"Студентам\",\"URL\":\"/students\",\"Title\":\"Студентам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_students\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4137,\"Text\":\"Абитуриентам\",\"URL\":\"/university\",\"Title\":\"Абитуриентам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_abit\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4138,\"Text\":\"Партнёрам\",\"URL\":\"https://edumakers.yandex.ru/\",\"Title\":\"Партнёрам\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_partner\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4139,\"Text\":\"Хендбуки\",\"URL\":\"/handbook\",\"Title\":\"События\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4140,\"Text\":\"Журнал\",\"URL\":\"/journal\",\"Title\":\"Журнал\",\"MetrikaCounter\":null,\"MetrikaGoalId\":\"h_journal\",\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},{\"id\":4141,\"Text\":\"AI\",\"URL\":\"https://education.yandex.ru/ai\",\"Title\":\"AI\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":false,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":true,\"Description\":null,\"MetrikaGoalParams\":null}],\"Logo\":{\"id\":3,\"URL\":\"/\",\"Title\":\"Яндекс Образование\",\"Image\":{\"data\":{\"id\":8,\"attributes\":{\"name\":\"edu_logo.svg\",\"alternativeText\":null,\"caption\":null,\"width\":233,\"height\":24,\"formats\":null,\"hash\":\"edu_logo_9eee76ff17\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":13.44,\"url\":\"https://yastatic.net/s3/education-portal/media/edu_logo_9eee76ff17.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-02-09T11:00:59.235Z\",\"updatedAt\":\"2024-12-04T04:36:42.717Z\"}}},\"ImageMobile\":{\"data\":{\"id\":10764,\"attributes\":{\"name\":\"logo_mobile.svg\",\"alternativeText\":null,\"caption\":null,\"width\":179,\"height\":25,\"formats\":null,\"hash\":\"logo_mobile_8bc5eb38fb\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":18.25,\"url\":\"https://yastatic.net/s3/education-portal/media/logo_mobile_8bc5eb38fb.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2024-07-17T12:53:33.633Z\",\"updatedAt\":\"2024-12-04T04:35:29.061Z\"}}}},\"isLoggedIn\":false,\"passportUrl\":\"https://passport.yandex.ru/auth/?origin=education\u0026retpath=https%3A%2F%2Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis\",\"passportEnv\":\"prod\",\"passportUserData\":{\"avatarId\":\"$undefined\",\"plus\":false,\"name\":\"$undefined\"}}]\n"])</script><script nonce="">self.__next_f.push([1,"20:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Чему вы научились\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"manifest\",\"href\":\"/manifest.webmanifest\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"initial-scale=1, width=device-width\"}],[\"$\",\"meta\",\"4\",{\"name\":\"mobile-web-app-capable\",\"content\":\"yes\"}],[\"$\",\"meta\",\"5\",{\"name\":\"BookSlug\",\"content\":\"math\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://education.yandex.ru/handbook/math/article/math-glava-chetire-chemu-vi-nauchilis\"}],[\"$\",\"meta\",\"7\",{\"name\":\"mobile-web-app-capable\",\"content\":\"yes\"}],[\"$\",\"meta\",\"8\",{\"name\":\"apple-mobile-web-app-title\",\"content\":\"Яндекс Образование\"}],[\"$\",\"link\",\"9\",{\"href\":\"https://yastatic.net/s3/education-portal/pwa/logos/ios/512.png\",\"media\":\"$undefined\",\"rel\":\"apple-touch-startup-image\"}],[\"$\",\"meta\",\"10\",{\"name\":\"apple-mobile-web-app-status-bar-style\",\"content\":\"black-translucent\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:title\",\"content\":\"Чему вы научились\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:description\",\"content\":\"Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов.\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:url\",\"content\":\"https://education.yandex.ru/handbook/math/article/math-glava-chetire-chemu-vi-nauchilis\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image\",\"content\":\"https://yastatic.net/s3/education-portal/media/opengraf_4d3d33f601_7c25b24838.webp\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:title\",\"content\":\"Чему вы научились\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:description\",\"content\":\"Чему вы научились - Хендбук от Яндекс.Образования. Откройте для себя передовые подходы, практические советы и вдохновляющие идеи от наших экспертов.\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:image\",\"content\":\"https://yastatic.net/s3/education-portal/media/opengraf_4d3d33f601_7c25b24838.webp\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script nonce="">self.__next_f.push([1,"25:\"$20:metadata\"\n"])</script><script nonce="">self.__next_f.push([1,"56:I[97469,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"HandbookBookProgressProvider\"]\n57:I[63314,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"SelectedNoteProvider\"]\n58:I[71379,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"ArticleSidePanel\"]\n59:T14e4,"])</script><script nonce="">self.__next_f.push([1,"Этот учебник написан группой людей, влюблённых в математику и анализ данных. Наша цель — помочь вам посмотреть на математику под таким углом, под которым вы на неё ещё не смотрели.\n\nНе у всех была возможность увлечься этим предметом. Может, в школе (если повезло с преподавателем) решать примеры и несложные задачки было интересно и весело. А вот в университете вычислять интегралы, детерминанты, решать матричные уравнения и задачи оптимизации наверняка было уже не так увлекательно.\n\nКто-то мог подумать: «А зачем мне это? Почему я трачу своё время на эти неживые и никому не нужные понятия и алгоритмы?» И любовь к математике угасала.\n\nНо сейчас постоянно на слуху вещи, связанные с анализом данных: чистая аналитика, машинное или глубинное обучение. И там, кроме кода, приходится сталкиваться с большим количеством математики внутри алгоритмов. Собственно, весь анализ данных — это математика.\n\nИ здесь как раз могут пригодиться давно забытые знания высшей математики из института. Мы поможем вам вспомнить их — или открыть заново. Так, чтобы математика осталась красивой строгой наукой, с определениями и доказательствами, но при этом чтобы вы понимали, зачем нужен тот или иной математический инструмент в практических задачах анализа данных.\n\nЭтот учебник пригодится тем, кто:\n\n- изучал математику раньше, но хочет осознать её уже в контексте анализа данных;\n- прямо сейчас изучает машинное или глубинное обучение и чувствует некоторые пробелы в своих математических знаниях;\n- связан с рекламой и маркетингом, медициной, социальными науками или экономикой — для расширения кругозора и поиска новых идей, которые могут помочь в работе.\n\nХендбук состоит из нескольких глав. Каждая посвящена кусочку некоторой большой науки — это может быть математический анализ, линейная алгебра, теория вероятностей и дискретная математика или теория графов.\n\nГлавы будут идти вперемешку: в первых мы расскажем об основных понятиях из каждой науки и посмотрим, какие задачи анализа данных можно решать, оперируя лишь базовыми знаниями высшей математики. А в следующих углубимся в различные аспекты высшей математики, обязательно с задачами из аналитики, машинного или глубинного обучения.\n\nМы постарались сделать хендбук максимально практическим и интерактивным —\u0026nbsp;разбавили теорию примерами, квизами и кусочками кода на Python. Также мы подготовили задания, где необходимо самостоятельно писать код и применять те или иные математические инструменты и конструкции в задачах анализа данных.\n\n**Важно:** мы предполагаем, что вы владеете Python и у вас уже установлено необходимое окружение. Если нет — ничего страшного, это не помешает вам овладеть теорией. Но на всякий случай — вот [хендбук](https://education.yandex.ru/handbook/python) «Основы Python». А здесь —\u0026nbsp;[инструкция](https://education.yandex.ru/handbook/python/article/interpretatora-i-nastroika-sredi-razrabotki-pervii-shag-v-programmirovanii) по установке Python.\n\nВот и всё. Желаем вам провести время с нашим хендбуком не только с пользой, но и с удовольствием!"])</script><script nonce="">self.__next_f.push([1,"5a:T19e2,"])</script><script nonce="">self.__next_f.push([1,"В конце некоторых параграфов вы можете встретить задачи, которые помогут вам лучше разобраться в теме. В этом приложении мы расскажем, как работать с системой их проверки. Спойлер: это не сложно, но есть нюансы.\n\nДавайте разбираться!\n\n## Интерфейс\n\nСамое главное: мы рекомендуем решать задачи с десктопных устройств. Это удобнее, плюс в мобильной версии некоторые элементы могут отображаться некорректно.\n\nДалее — поговорим об интерфейсе. При переходе внутрь задачи вы увидите в левой части экрана описание, а в правой — поле редактора, в котором можно писать код решения.\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_6_fcc14cc482.webp)\n\nОписание задачи включает:\n\n- Условие.\n\n- Формат ввода/вывода. В этом блоке описаны данные, аналогичные тем, что будут переданы в вашу программу при автоматической проверке задания. Конкретный пример входных данных можно посмотреть в блоке «Пример» (о нём далее).\n\n- Пример. Здесь показаны результаты, которые должна выдать программа при правильной обработке данных.\n\n- Ограничения по памяти и времени, в которые должна уложиться ваша программа.\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_7_ca72f38eb8.webp)\n\nОтправить решение можно через поле редактора, либо загрузив файл с вашей программой.\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_8_86d233ea2b.webp)\n\nПосле отправки решения система обработает ваш код и начнёт тестировать его на различных вариантах входных данных, сопоставляя ожидаемый эталонный вывод с результатами работы вашего кода.\n\nПары входных и выходных тестовых данных называются тестами.\n\nПроверка решения может занять некоторое время, отследить прогресс проверки вы можете на вкладке «Отправленные решения».\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_9_9d96765712.webp)\n\nВ зависимости от результата проверки вы увидите статус:\n\n- Решена полностью — ваше решение успешно скомпилировалось и прошло все тесты.\n- Решена неверно — код не компилируется или не проходит тестирование.\n\n## Как понять в чем ошибка?\n\nЧтобы узнать детали проверки, нужно перейти внутрь теста:\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_10_faa5910cbc.webp)\n\nЗдесь вы увидите вердикт по решению. Вот некоторые варианты вердиктов:\n\n1. **OK** — решение прошло все тесты.\n\n2. **CE (Compilation Error)** — ошибка компиляции, в программе допущена синтаксическая или семантическая ошибка.\n\nВ этом случае разобраться поможет отчёт об ошибке в блоке «Лог компиляции»:\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_11_3a9498dc89.webp)\n\nСистема подскажет, какого рода ошибка присутствует в коде, и в какой строке она находится.\n\n3. **WA (Wrong Answer)** — ваша программа выдала неправильный результат на одном из тестов.\n\nДля тестов из примеров отображается ввод, вывод вашей программы, вывод системы проверки ответа и правильный ответ. Это поможет с отладкой вашей программы.\n\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_12_d37061f9ad.webp)\n\nМы не раскрываем другие тестовые данные. Попробуйте самостоятельно рассмотреть различные нетривиальные варианты входных данных и проверить, как на них отрабатывает ваше решение. Это поможет вам научиться тестировать свой код самостоятельно.\n\n4. **RE (Runtime Error)** — ошибка выполнения. Например, это может быть ошибка выхода за границы массива или необработанные исключения.\n\n5. **TL (Time Limit)** — на одном из тестов решение работает слишком долго.\n\n6. **ML (Memory Limit)** — на одном из тестов решение потребляет слишком много памяти.\n\nПодробный список ответов проверяющей системы можно посмотреть [здесь](https://contest.yandex.ru/errors).\n\nВ случае, если вы чувствуете, что вам необходима поддержка и взгляд со стороны, попробуйте обсудить задачу с участниками [комьюнити](https://t.me/+bikU3_M1x0s0YzMy) хендбука.\n\nВот и всё! Надеемся, что эти советы помогут разобраться с системой проверки заданий. Желаем успехов в учёбе!"])</script><script nonce="">self.__next_f.push([1,"5b:Td61,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы разберёмся, как графы помогают моделировать окружающий нас мир, познакомимся с их ключевыми свойствами и узнаем, как они применяются для решения реальных задач.\n\nС помощью графов вы сможете:\n\n- **Оптимизировать маршруты** — например, планировать кратчайший путь для курьера.\n- **Решать задачи связей** — анализировать, как связаны пользователи соцсетей или как распространяются вирусы.\n- **Упрощать сложные системы** — визуализировать, как распределяются ресурсы в компании или как устроены сложные структуры.\n\nИ многое другое. Поверьте, графы — это мощный инструмент, который может помочь вам в жизни и работе. Вот как устроена эта глава: \n\n- В первом параграфе мы погрузимся в основы теории графов. Поговорим о том, что такое `вершины` и `рёбра` и как с их помощью можно описать связи между элементами любой системы. Вы научитесь видеть графы в повседневной жизни: в сети друзей в социальных медиа, в структуре организации или даже в связях между разными концепциями.\n- Во втором — рассмотрим различные виды графов. Разберёмся, чем отличаются обыкновенные графы от мультиграфов, что такое ориентированные и неориентированные графы, а также почему понятие `связности` так важно. Это поможет вам выбирать наиболее подходящую модель графа для решения конкретных задач.\n- В заключительном поговорим о `деревьях` — особом типе графов. Изучим их уникальные свойства, обсудим минимальные остовные деревья и познакомимся с алгоритмами, которые помогают находить оптимальные пути в системах с весами. Это особенно полезно для задач оптимизации, например при планировании эффективных маршрутов доставки или построении экономичных сетей.\n\nИзучив эту главу, вы сможете:\n\n- Понимать структуру графов и их основные элементы.\n- Классифицировать графы и выбирать подходящие модели для анализа.\n- Использовать свойства деревьев для решения задач оптимизации.\n\nДавайте приступим! "])</script><script nonce="">self.__next_f.push([1,"5c:T7522,"])</script><script nonce="">self.__next_f.push([1,"Представьте, что вы работаете в консалтинге.\u0026nbsp;К вам обратился крупный торговый центр, который хочет оптимизировать маршруты покупателей. Проблема такая: посетители распределяются неравномерно. Где-то их слишком много и возникают заторы, а где-то их совсем нет, и владельцы магазинов жалуются на нехватку клиентов.\n\nКонечно, можно методом проб и ошибок попробовать разные варианты навигации, но это потребует много ресурсов и не факт, что решит проблему. Другой вариант —\u0026nbsp;воспользоваться теорией графов.\n\nДело в том, что торговый центр можно отлично представить в виде графа: комнаты и коридоры будут узлами, а пути между ними — рёбрами. С такой моделью удобнее проводить вычисления и анализировать данные.\n\nНапример, мы можем смоделировать движение покупателей с помощью алгоритмов анализа графов и найти потенциальные точки перегрузок. Или предложить наиболее короткие маршруты для популярных мест из разных точек входа. Или сделать маршрут мимо наименее посещаемых магазинов.\n\n\u003e Вариантов много —\u0026nbsp;главное, что методами теории графов можно решать задачи оптимизации, находя оптимальные пути и решения для различных процессов. А наглядная визуализация делает структуру системы более понятной, облегчая восприятие и передачу информации.\n\nВ виде графа можно представить и многие другие системы. Главное, чтобы элементы системы были связаны друг с другом. Например, это могут быть:\n\n- **Транспортные сети.** Графы могут моделировать города как вершины и маршруты (дороги, железнодорожные линии) как рёбра.\n- **Компьютерные сети.** Компьютеры или серверы представлены как вершины, а сетевые соединения между ними — как рёбра.\n- **Биологические сети.** Графы используются для моделирования нейронных взаимодействий, где вершины представляют собой нейроны, а рёбра — синаптические связи между ними.\n\n![2.2.1..webp](https://yastatic.net/s3/education-portal/media/2_2_1_e70072a03b.webp)\n\n![2.2.1(1).webp](https://yastatic.net/s3/education-portal/media/2_2_1_1_248c4ec8a3.webp)\n\n![2.2.1(2).webp](https://yastatic.net/s3/education-portal/media/2_2_1_2_7f125a2795.webp)\n\nА ещё графы помогают выявлять скрытые закономерности и паттерны в данных, что облегчает анализ сложных систем. Например, с помощью графов маркетологи могут сегментировать покупателей — чтобы по добавленным в корзину товарам предложить покупателю вещи, которые могут его заинтересовать, или персональную скидку.\n\nНа этом возможности графов не заканчиваются. Модели машинного и глубинного обучения можно адаптировать для работы с данными графовой структуры. Например, графовые нейронные сети позволяют моделям учитывать все переплетения связей между объектами. Это как дать модели не только перечень продуктов, но и рецепты, показывающие, как они сочетаются друг с другом. В результате предсказания становятся более точными и осмысленными.\n\nВот, теперь вы знаете, зачем нужны графы. Далее разберёмся, что такое граф с точки зрения математики.\n\n## Фундаментальные понятия теории графов\n\nТеория графов основывается на нескольких фундаментальных понятиях. В этом разделе мы рассмотрим их детально и приведём примеры, чтобы вы лучше усвоили материал.\n\nГраф $G$ — это математическая структура, состоящая из двух множеств: множества вершин $V$и множества ребер $E$. Формально граф записывается как\\\n$G = (V, E)$, где:\n\n- $V$ — множество вершин, представляющих объекты или сущности $(V \\neq \\emptyset)$,\n- $E$ — множество ребер, представляющих связи или отношения между парами вершин.\n\n`Вершина` — базовый элемент графа, обозначающий объект или сущность в рассматриваемой системе. Вершины обычно обозначаются заглавными буквами $(A,B,C)$ или числами $(1,2,3)$. Степенью вершины обозначают количество рёбер, прилежащих к вершине.\n\n{% cut \"Некоторые свойства вершин\" %}\n\n**Изолированная вершина.** Вершина называется изолированной, если она не связана ни с одной другой вершиной, то есть из неё не выходит ни одно ребро. Изолированные вершины часто символизируют объекты без связей с остальной частью системы.\n\n**Висячая вершина.** Вершина называется висячей, если из неё выходит ровно одно ребро. Такая вершина всегда является концом только одного ребра, которое соединяет её с другой вершиной, и может служить концом пути или периферийным элементом в графе.\n\n**Шарнир.** Вершина графа называется шарниром, если её удаление приводит к потере связности графа (граф перестаёт быть единым целым и распадается на несколько частей). Шарнирные вершины являются критическими точками, связывающими части графа.\n\n{% endcut %}\n\n`Ребро` — соединение между двумя вершинами, отражающее существование связи или отношения между ними.\n\n{% cut \"Некоторые свойства рёбер\" %}\n\n**Смежные рёбра.** Два ребра называются смежными, если они имеют общую вершину. Это означает, что они делят одну и ту же вершину, сходясь в ней.\n\n**Кратные рёбра.** Два ребра называются кратными, если они соединяют одну и ту же пару вершин. Такие рёбра позволяют моделировать множественные связи между одними и теми же объектами в графе.\n\n**Направленные рёбра.** Ребро называется направленным, если оно имеет определённое направление из одной вершины в другую. Такое ребро соединяет упорядоченную пару вершин и направление указывает, из какой вершины ребро выходит и в какую входит. Направленные рёбра позволяют моделировать асимметричные отношения между вершинами.\n\n**Петля.** Так называют ребро, которое соединяет вершину с самой собой. В таком случае оба конца ребра совпадают с одной и той же вершиной.\n\n**Мост.** Мостом называется такое ребро графа, удаление которого приводит к увеличению числа компонент связности графа. Иными словами, после удаления моста граф становится менее связным.\n\n{% endcut %}\n\nВизуализируем понятия на двух графах.\n\n![2.2.2.webp](https://yastatic.net/s3/education-portal/media/2_2_2_0fc7c2d791.webp)\n\nПервый граф можно представить как сеть гиперссылок между веб-страницами: каждая вершина соответствует отдельной странице, а ребро указывает на существование ссылки между ними. В этой модели страница 5 содержит ссылку на саму себя (петля), страницы 1 и 3 имеют по две взаимные ссылки друг на друга (кратное ребро), а страница 9 полностью изолирована — на неё нет ссылок, и она сама ни на кого не ссылается.\n\nВторой граф можно представить как граф финансовых транзакций между тремя людьми — как они переводили друг другу деньги.\n\nПрежде чем двинуться дальше, обратите внимание на визуализацию свойств рёбер и вершин этих графов:\n\n- Петля — ребро 5–5.\n- Кратное ребро — 1–3.\n- Мосты — 3–5, 4–6, 6–7, 6–8..\n- Направленные рёбра — 10–11–12.\n- Шарниры — вершины 3, 4, 6.\n- Висячие вершины — 7, 8.\n- Изолированная вершина — 9.\n\nДвинемся дальше. К фундаментальным понятиям также относятся `инцидентность` и `смежность`. Они описывают отношения между вершинами и рёбрами в графе.\n\nВершина и ребро считаются инцидентными, если это ребро соединяет данную вершину с какой-либо другой вершиной.\n\n- **Инцидентное (прилежащее) ребро:** ребро, которое соединяет данную вершину с другой вершиной.\n- **Инцидентная (прилежащая) вершина:** вершина, которая является концом данного ребра.\n\nЗдесь важно отметить, что инцидентность относится исключительно к паре «вершина — ребро». Это означает, что ребро инцидентно своим конечным вершинам и каждая вершина инцидентна всем рёбрам, которые к ней прилегают.\n\nЭто может звучать запутанно, разберём на примере. Посмотрите на граф:\n\n![2.2.3.webp](https://yastatic.net/s3/education-portal/media/2_2_3_4896123f2d.webp)\n\nНа этом графе:\n\n- вершины 1 и 2 инцидентны ребру 1–2;\n- вершины 1 и 3 инцидентны ребру 1–3;\n- ребро 1–2 инцидентно вершинам 1 и 2;\n- ребро 1–3 инцидентно вершинам 1 и 3.\n\nСмежность, в свою очередь, описывает отношения между двумя вершинами или двумя рёбрами:\n\n- **Смежные вершины**: две вершины считаются смежными, если они соединены общим ребром.\n- **Смежные рёбра:** два ребра считаются смежными, если они имеют общую вершину, через которую соединяются.\n\nНа иллюстрации выше:\n\n- вершины 1, 2 и 1, 3 — смежные;\n- рёбра 1–2 и 1–3 — смежные.\n\n\u003e Выходит, что инцидентность показывает связь между вершинами и рёбрами, а смежность помогает описать взаимное расположение вершин и рёбер в графе.\n\nЕщё одно понятие — это `путь`. Путём называют последовательность вершин, в которой каждая пара соседних вершин соединена ребром. Например, в графе можно рассмотреть путь `1` — `3` — `2` — `5`, где каждая вершина последовательно связана с предыдущей, ведь, чтобы следовать по пути, нужно двигаться только по существующим рёбрам графа, переходя от одной вершины к другой.\n\nПуть называется `простым`, если все его вершины попарно различны. Это ограничение означает, что в простом пути вершины не повторяются, что может быть важным при анализе графов, где нужно избегать `циклов` или `замкнутых маршрутов`.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"4\" src=\"https://yastatic.net/s3/education-portal/media/2_2_4_503c24eafd.webp\"\u003e\n  \u003cfigcaption\u003e\n    Простой путь\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cfigure\u003e\n  \u003cimg alt=\"4\" src=\"https://yastatic.net/s3/education-portal/media/2_2_4_1_28c0f9dea2.webp\"\u003e\n  \u003cfigcaption\u003e\n    Цикл\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n`Цикл` — это особый тип пути, который начинается и заканчивается в одной и той же вершине. Цикл можно записывать с любой вершины, с которой он начинается. Например, для цикла `2` — `3` — `5` — `2` начальная и конечная вершина совпадают, что делает его замкнутым.\n\n`Длина цикла` — это количество рёбер, входящих в него, то есть в данном случае длина равна трём. Цикл называется `простым`, если в его составе все вершины и рёбра различны, за исключением начальной и конечной вершины, которые совпадают.\n\nПродолжим. Граф называется `полным`, если все его вершины соединены между собой рёбрами. В таком графе для $V$ вершин максимальное количество рёбер равно $\\frac{V(V - 1)}{2}$ в `неориентированном` графе и $V(V - 1)$ в `ориентированном`.\n\n\u003e Подробнее о неориентированных и ориентированных графах мы поговорим в следующем параграфе. Пока просто запомните эти термины — они означают, есть в графе направленные рёбра или нет.\n\n`Плотный граф` — это граф, в котором число рёбер близко к максимально возможному. Если же число рёбер значительно меньше максимального, граф называется `разреженным`.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"4\" src=\"https://yastatic.net/s3/education-portal/media/2_2_5_885eb71811.webp\"\u003e\n  \u003cfigcaption\u003e\n    Полный, плотный и разреженный графы\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nКроме графов существуют и `подграфы`. Для начала дадим строгое описание, а потом поясним простыми словами.\n\nИтак, подграф — это граф $Н$, вершины и ребра которого являются подмножествами вершин и ребер исходного графа $G$. Если взять множество вершин $V'$ графа $H$ как подмножество вершин $V$ исходного графа $G$ и множество ребер $E'$ графа $H$, соединяющих вершины из $V'$, как подмножество ребер $E$ графа $G$, то такой граф $H$ называется подграфом графа $G$.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"5\" src=\"https://yastatic.net/s3/education-portal/media/2_2_6_e81db79f67.webp\"\u003e\n  \u003cfigcaption\u003e\n    Граф G и его подграф H\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПроще говоря, если в исходном графе удалить одну вершину вместе со всеми рёбрами, которые её соединяют с другими вершинами (то есть инцидентными ей рёбрами), то получившаяся структура будет подграфом исходного графа.\n\nПодграф можно получить также, выделив несколько вершин и рёбра между ними в исходном графе, что создаст локализованное подмножество связей, сохраняющее структуру и свойства исходной модели. Это бывает полезно для анализа отдельных частей сложной системы, когда необходимо сконцентрироваться на конкретных элементах или изучить локальные закономерности без учёта остальной части графа.\n\nС терминами разобрались. Теперь поговорим о том, как построить граф.\n\n## Как задать граф: способы представления\n\nГрафы можно описывать разными способами, у каждого из которых есть свои преимущества в зависимости от задачи.\n\nВот основные методы представления графов:\n\n- список рёбер;\n- матрица смежности;\n- набор степеней вершин.\n\nРассмотрим их подробнее.\n\n### Список рёбер\n\nЭто один из самых простых способов описания графа, где он представляется как набор пар вершин, соединённых рёбрами.\n\nВ неориентированном графе рёбра не имеют направления и связь между двумя вершинами двусторонняя. Поэтому каждая пара вершин записывается только один раз. В ориентированном графе, напротив, рёбра имеют направление, что позволяет моделировать односторонние связи. В этом случае каждая пара вершин записывается с учётом направления и указанием, от какой вершины к какой идёт связь.\n\n![2.2.7.webp](https://yastatic.net/s3/education-portal/media/2_2_7_1de539df00.webp)\n\nПреимущество представления графа списком рёбер заключается в простоте реализации этого подхода и удобстве его использования при небольшом количестве связей. Этот способ не требует большого объёма памяти и позволяет легко добавлять и удалять рёбра.\n\nОднако он может оказаться неэффективным для задач, требующих частого обращения к информации о степени вершин или быстрого поиска путей между двумя вершинами.\n\n### Матрица смежности\n\nЭто ещё один способ описания графа, который использует квадратную матрицу, где строки и столбцы представляют вершины. Элемент матрицы $a_{ij}$ равен $1$, если вершины $i$ и $j$ соединены, и $0$ — в противном случае. В неориентированном графе такая матрица симметрична относительно главной диагонали.\n\n![2.2.8.webp](https://yastatic.net/s3/education-portal/media/2_2_8_ac69c9f7a9.webp)\n\nНа визуализации выше матрица отображает, есть ли ребро между вершинами. Номера вершин —\u0026nbsp;порядковые номера строк и колонок. То есть в строке №1 в колонках №3 и №4 стоят $1$, что указывает на наличие рёбер, соединяющих вершину 1 с вершинами 3 и 4.\n\nЕсли у рёбер графа есть вес, то в матрице смежности вместо $1$ можно записывать вес ребра, соединяющего вершины $i$ и $j$. Это позволяет представлять взвешенные графы и учитывать интенсивность или стоимость связей между вершинами.\n\nПомните, выше мы говорили про `плотные` и `разреженные` графы? Так вот, для разреженных графов матрица смежности становится менее эффективной, поскольку большинство её элементов равны нулю.\n\nВ таких случаях применяется альтернативное представление — `список смежности`, где для каждой вершины хранится список индексов вершин, с которыми она соединена. Этот метод значительно экономит память для разреженных графов.\n\nНапример, для графа с миллионом вершин, где вершины соединены по кругу, матрица смежности потребовала бы хранения $10^6 \\times 10^6$ элементов, тогда как список смежности потребует всего миллион записей, в каждой из которых будет всего два индекса.\n\n### Набор степеней вершин\n\nВ данном случае граф представляется списком степеней каждой вершины, то есть количеством рёбер, которые к ней прилегают. Этот метод удобен для анализа свойств графа, таких как плотность связей или распределение степеней вершин.\n\nНа иллюстрации ниже — два графа, в каждом из которых степень каждой вершины равна 2. Однако графы различны: в первом графе вершины соединены так, что образуются две отдельные компоненты, а во втором графе вершины соединены в замкнутый цикл.\n\n![2.2.9.webp](https://yastatic.net/s3/education-portal/media/2_2_9_4a006fc256.webp)\n\nОсновное преимущество представления графа через набор степеней — его компактность, особенно если у графа большое количество вершин и небольшое количество рёбер. Однако недостаток этого способа заключается в том, что он не позволяет точно восстановить структуру графа, так как множество различных графов могут иметь одинаковое распределение степеней вершин — как вы могли убедиться на иллюстрации выше.\n\n## Визуализация графов с помощью Python\n\nТеперь, когда мы разобрались с основными понятиями теории графов и способами их описания, давайте перейдём к практическим примерам. В этом разделе мы познакомимся с библиотекой `networkx`, которая позволяет создавать и визуализировать графы разными способами.\n\nПосмотрим на код:\n\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np \n\n# 1. Создание графа из списка рёбер\nedges = [(1, 2), (1, 3), (2, 3), (2, 4), (2, 5), (3, 5)]\ngraph1 = nx.Graph()\ngraph1.add_edges_from(edges)\n\n# 2. Создание графа из матрицы смежности\nadjacency_matrix = np.array([\n    [0, 1, 1, 0, 0],\n    [1, 0, 1, 1, 1],\n    [1, 1, 0, 0, 1],\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 0, 0]\n])\ngraph2 = nx.from_numpy_array(adjacency_matrix)\n\n# 3. Создание графа из списка степеней вершин\ndegrees = [4, 2, 2, 2, 2]\ngraph3 = nx.random_degree_sequence_graph(degrees)  # Создание случайного графа с заданными степенями\n\n# Визуализация графов\nplt.figure(figsize=(12, 4))\n\nplt.subplot(131)\nnx.draw(graph1, with_labels=True)\nplt.title('Из списка рёбер')\n\nplt.subplot(132)\nnx.draw(graph2, with_labels=True)\nplt.title('Из матрицы смежности')\n\nplt.subplot(133)\nnx.draw(graph3, with_labels=True)\nplt.title('Из списка степеней')\n\nplt.tight_layout()\nplt.show()\n```\n\nВот что тут происходит:\n\n- Мы создали три графа, используя разные методы описания графов.\n- Затем `plt.subplot` разделил график на три части для отображения всех трёх созданных графов.\n- Для визуализации мы использовали функцию `nx.draw()`, которая рисует граф.\n- Также с помощью параметра `with_labels=True` мы добавили метки к вершинам.\n\nИ вот какая получилась визуализация:\n\n![2.2.10.webp](https://yastatic.net/s3/education-portal/media/2_2_10_4490101fc4.webp)\n\nНа этом всё — увидимся в следующем параграфе. А чтобы закрепить новые знания, рекомендуем вам пройти квиз и прорешать задачи для этого параграфа. Но прежде чем приступить к ним, советуем сперва взглянуть на небольшой\u0026nbsp;гайд\u0026nbsp;о том, как пользоваться системой проверки заданий.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715783.74d68e54244a818308404c3f43bec5e586d9e8db/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715783.74d68e54244a818308404c3f43bec5e586d9e8db\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"5d:T8eea,"])</script><script nonce="">self.__next_f.push([1,"При исследовании графов важно не только знать способы их хранения и анализа, но и понимать их особенные характеристики — от этого зависит выбор подходящего для вашей задачи графа.\n\nРассмотрим, например, граф социальных связей. Если мы выберем группу из пяти случайных людей, перед нами откроются две интересные перспективы:\n\n1. Если мы хотим узнать, кто с кем знаком, можно представить людей как вершины графа, а факт знакомства — как рёбра. Такой граф покажет все дружеские связи внутри группы.\n2. Если же нас интересуют денежные переводы между ними, мы построим ориентированный граф. Здесь вершины будут представлять людей, а рёбра — транзакции, и каждое ребро укажет направление, от кого к кому поступили средства.\n\nЧтобы точнее описать различные сценарии и отношения, давайте разберёмся, как графы классифицируются в зависимости от рёбер, петель, направленности и других характеристик.\n\nТакже в этом параграфе мы познакомимся с леммой о рукопожатиях — важным результатом, который связывает структуру графа с его свойствами. Понимание этой леммы позволит глубже разобраться в особенностях различных типов графов и их взаимосвязях.\n\n## Классификация №1: по кратным рёбрам и петлям\n\nПервый шаг в классификации графов — анализ наличия кратных рёбер и петель.\n\n`Обыкновенный граф` — это строгая, упорядоченная структура, где между любыми двумя вершинами может быть не более одного ребра и отсутствуют петли, то есть рёбра, соединяющие вершину саму с собой.\n\nТакой тип графа хорош для ситуаций, где важно учитывать лишь одно отношение между элементами, например в логистическом графе транспортной сети с двусторонними дорогами, где каждая связь между двумя точками представлена единственным ребром.\n\nЕсли граф допускает кратные рёбра, то он называется `мультиграфом`. В этом случае между двумя вершинами может быть несколько рёбер, а также разрешаются петли.\n\nМультиграф подойдёт, если нужно моделировать систему, где может существовать несколько видов связей между одними и теми же элементами. Такие системы включают транспортные сети с различными маршрутами между городами (автобусные, железнодорожные, авиалинии) или компьютерные сети со множеством каналов связи между серверами.\n\n`Граф с петлями` допускает наличие рёбер, соединяющих вершину саму с собой. Это полезно, когда важно учитывать возможные самоотношения, например обратные связи в системе.\n\nДанные графы применяются, например, в сетях рекомендаций, где петля может указывать на повторное взаимодействие пользователя с одним и тем же объектом (например, заказ любимого блюда в ресторане или повторное приобретение товара).\n\n`Пустой граф`, в отличие от всех предыдущих, представляет собой структуру без рёбер, состоящую только из изолированных вершин. Здесь каждый элемент автономен и не имеет связей с другими.\n\nОн редко встречается на практике, однако играет важную роль в теории графов — используется при изучении предельных случаев и формулировке общих теорем, где необходимо учитывать все возможные типы графов, включая крайние. То есть пустой граф — это теоретическая модель, а иногда и отправная точка для построения более сложных структур.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"1\" src=\"https://yastatic.net/s3/education-portal/media/2_3_1_f4d6c4fd93.webp\"\u003e\n  \u003cfigcaption\u003e\nОбыкновенный граф, мультиграф, граф с петлями и пустой граф\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n## Классификация №2: по направленности рёбер\n\nДругая важная характеристика графов — это направленность рёбер. В предыдущем параграфе мы коротко коснулись этой темы, сейчас разберёмся полнее.\n\n`Неориентированный граф` — это граф, где рёбра не имеют направления, что делает связь между вершинами двусторонней. Например, если вершина $A$ соединена с вершиной $B$, то и $A$ связана с $B$, и $B$ связана с $A$.\n\nТакие графы удобны для моделирования симметричных отношений, где взаимосвязи равноправны. Например, такую модель можно использовать, чтобы отразить социальный граф знакомств, где связь между двумя людьми указывает на взаимное согласие на общение.\n\nУ `ориентированного графа`, напротив, есть направление рёбер, что делает связи между вершинами односторонними. Если, например, ребро направлено от вершины $A$ к вершине $B$, это означает, что $A$ связана с $B$, но не наоборот.\n\nТакой граф полезен для направленных взаимодействий. Например, в гейм-дизайне для проектирования сюжетных линий в зависимости от выбора игрока.\n\nНеориентированный граф можно рассматривать как частный случай ориентированного, в котором каждое ребро направлено в обе стороны.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"2\" src=\"https://yastatic.net/s3/education-portal/media/2_3_2_b3467b9cf8.webp\"\u003e\n  \u003cfigcaption\u003e\nНеориентированный и ориентированный граф\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n## Классификация №3: по связности\n\nГрафы также различаются по степени связности.\n\n`Связный неориентированный граф` — это граф, в котором можно добраться от любой вершины к любой другой по последовательности рёбер. Такой граф образует единую компоненту связности, что полезно в моделях, где каждый элемент связан с другими.\n\nНапример, в логистической сети транспортной системы города или пункты доставки представлены вершинами, а дороги между ними — рёбрами. `Связный граф` в этом случае гарантирует, что из любого города можно доставить груз в любой другой, что важно для эффективного функционирования логистики и оптимизации маршрутов.\n\nВ случае `сильно связного ориентированного графа` каждая вершина достижима из любой другой с учётом направлений рёбер. Примером такого графа может служить система обмена электронными сообщениями внутри организации, где каждый сотрудник может отправить сообщение любому другому сотруднику через внутреннюю почтовую систему.\n\nЗдесь узлы — сотрудники, а направленные рёбра — возможность отправки сообщения от одного сотрудника к другому. Сильная связность обеспечивает полную коммуникацию внутри организации.\n\n`Слабо связный ориентированный граф` — это граф, который становится связным, если его рёбра рассматривать как неориентированные. Иными словами, если игнорировать направления рёбер, вершины будут соединены.\n\nНапример, в графе подписок социальных сетей, где узлы представляют пользователей, а направленные рёбра — подписки одного пользователя на другого, такая связность позволяет изучать общую структуру сети через цепочки подписок, образуя связный граф.\n\nОднако важно подчеркнуть, что слабо связный граф не становится сильно связным, поскольку достижимость всех вершин в обоих направлениях в нём не гарантируется.\n\nВот как выглядят эти графы:\n\n![2.3.3.webp](https://yastatic.net/s3/education-portal/media/2_3_3_419e7a8fd5.webp)\n\nНа иллюстрации выше:\n\n- Граф 1–6 — связный неориентированный.\n- Граф 7–10 — сильно связный ориентированный.\n- Граф 11–14 — слабо связный ориентированный.\n\n## Классификация №4: по полноте\n\nМы коснулись полноты в предыдущем параграфе. Давайте чуть углубимся.\n\n`Полные графы` — это графы, в которых каждая пара различных вершин соединена рёбрами. В `полном неориентированном графе` каждая пара вершин соединена ровно одним ребром, что создаёт максимально плотную структуру, где каждая вершина напрямую связана с любой другой.\n\nВ контексте анализа данных полный граф может использоваться для отображения всех возможных связей между объектами, например для вычисления попарных расстояний или сходства между элементами в задаче кластеризации.\n\nВ `полном ориентированном графе` каждая пара различных вершин соединена двумя рёбрами, направленными в противоположные стороны. Такое построение позволяет каждой вершине иметь прямую и обратную связь с любой другой.\n\nПолные ориентированные графы могут быть полезны для моделирования систем, где важно учитывать двусторонние взаимодействия между всеми элементами: допустим, в сети финансовых потоков такая структура может отображать движение средств между узлами (компаниями, банками или регионами), где каждая вершина связана с любой другой как отправителем, так и получателем. Это помогает анализировать максимальные потоки, баланс транзакций и выявлять ключевые точки концентрации финансовых ресурсов.\n\nТеперь, разобрав основные типы графов и их классификации, рассмотрим одно из фундаментальных свойств графов — `лемму о рукопожатиях`.\n\n\u003e Это полезный инструмент — она позволяет быстро определить общее количество связей в графе по количеству связей каждого элемента. Зная это, мы можем получить другие данные или проверить правильность нашего графа.\n\nДавайте посмотрим, как это работает в реальной ситуации.\n\n## Лемма о рукопожатиях\n\nПредставьте вечеринку, где каждый гость поочерёдно здоровается с остальными гостями.  Сколько всего рукопожатий между ними произошло? Чтобы ответить на этот вопрос, представим гостей как вершины графа, а рукопожатия между ними — как рёбра, соединяющие вершины.\n\nЛемма о рукопожатиях утверждает, что сумма степеней всех вершин графа всегда равна удвоенному числу его рёбер. Это значит, что, чтобы найти количество рукопожатий на вечеринке, нужно сложить количество рукопожатий каждого гостя и разделить на 2 — потому что каждое рукопожатие учитывается дважды: по одному разу для каждого из участников.\n\nФормально это можно записать следующим образом:\n\n$\\sum_{v∈V(G)} \\deg(v) = 2⋅|E(G)|$, где:\n\n- $\\deg(v)$ — степень вершины $v$, то есть количество рёбер, соединяющих эту вершину с другими;\n- $\\sum_{v∈V(G)} \\deg(v)$ — это сумма степеней всех вершин;\n- $2⋅|E(G)|$ — удвоенное число рёбер графа.\n\nЭто выражение показывает, что сумма степеней всех вершин графа всегда является чётным числом, так как каждое ребро добавляет ровно по единице к степени двух вершин, которые оно соединяет.\n\nПопробуйте самостоятельно посчитать сумму степеней вершин и удвоенное количество рёбер для любого графа. Вот небольшой пример в Python с библиотекой `networkx`:\n\n```python\nimport networkx as nx\nimport random\n\nrandom.seed(42) # Фиксируем состояние для воспроизводимости\ngraph = nx.erdos_renyi_graph(10, 0.3) # Создаём случайный граф с 10 вершинами и вероятностью ребра 0.3\n\n# Считаем сумму степеней всех вершин\nsum_degrees = sum(dict(graph.degree()).values())\n\n# Считаем удвоенное количество рёбер\ndouble_edges = 2 * graph.number_of_edges()\n\nprint(f\"Сумма степеней: {sum_degrees}\") # должно получиться 34\nprint(f\"Удвоенное количество рёбер: {double_edges}\") # должно получиться 34\n```\n\nВ этом примере мы создаём случайный граф, а затем считаем сумму степеней его вершин и удвоенное количество ребер, которые оказываются равны, что и подтверждает лемму о рукопожатиях.\n\n### Доказательство леммы о рукопожатиях\n\nДля наглядности рассмотрим сначала `пустой` граф — граф без рёбер. В таком графе сумма степеней всех вершин равна нулю. При добавлении нового ребра степень каждой из двух инцидентных ему вершин увеличивается на единицу. Это значит, что общая сумма степеней возрастает на два.\n\nТаким образом, добавление каждого ребра в граф увеличивает сумму степеней на два, что в итоге и приводит к удвоенному количеству рёбер.\n\n### Следствие из леммы о рукопожатиях\n\nОдним из важных выводов из этой леммы является следующее утверждение:\n\n\u003e В любом графе число вершин нечётной степени является чётным.\n\nЭто следствие логически вытекает из формулы: так как сумма степеней всех вершин чётна, то и число вершин, имеющих нечётные степени, также должно быть чётным.\n\nПрежде чем двинуться дальше — вот небольшая таблица о том, в каких сферах может пригодиться лемма о рукопожатиях:\n\n![2.3.4.webp](https://yastatic.net/s3/education-portal/media/2_3_4_d13368c96a.webp)\n\nТеперь, когда мы познакомились с основными свойствами графов и леммой о рукопожатиях, перейдём к изучению `двудольных графов` — особого типа графов с интересной структурой, который применяется в различных областях, от алгоритмов поиска до моделирования систем.\n\n## Двудольные графы\n\n`Двудольный граф` — это граф, в котором все вершины можно разделить на две группы, или доли, так, что рёбра соединяют только вершины из разных долей.\n\nВ двудольном графе не существует рёбер, которые соединяли бы вершины внутри одной и той же доли. Это свойство двудольных графов делает их легко распознаваемыми и структурно простыми.\n\nПредставьте себе двудольный граф в роли большой торговой площадки, где покупатели и товары разошлись по двум лагерям. Каждый покупатель соединён с теми товарами, которые он добавил в корзину, образуя рёбра графа — своего рода «покупательские следы». Подобный граф отлично показывает, кто что купил, и помогает найти популярные товары, к которым тянутся многие покупатели.\n\nИли другой пример: университет. Одна доля графа — это студенты, а другая — курсы, и рёбра связывают студентов с теми курсами, на которые они записались. Такой граф помогает увидеть, какие курсы наиболее востребованы и как распределяются интересы студентов.\n\n![2.3.5.webp](https://yastatic.net/s3/education-portal/media/2_3_5_6618f142a2.webp)\n\nДля наглядного представления двудольного графа часто используется графическая визуализация, где вершины каждой доли располагаются на отдельной прямой линии. Такая схема позволяет сразу увидеть, какие вершины принадлежат каждой доле и как они связаны между собой.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"6\" src=\"https://yastatic.net/s3/education-portal/media/2_3_6_0f13471480.webp\"\u003e\n  \u003cfigcaption\u003e\nДвудольный граф\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nА чтобы и без красивой иллюстрации понять, что перед вами именно двудольный граф, можно воспользоваться теоремой Кёнига. Давайте разберём её подробнее.\n\n### Теорема Кёнига\n\nЭта теорема гласит, что граф является двудольным тогда и только тогда, когда в нём нет циклов вообще или нет циклов нечётной длины. Это свойство удобно использовать при анализе графа, так как проверка на отсутствие нечётных циклов позволяет быстро определить, является ли граф двудольным.\n\nТеорему Кёнига можно объяснить следующим образом:\n\n- Если граф двудольный, то в нём действительно не может быть циклов нечётной длины. Представьте, что мы начинаем обход цикла с вершины одной доли и переходим по рёбрам к вершинам другой доли. Чтобы вернуться в начальную вершину, мы должны пройти чётное количество рёбер, и, следовательно, цикл будет чётным.\n- Если в графе нет циклов нечётной длины, то его можно разделить на две доли. Для доказательства этого можно использовать метод раскраски: раскрасьте вершины графа в два цвета (например, чёрный и белый), так чтобы соседние вершины всегда имели разные цвета. Если в графе нет циклов нечётной длины, такая раскраска будет успешной, и граф можно будет считать двудольным.\n\n{% cut \"Строгое доказательство теоремы Кёнига\" %}\n\n**Утверждение:** Граф $G$ является двудольным тогда и только тогда, когда все циклы в графе $G$ имеют чётную длину.\n\n**Доказательство:**\n\n1. **Необходимое условие**\n   Пусть граф $G$ — двудольный. Разделим его вершины на две доли, обозначенные $U$ и $V$, где рёбра могут соединять вершины только из разных долей. Если мы начнём обход цикла с некоторой вершины в доле $U$, то каждый шаг, представляющий ребро, переносит нас из одной доли в другую. Следовательно, чтобы вернуться в начальную вершину, нам нужно пройти чётное количество рeбер, поскольку каждый переход переносит нас в противоположную долю. Таким образом, любой цикл в двудольном графе имеет чётную длину.\n\n2. **Достаточное условие**\n   Пусть теперь граф $G$ не содержит циклов нечётной длины. Выберем произвольную вершину $u$ и определим её как начальную. Разделим все вершины графа на два множества: $U$, содержащее вершины, которые находятся на чётном расстоянии от $u$, и $V$, содержащее вершины, которые расположены на нечётном расстоянии от $u$ (если расстояние определяется минимальным числом рёбер). Такое разбиение гарантирует, что $U \\cap V = \\emptyset$ и $U \\cup V$ включает все вершины графа.\n\nТеперь, допустим, что существует ребро, соединяющее две вершины внутри одной доли, например вершины $a$ и $b$ в $U$. Пусть $P_a$ — кратчайший путь от $u$ до $a$ и $P_b$ — кратчайший путь от $u$ до $b$. Поскольку обе вершины находятся в $U$, длины путей $P_a$ и $P_b$ будут чётными. Если теперь соединить эти пути ребром $ab$, то получится цикл, длина которого также будет нечётной, что противоречит предположению об отсутствии нечётных циклов. Аналогичное рассуждение верно и для вершин в $V$.\n\n{% endcut %}\n\nТеорема Кёнига важна для понимания структуры двудольных графов, так как она позволяет легко проверять двудольность графа с помощью анализа циклов. Это свойство лежит в основе многих алгоритмических приложений. Например:\n\n- Паросочетания в задачах распределения ресурсов. Представьте компанию, которая хочет распределить сотрудников (одна доля графа) на проекты (другая доля графа) так, чтобы максимально учесть предпочтения сотрудников и требования проектов. Двудольный граф здесь позволяет применять алгоритмы нахождения максимального паросочетания, эффективно решая задачу.\n- Оптимальное разбиение в логистике. Например, при организации распределительных центров, где точки доставки и склады представляют собой две доли графа. Оптимальное разбиение помогает минимизировать транспортные расходы, создавая сбалансированные маршруты между складами и клиентами.\n\nТеперь рассмотрим подробнее несколько видов двудольных графов.\n\n### Полный двудольный граф\n\nВ таком графе каждая вершина одной доли соединена с каждой вершиной другой доли. Он обозначается как $K_{m,n}$, где $m$ и $n$ — это количество вершин в каждой из долей.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"7\" src=\"https://yastatic.net/s3/education-portal/media/2_3_8_75837bed26.webp\"\u003e\n  \u003cfigcaption\u003e\nПолный двудольный граф\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНапример, $K_{2,3}$ — это граф, где одна доля содержит 2 вершины, а другая — 3 вершины. Граф $K_{1,n}$ представляет собой звезду, где одна центральная вершина соединена со всеми вершинами другой доли. В полном двудольном графе $K_{m,n}$ содержится $m \\times n$ рёбер и каждая вершина имеет степень, равную количеству вершин в другой доле.\n\n### Бинарное дерево\n\nЭто особый вид двудольного графа, в котором каждая вершина имеет не более двух потомков и не содержит циклов. В бинарных деревьях существуют понятия корня (вершина, не имеющая предков), листьев (вершины без потомков) и узлов (все остальные вершины). Деревья — это важная структура в информатике, особенно в алгоритмах сортировки, поиска и сжатия данных.\n\nХотя двудольные графы могут быть различными по форме, все деревья, независимо от количества потомков у каждой вершины, тоже считаются двудольными графами.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"8\" src=\"https://yastatic.net/s3/education-portal/media/2_3_7_30e40adea0.webp\"\u003e\n  \u003cfigcaption\u003e\nБинарное дерево\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа иллюстрации выше: вершина `1` — это корень, `8–13` — листья, `2–7` — узлы.\n\nВ этом разделе мы рассмотрели основные типы графов, лемму о рукопожатиях и свойства двудольных графов. Эти концепции формируют базис для анализа графов и поиска оптимальных решений в задачах, связанных с сетями и структурами данных.\n\nЗакрепим эти знания на практике c помощью задач, которые демонстрируют применение теории графов в реальных примерах.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715798.4dc459eb3bff739bb9586a221157f9b20b095adf/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715798.4dc459eb3bff739bb9586a221157f9b20b095adf\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\n- Граф не является обыкновенным, так как содержит петлю в вершине 1 и кратное ребро между вершинами 4 и 5.\n- Граф является мультиграфом, так как содержит кратное ребро.\n- Граф содержит петли.\n- Граф является неориентированным, так как рёбра не имеют направления.\n- Граф является связным, так как все вершины достижимы друг от друга.\n- Граф не является сильно связным и слабо связным, так как эти понятия применимы только к ориентированным графам, а данный граф является неориентированным.\n\n{% endcut %}\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715800.10d898f3bd311f718f86002b0f173d4c5b6f55ba/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715800.10d898f3bd311f718f86002b0f173d4c5b6f55ba\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nВ социальной сети любые пользователи могут быть связаны друг с другом, что нарушает принцип двудольности.\n\n{% endcut %}\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715801.ca4488a3d9733eab5328982327b685b1eb1d5fc8/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715801.ca4488a3d9733eab5328982327b685b1eb1d5fc8\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nПолный двудольный граф может быть использован для отображения всех возможных сочетаний ресторанов и желаемых блюд, что полезно для поиска.\n\n{% endcut %}\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715802.9a76ea1235ce5d7698055f4f9ce3e92756406004/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715802.9a76ea1235ce5d7698055f4f9ce3e92756406004\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nГраф является двудольным, потому что все вершины разделены на две группы (пользователи и рестораны) и рёбра соединяют только вершины из разных групп.\n\nПример программы на Python для визуализации такого графа:\n\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Создаём граф\nG = nx.Graph()\n\n# Добавляем вершины\nG.add_nodes_from(['A1', 'A2', 'A3', 'B1', 'B2', 'B3'])\n\n# Добавляем рёбра\nG.add_edges_from([('A1', 'B1'), ('A1', 'B2'), ('A1', 'B3'), ('A2', 'B2'), ('A2', 'B3'), ('A3', 'B2'), ('A3', 'B3')])\n\n# Определяем цвета для вершин\nnode_colors = ['red' if node.startswith('A') else 'blue' for node in G.nodes]\n\n# Рисуем граф\nnx.draw(G, with_labels=True, node_color=node_colors)\n\n# Отображаем граф\nplt.show()\n```\n\n{% endcut %}"])</script><script nonce="">self.__next_f.push([1,"5e:T562a,"])</script><script nonce="">self.__next_f.push([1,"Представьте себе организационную схему крупной компании. В самом верху будет руководитель, чуть ниже — его заместители, затем — главы департаментов и подчинённые им главы отделов. В самом низу — линейные сотрудники.\n\nВ мире графов такие схемы называются `деревьями`. Они выделяются своей уникальной структурой: деревья не содержат циклов, всегда связны и имеют ровно на одно ребро меньше, чем вершин.\n\nЭти свойства делают деревья чрезвычайно удобными для задач, где важно избежать избыточности и построить оптимальные связи. Именно поэтому деревья заслуживают отдельного внимания как одна из ключевых моделей в теории графов.\n\n## Типы деревьев\n\n`Остовное дерево` — это подграф связного графа, который содержит все его вершины и также представляет собой дерево.\n\nМожно представить остовное дерево как минимальный набор дорог, соединяющих все города страны без образования круговых маршрутов. Это важно в задачах оптимизации, где нужно минимизировать стоимость или длину связей.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"1\" src=\"https://yastatic.net/s3/education-portal/media/2_4_1_6f2c9f57f7.webp\"\u003e\n  \u003cfigcaption\u003e\nОстовное дерево\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЕсли мы возьмём несколько деревьев и поместим их вместе без соединения между ними, мы получим `лес`.\n\nВ терминологии графов это несвязный неориентированный граф без циклов, состоящий из нескольких компонент, каждая из которых является деревом. Как множество отдельных островов с собственными деревьями, не связанных между собой.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"2\" src=\"https://yastatic.net/s3/education-portal/media/2_4_2_bdc4fff3b3.webp\"\u003e\n  \u003cfigcaption\u003e\nЛес\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nС типами разобрались, давайте теперь рассмотрим ключевые теоремы о деревьях.\n\n## Теоремы о деревьях\n\nДеревья обладают рядом уникальных `свойств`, которые делают их удобными для анализа и практических задач:\n\n1. В любом дереве с двумя и более вершинами есть хотя бы одна висячая вершина (лист, вершина степени 1).\n2. В дереве число вершин всегда на единицу больше, чем число рёбер.\n3. У любого связного графа существует остовное дерево\n\n\u003e Это важно: свойство №1 лежит в основе многих алгоритмов обработки деревьев. Свойство №2 помогает интуитивно понять структуру деревьев. А свойство №3 помогает оптимизировать графы.\n\nИ вот как можно формализовать и доказать эти свойства.\n\n### Теорема №1: висячая вершина в дереве\n\nУ нас есть утверждение: в дереве с более чем одной вершиной всегда существует хотя бы одна висячая вершина (лист).\n\nПопробуем доказать его. Представьте, что вы гуляете по лабиринту без циклов (нашему дереву). Начав с любой вершины, вы можете идти по рёбрам, не возвращаясь назад. Поскольку дерево конечно, вы в итоге достигнете точки, из которой нет выхода, — это и будет висячая вершина.\n\n### Теорема №2: соотношение между вершинами и рёбрами\n\nУтверждение следующее: в дереве число вершин на единицу больше, чем число рёбер, то есть $n = q + 1$, где $n$ — число вершин, $q$ — число рёбер.\n\n**Доказательство по индукции:**\n\n- **База индукции:** Для дерева с одной вершиной $(n=1)$ нет рёбер $(q=0)$ и соотношение $n=q+1$ выполняется.\n- **Шаг индукции:** Предположим, что теорема верна для всех деревьев с $n−1$ вершиной. Возьмём дерево с $n$ вершинами. По теореме 1, у него есть висячая вершина. Удалим эту вершину и её единственное ребро. Оставшееся — дерево с $n−1$ вершиной и $q−1$ рёбрами, для которого, по предположению, верно $n−1=(q−1)+1$. Возвращаясь к исходному дереву, получаем $n=q+1$.\n\nЭто как строить цепочку из звеньев: каждое новое звено (вершина) добавляет одну связь (ребро), сохраняя целостность цепочки без замкнутых колец.\n\n### Теорема №3: остовное дерево связного графа\n\nМы утверждаем, у любого связного графа существует остовное дерево.\n\nРассмотрим связный граф, который можно представить как структуру со множеством узлов и циклов. Наша задача — удалить из графа избыточные рёбра, сохраняя связность и избегая образования циклов.\n\n1. Начинаем с исходного связного графа.\n2. Пока присутствуют циклы, удаляем по одному ребру из каждого из них. Это сохраняет связность, но устраняет замкнутость.\n3. В результате получаем граф без циклов, который остаётся связным, — остовное дерево.\n\n***\n\nМы рассмотрели основные свойства и теоремы, которые формализуют фундаментальные характеристики деревьев, подчёркивая их простоту и эффективность. Однако на практике связи между объектами редко бывают равнозначными: они часто сопровождаются дополнительной информацией — некоторыми `весами`.\n\nИменно здесь на первый план выходят взвешенные деревья, позволяющие учитывать эти особенности и решать задачи оптимизации, где важна не только структура, но и её параметры.\n\n## Взвешенные деревья\n\nВ реальном мире связи между объектами часто имеют вес: расстояние между городами, стоимость перелётов, время передачи данных. `Взвешенное дерево` учитывает эти веса, что позволяет решать задачи оптимизации. Вес ребра может представлять различные величины, например:\n\n- Расстояние. В транспортной сети вес ребра может представлять расстояние между двумя городами.\n- Время. В сети передачи данных вес ребра может представлять время, необходимое для передачи данных между двумя узлами.\n- Стоимость. В сети поставок вес ребра может представлять стоимость перевозки груза между двумя складами\n\n\u003cfigure\u003e\n  \u003cimg alt=\"3\" src=\"https://yastatic.net/s3/education-portal/media/2_4_3_564ce9ef91.webp\"\u003e\n  \u003cfigcaption\u003e\nВзвешенное дерево\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nДля поиска оптимальных путей во взвешенных деревьях есть специальные алгоритмы. Рассмотрим их подробнее.\n\n### Алгоритм Дейкстры\n\n**Суть:**\n\nЭто оптимальный выбор для нахождения кратчайших путей от начальной вершины к остальным, если все веса рёбер неотрицательные. Его асимптотическая сложность делает его подходящим для больших графов при условии, что отсутствуют отрицательные веса.\n\n**Принцип работы:**\n\nПредставьте, что вы путешественник в незнакомом городе и хотите посетить все достопримечательности с минимальными затратами времени.\n\n1. Инициализация:\n   - Устанавливаем начальную вершину с нулевым расстоянием.\n   - Остальные вершины получают бесконечное расстояние.\n   - Все вершины считаются необработанными.\n2. Шаги алгоритма:\n   - Выбираем необработанную вершину с наименьшим текущим расстоянием до неё.\n   - Для каждого соседа текущей вершины:\n     - Вес ребра между текущей вершиной и соседом — это заданная величина, характеризующая связь между ними.\n     - Вычисляем новое потенциальное расстояние до соседа: сумма текущего расстояния до вершины и веса ребра между ними.\n     - Если это расстояние меньше ранее известного расстояния до соседа, обновляем его.\n   - Помечаем текущую вершину как обработанную.\n3. Завершение:\n   - Процесс повторяется, пока все вершины не будут обработаны.\n\n**Преимущества и ограничения:**\n\n- Плюсы: эффективен и гарантирует оптимальные решения для графов с неотрицательными весами.\n- Минусы: не работает с отрицательными весами рёбер.\n\n{% cut \"Реализация на Python (не открывайте сразу, сначала подумайте сами!)\" %}\n\nВ данной реализации граф представлен в виде словаря смежности. Это словарь, где ключами являются вершины, а значениями — списки пар `(соседняя_вершина, вес_ребра)`. Такой способ удобен в Python, особенно когда вершины имеют строковые имена. Он эквивалентен списку смежности, но предоставляет более удобный доступ к элементам.\n\n```python\nimport heapq\n\ndef dijkstra(graph, start):\n    distance = {vertex: float('inf') for vertex in graph}\n    distance[start] = 0\n    priority_queue = [(0, start)]  # (расстояние, вершина)\n\n    while priority_queue:\n        current_distance, current_vertex = heapq.heappop(priority_queue)\n\n        # Если найден более короткий путь, пропускаем\n        if current_distance \u003e distance[current_vertex]:\n            continue\n\n        for neighbor, weight in graph[current_vertex]:\n            distance_to_neighbor = current_distance + weight\n\n            if distance_to_neighbor \u003c distance[neighbor]:\n                distance[neighbor] = distance_to_neighbor\n                heapq.heappush(priority_queue, (distance_to_neighbor, neighbor))\n\n    return distance\n\n# Пример использования\ngraph = {\n    'A': [('B', 1), ('C', 4)],\n    'B': [('C', 2), ('D', 5)],\n    'C': [('D', 1)],\n    'D': []\n}\n\ndistances = dijkstra(graph, 'A')\nprint(distances)  # {'A': 0, 'B': 1, 'C': 3, 'D': 4}\n```\n\n{% endcut %}\n\n### Алгоритм Беллмана — Форда\n\n**Суть:**\n\nПозволяет находить кратчайшие пути от заданной вершины до всех остальных, даже если в графе присутствуют отрицательные веса рёбер. Однако алгоритм лучше применять для средних по размеру графов, поскольку его временная сложность составляет $O(VE)$, где:\n\n- $V$ — количество вершин в графе,\n- $E$ — количество рёбер в графе.\n\n**Принцип работы:**\n\nАлгоритм последовательно релаксирует (обновляет) расстояния, проходя по всем рёбрам многократно.\n\n1. Инициализация:\n   - Как в алгоритме Дейкстры.\n2. Шаги алгоритма:\n   - Повторяем процесс релаксации ребер  $V−1$ раз, где $V$ — количество вершин.\n   - Проверяем на наличие отрицательных циклов.\n\n**Преимущества и ограничения:**\n\n- Плюсы: работает с отрицательными весами.\n- Минусы: более медленный, сложность $O(VE)$.\n\n{% cut \"Реализация на Python (не открывайте сразу, сначала подумайте сами!)\" %}\n\n```python\ndef bellman_ford(graph, source):\n    distance = {vertex: float('inf') for vertex in graph}\n    distance[source] = 0\n\n    for _ in range(len(graph) - 1):\n        for vertex in graph:\n            for neighbor, weight in graph[vertex]:\n                if distance[vertex] + weight \u003c distance[neighbor]:\n                    distance[neighbor] = distance[vertex] + weight\n\n    # Проверка на отрицательные циклы\n    for vertex in graph:\n        for neighbor, weight in graph[vertex]:\n            if distance[vertex] + weight \u003c distance[neighbor]:\n                raise ValueError(\"Граф содержит отрицательный цикл\")\n\n    return distance\n\n# Пример использования\ngraph = {\n    'A': [('B', 4), ('C', 2)],\n    'B': [('C', -1), ('D', 2)],\n    'C': [('D', 3)],\n    'D': []\n}\n\ndistances = bellman_ford(graph, 'A')\nprint(distances)  # {'A': 0, 'B': 4, 'C': 2, 'D': 5}\n```\n\n{% endcut %}\n\n### Алгоритм Флойда — Уоршелла\n\n**Суть:**\n\nПодходит для поиска кратчайших путей между всеми парами вершин. Он работает на основе матрицы расстояний, которая представляет собой квадратную таблицу, где элемент на пересечении строки $i$ и столбца $j$ показывает текущее минимальное расстояние от вершины\n$i$ до вершины $j$. Постепенно обновляя значения в этой таблице, алгоритм находит минимальные пути между всеми парами вершин.\n\nПри этом алгоритм способен обрабатывать графы с отрицательными весами рёбер, хотя его сложность $O(V^3)$ делает его менее эффективным для очень больших сетей.\n\n**Принцип работы:**\n\nПредставьте, что вы хотите узнать минимальные расстояния между всеми парами городов в стране.\n\n1. Инициализация:\n   - Создаём матрицу расстояний между всеми вершинами.\n2. Шаги алгоритма:\n   - Постепенно улучшаем оценки расстояний, рассматривая все возможные промежуточные вершины.\n\n**Преимущества и ограничения:**\n\n- Плюсы: работает с отрицательными весами рёбер и прост в реализации.\n- Минусы: сложность $O(V^3)$ делает алгоритм медленным для больших графов.\n\n{% cut \"Реализация на Python (не открывайте сразу, сначала подумайте сами!)\" %}\n\n```python\nimport numpy as np\n\ndef floyd_warshall(graph):\n    num_vertices = len(graph)\n    dist = np.array(graph)\n\n    for k in range(num_vertices):\n        for i in range(num_vertices):\n            for j in range(num_vertices):\n                if dist[i][k] + dist[k][j] \u003c dist[i][j]:\n                    dist[i][j] = dist[i][k] + dist[k][j]\n\n    return dist\n\n# Пример использования\ngraph = [\n    [0, 3, float('inf'), 5],\n    [2, 0, float('inf'), 4],\n    [float('inf'), 1, 0, float('inf')],\n    [float('inf'), float('inf'), 2, 0]\n]\n\nshortest_paths = floyd_warshall(graph)\nprint(shortest_paths)\n```\n\n{% endcut %}\n\n***\n\nТеория графов и, в частности, деревья — это мощный инструмент для моделирования и решения разнообразных задач. Понимая, как элементы связаны между собой, мы можем принимать более обоснованные решения и находить эффективные пути решения проблем.\n\nЕсли вы хотите углубить свои знания и применить их на практике, с алгоритмическими заданиями по графам вы можете ознакомиться в [хендбуке](https://education.yandex.ru/handbook/algorithms/article/priroda-grafa) по алгоритмическому программированию.\n\nВ нём рассматриваются такие важные темы, как поиск в ширину (BFS), поиск в глубину (DFS), обходы графа, алгоритм нахождения компонент связности в графе и многое другое. Это поможет вам освоить ключевые алгоритмы и применить теоретические знания на практике, делая ваши решения ещё более эффективными и продуманными.\n\nА сейчас перейдём к квизам и задачам на закрепление материала.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715831.cdfad2277c42901963fd50dc0410d41039108704/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715831.cdfad2277c42901963fd50dc0410d41039108704\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nГраф содержит цикл: A-B-D-H-C-A. Дерево по определению не может иметь циклов.\n\n{% endcut %}\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715832.fd5a1777a2c5d07722d877256b89d6133decaf94/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715832.fd5a1777a2c5d07722d877256b89d6133decaf94\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nГраф является связным, и число рёбер равно числу вершин минус $1$ ($7 - 1 = 6$), что соответствует определению дерева.\n\n{% endcut %}\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13715835.70ad849b10171225ef64320683efef43c07dfd0e/?iframe=1\" frameborder=\"0\" name=\"ya-form-13715835.70ad849b10171225ef64320683efef43c07dfd0e\" width=\"650\"\u003e\u003c/iframe\u003e\n\n{% cut \"Объяснение\" %}\n\nКорнем дерева является такая вершина, из которой можно добраться до всех остальных вершин, но в неё не входит ни одного ребра.\n\n{% endcut %}"])</script><script nonce="">self.__next_f.push([1,"5f:Tcbe,"])</script><script nonce="">self.__next_f.push([1,"Давайте коротко вспомним. \n\n- Вы знаете, что такое граф и его базовые элементы — вершины и рёбра. Уже умеете различать типы графов: ориентированные и неориентированные, мультиграфы, графы с петлями и так далее.\n- Можете вычислить общее количество связей в графе благодаря лемме о рукопожатиях.\n- Понимаете, как структура графа определяет его свойства и область применения: в каких ситуациях стоит применять двудольный граф, а в каких — деревья.\n- Знаете о свойствах деревьев и теоремах, которые их подтверждают.\n\nКак видите, в этой главе мы глубоко погрузились в мир теории графов, изучив её основные понятия, свойства и практическое применение. Теперь у вас есть фундамент, на котором можно строить дальнейшее изучение сложных связанных структур и алгоритмов.\n\nСамое важное — то, как мы показали применение графов в реальной жизни. Оптимизация маршрутов, системы подписок, распределение ресурсов — лишь малая часть задач, где графы проявляют свою мощь. Мы стремились сделать каждую тему не только теоретически значимой, но и практически полезной, чтобы вы могли видеть, как эти концепции работают на практике.\n\nБлагодаря графам мы научились видеть связи. Но что, если нам нужно понять изменения? Тут поможет математический анализ — наука о том, как всё движется и достигает совершенства. В следующей главе мы разберём, как оптимизировать системы и анализировать их устойчивость, вычисляя градиенты и исследуя критические точки. \n\nВы увидите, как математика объясняет не только данные, но и саму природу прогресса, помогая упрощать сложные процессы и находить оптимальные решения.\n\nИ напоследок: если вам любопытно решить реальную (а не учебную) задачу, связанную с графами, – переходите по [ссылке](https://colab.research.google.com/drive/1ydlhO50QaBcqNOmsPA3vvibyAfC9iBRc?usp=sharing). Она «со звёздочкой», так что желаем удачи!"])</script><script nonce="">self.__next_f.push([1,"60:Te5e,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы углубимся в основы математического анализа. Это фундамент анализа данных, оптимизации и машинного обучения. \n\nОбсудим:\n\n- **Функции и их типы.** Мы начнём с изучения функций как базового инструмента для моделирования зависимостей между переменными. Рассмотрим основные типы функций, их свойства и примеры.\n- **Пределы и непрерывность.** Вы узнаете, как анализировать поведение функций на малых интервалах, что важно для понимания устойчивости моделей и корректной работы алгоритмов оптимизации.\n- **Дифференцирование.** Мы разберём, как вычислять производные функций, какие у них есть свойства и как они помогают находить ключевые точки (экстремумы). Покажем, как дифференцирование используется в методах оптимизации.\n- **Применение в задачах оптимизации.** Вы научитесь находить минимумы и максимумы функций, используя производные и анализ их поведения. Этот раздел покажет, как математика связывается с реальными прикладными задачами.\n\nПока хендбук в разработке, мы сосредоточимся на первой части главы – анализе функций, пределов и непрерывности. Заложим фундамент для понимания более сложных концепций, таких как градиентные методы оптимизации, сходимость алгоритмов и выпуклая оптимизация. \n\nВ дальнейшем мы дополним главу материалами по интегрированию, анализу многомерных функций, исследованию их свойств, а также методам второго порядка и их приложениям в машинном обучении.\n\nПрочитав эту главу, вы сможете:\n\n- Оценивать ключевые свойства функций и их зависимость от входных данных.\n- Находить пределы и определять непрерывность функций.\n- Использовать производные для анализа поведения функций и решения задач оптимизации.\n\nИ последнее: мы предполагаем, что вы уже знакомы с понятиями этой главы. Наша цель — кратко напомнить их и показать практическое применение в контексте машинного обучения. А если вы хорошо владеете этими темами, наши объяснения помогут структурировать знания и связать их с решением задач в реальных сценариях анализа данных и разработки моделей.\n\nДавайте начнём!"])</script><script nonce="">self.__next_f.push([1,"61:Tc0d7,"])</script><script nonce="">self.__next_f.push([1,"Как с помощью моделей мы можем прогнозировать результаты и почему поведение этих моделей может резко меняться при малых изменениях в данных?\n\nОтветы на эти вопросы кроются в понятиях предела и непрерывности. В этом параграфе мы разберём, как эти математические идеи связаны с устойчивостью алгоритмов оптимизации, их сходимостью и общей надёжностью решений в задачах анализа данных.\n\nНо начнём мы наше погружение с `функций`. Это основной инструмент математики, и он широко используется в анализе данных и машинном обучении для моделирования отношений между переменными.\n\n## Функции\n\n`Функция` — это правило, сопоставляющее каждому элементу области определения ровно одно значение из области значений.\n\nПроще говоря, функция принимает на вход некоторое значение и выдаёт соответствующее ему выходное значение. В математике функцию обычно обозначают как $f(x)$, где $x$ — входная переменная, а $f(x)$ — соответствующее ей выходное значение.\n\nПример — функция $f(x) = 2x$.\n\nОна берёт число $x$ (входное значение) и возвращает результат его удвоения (выходное значение).\n\n- Если на вход подать $x = 3$, то функция вернёт $f(3) = 2 \\cdot 3 = 6$.\n- Если на вход подать $x = 5$, то функция вернёт $f(5) = 2 \\cdot 5 = 10$.\n\nЭто правило связывает каждое входное значение с ровно одним выходным значением, что соответствует определению функции.\n\n\u003e Множество входных значений ещё называют `областью определения`, а множество выходных значений — `областью значений`. Дальше мы будем использовать именно эти термины, не теряйтесь!\n\nВот некоторые типы функций:\n\n#|\n||\n\n**Тип функции**\n\n|\n\n**Формула**\n\n|\n\n**Пояснения**\n\n|\n\n**Для чего нужна**\n\n|\n\n**Пример**\n\n||\n||\n\n**Линейная функция**\n\n|\n\n$f(x) = mx + b$\n\n|\n\n$m$ — коэффициент наклона, $b$ — сдвиг\n\n|\n\nМоделирование прямой пропорциональности\n\n|\n\nЗависимость цены товара от количества. Если $m \u003e 0$, цена растёт, если $m \u003c 0$, падает\n\n||\n||\n\n**Квадратичная функция**\n\n|\n\n$f(x) = ax^2 + bx + c$\n\n|\n\n$a$, $b$, $c$ — константы, $a \\neq 0$\n\n|\n\nМоделирование параболических зависимостей\n\n|\n\nЗатраты на производство товара в зависимости от объёма выпуска\n\n||\n||\n\n**Экспоненциальная функция**\n\n|\n\n$f(x) = a e^{kx}$\n\n|\n\n$a$ — масштаб, $k$ — скорость роста или распада, $e$ — основание натурального логарифма, $k \\neq 0$\n\n|\n\nОписание процессов роста или распада\n\n|\n\nРост популяции бактерий или распад радиоактивных веществ\n\n||\n||\n\n**Логарифмическая функция**\n\n|\n\n$f(x) = a \\ln(bx)$\n\n|\n\n$a$, $b$ — константы, $b \u003e 0$\n\n|\n\nАнализ процессов с затухающей динамикой\n\n|\n\nУровень насыщения в маркетинговых кампаниях: добавление рекламы увеличивает продажи, но с каждым разом эффект от рекламы становится меньше\n\n||\n||\n\n**Гиперболическая функция**\n\n|\n\n$f(x) = \\frac{a}{x}$\n\n|\n\n$a$ — масштаб, $x \\neq 0$\n\n|\n\nМоделирование процессов, где результат убывает обратно пропорционально входным данным\n\n|\n\nРаспределение давления в жидкости по мере удаления от источника\n\n||\n|#\n\nФункции особенно важны в анализе данных и машинном обучении, поскольку они:\n\n- Позволяют моделировать зависимость между входными признаками и целевой переменной.\n- Помогают оценить качество модели (через функцию потерь).\n- Лежат в основе алгоритмов оптимизации, которые настраивают параметры моделей, чтобы добиться их наилучшего функционирования.\n\nНо, чтобы эффективно работать с функциями в контексте оптимизации и анализа данных, необходимо понимать, как они ведут себя при изменении входных значений. Это приводит нас к изучению понятий предела и непрерывности —\u0026nbsp;давайте рассмотрим их.\n\n## Понятие предела последовательности и функции\n\nВ анализе данных и машинном обучении многие алгоритмы работают итеративно, шаг за шагом приближаясь к оптимальному решению. Этот процесс невозможно представить без применения основ математического анализа, где понятие предела играет фундаментальную роль.\n\nХотя пределы редко применяются напрямую, они служат основой для других важных концепций — непрерывности, производных и интегралов, — которые позволяют описывать поведение алгоритмов, анализировать их сходимость и разрабатывать надёжные методы оптимизации.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Итеративное обновление параметра $\\theta$ методом градиентного спуска\" src=\"https://yastatic.net/s3/education-portal/media/3_2_1_612b57c09f.webp\"\u003e\n  \u003cfigcaption\u003e\n\nИтеративное обновление параметра $\\theta$ методом градиентного спуска\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНапример, для минимизации функции потерь мы применяем градиентный спуск. Это один из самых распространённых методов оптимизации. Он обновляет параметры модели на каждом шаге, изменяя их значения по направлению наискорейшего убывания функции потерь. Таким образом, параметры модели образуют последовательность значений, которая с каждой итерацией становится всё ближе к оптимальному решению.\n\nАнализ пределов таких последовательностей позволяет оценить, сходятся ли они к желаемому результату, а понятие непрерывности помогает понять, как малые изменения входных данных влияют на выход модели.\n\n\u003e Это основа для разработки надёжных, устойчивых и эффективных алгоритмов.\n\nТеперь давайте детальнее посмотрим на определения предела и непрерывности.\n\n### Предел последовательности\n\n`Предел последовательности` — это значение, к которому приближаются элементы последовательности по мере роста их номеров.\n\nФормально последовательность чисел $\\{a_n\\}$ имеет конечный предел $A$ (записывают $\\lim_{n \\to \\infty} a_n = A$ или $a_n \\to A$ при $n \\to \\infty$), если для любого сколь угодно малого положительного числа $\\varepsilon \u003e 0$ существует такое натуральное число $N$, что для всех $n \u003e N$ выполняется неравенство:\n\n$$|a_n - A| \u003c \\varepsilon.\n$$\n\nЭто означает, что начиная с некоторого номера $N$ все члены последовательности находятся в $\\varepsilon$-окрестности числа $A$.\n\n**Пример:** Рассмотрим последовательность $a_n = \\dfrac{1}{n}$. Покажем, что $\\lim_{n \\to \\infty} \\dfrac{1}{n} = 0$.\n\nДля любого $\\varepsilon \u003e 0$ выберем $N = \\left[ \\dfrac{1}{\\varepsilon} \\right] + 1$. Тогда для всех $n \u003e N$:\n\n$$\\left| \\dfrac{1}{n} - 0 \\right| = \\dfrac{1}{n} \u003c \\dfrac{1}{N} \u003c \\varepsilon.\n$$\n\nТаким образом, данная последовательность сходится к нулю.\n\nВ некоторых задачах оптимизации нам потребуется понимание, как функция ведёт себя в окрестности определённой точки, поэтому рассмотрим определение `предела функции в точке`.\n\n### Предел функции в точке\n\nПредел функции описывает поведение её значений при приближении аргумента к определённой точке.\n\nЕсли значения функции становятся всё ближе к конкретному числу $L$ по мере приближения аргумента $x$ к $x_0$, говорят, что функция имеет предел $L$ в точке $x_0$.\n\nС точки зрения строгого определения, функция $f(x)$ имеет предел $L$ в точке $x_0$ (записывают $\\lim_{x \\to x_0} f(x) = L$), если для любого $\\varepsilon \u003e 0$ существует такое $\\delta \u003e 0$, что для всех $x$, удовлетворяющих условию $0 \u003c |x - x_0| \u003c \\delta$, выполняется неравенство:\n\n$$|f(x) - L| \u003c \\varepsilon.\n$$\n\nЭто означает, что значения функции $f(x)$ сколь угодно близки к $L$, если $x$ достаточно близок к $x_0$, но не равен $x_0$.\n\n**Пример:** Найдем предел $\\lim_{x \\to 2} (3x - 1)$.\n\nПусть $L = 3 \\cdot 2 - 1 = 5$. Для любого $\\varepsilon \u003e 0$ выберем $\\delta = \\dfrac{\\varepsilon}{3}$. Тогда при $0 \u003c |x - 2| \u003c \\delta$ имеем:\n\n$$|f(x) - L| = |3x - 1 - 5| = |3x - 6| = 3|x - 2| \u003c 3\\delta = \\varepsilon.\n$$\n\nТаким образом, предел существует и равен 5.\n\nОдин из примеров применения предела функции в задачах анализа данных — это прогнозирование значений временного ряда на основе предыдущих данных. Предположим, у нас есть модель, предсказывающая среднедневную температуру $T(t)$ в зависимости от времени $t$. Если при небольших изменениях времени $t$ предсказанное значение $T(t)$ меняется совсем незначительно, то при $t \\to t_0$ предсказываемая температура $T(t)$ приблизится к значению $T(t_0)$, наблюдаемому в этот момент.\n\nИными словами, при достаточно малых изменениях $t$ значение $T(t)$ остаётся близким к $T(t_0)$, что записывается следующим образом:\n\n$$|T(t) - T(t_0)| \u003c \\varepsilon,\n$$\n\nгде $\\varepsilon \u003e 0$ может быть сколь угодно малым.\n\nНо некоторые функции бывают `негладкими`. Это значит, что у них есть точки разрыва или участки, где невозможно вычислить производную (о производной мы поговорим подробно в следующем параграфе).\n\nДля анализа негладких функций применяются односторонние пределы. Рассмотрим их подробнее.\n\n### Односторонние пределы\n\nКогда функция приближается к точке с разных сторон, её поведение может быть различным. Для описания таких ситуаций существуют понятия `левостороннего` и `правостороннего пределов`:\n\n- **Левосторонний предел** $\\lim_{x \\to x_0 - 0} f(x)$ — предел функции при $x$, стремящемся к $x_0$ слева ($x \u003c x_0$).\n- **Правосторонний предел** $\\lim_{x \\to x_0 + 0} f(x)$ — предел функции при $x$, стремящемся к $x_0$ справа ($x \u003e x_0$).\n\nЧтобы функция имела предел в точке $x_0$, левосторонний и правосторонний пределы в этой точке должны существовать и быть равны между собой. В противном случае говорят, что функция имеет `разрыв`.\n\n**Пример 1:** Возьмем функцию активации в нейронных сетях `ReLU`: $f(x) = \\max(0, x)$. Сама эта функция непрерывна в точке $x = 0$, так как слева и справа пределы совпадают. Однако, если рассматривать её производную как новую функцию, то для неё в точке $x = 0$ левосторонний и правосторонний пределы различны ($0$ и $1$ соответственно). Значит, у производной в этой точке нет предела, хотя исходная функция остаётся непрерывной.\n\n![3.2.2.webp](https://yastatic.net/s3/education-portal/media/3_2_2_b9f60ae401.webp)\n\n**Пример 2:** Рассмотрим функцию ошибки `Huber Loss`, которая используется для задач регрессии. Она сочетает в себе свойства квадратичной и линейной функций, что делает её устойчивой к выбросам. Вот определение `Huber Loss`:\n\n$$L_\\gamma(a) = \\begin{cases}\n\\dfrac{1}{2} a^2, \u0026 \\text{если } |a| \\leq \\gamma \\\\\n\\gamma \\left( |a| - \\dfrac{1}{2} \\gamma \\right), \u0026 \\text{если } |a| \u003e \\gamma\n\\end{cases}\n$$\n\nГде:\n\n- $a = y - \\hat{y}$ — разница между истинным значением $y$ и предсказанием модели $\\hat{y}$.\n- $\\gamma\u003e 0$ — параметр, определяющий область перехода между квадратичной и линейной частями функции (порог).\n\n![3.2.3.webp](https://yastatic.net/s3/education-portal/media/3_2_3_99d2acf3f0.webp)\n\nДавайте проанализируем функцию:\n\n- При $|a| \\leq \\gamma$ функция ведет себя как квадратичная функция потерь $(\\dfrac{1}{2} a^2)$, что обеспечивает чувствительность к небольшим ошибкам и позволяет эффективно минимизировать малые отклонения.\n- При $|a| \u003e \\gamma$ функция становится линейной $(\\gamma (|a| - \\dfrac{1}{2} \\gamma))$, снижая влияние крупных ошибок (выбросов) и делая модель более устойчивой.\n\nВ точках $a = \\gamma$ и $a = -\\gamma$ функция `Huber Loss` остаётся непрерывной и имеет непрерывную первую производную (это показатель скорости изменений). Однако вторая производная испытывает скачок в этих точках, что приводит к излому на графике функции.\n\n- **Левосторонняя производная** при $a \\to \\gamma - 0$ равна $\\gamma$.\n- **Правосторонняя производная** при $a \\to \\gamma + 0$ равна $\\gamma$.\n\nХотя значения первой производной совпадают, вторая производная меняется скачкообразно (с 1 до 0) в точках $a = \\pm\\gamma$. Это означает, что функция не является `гладкой второго порядка` в этих местах, несмотря на непрерывность и дифференцируемость её первой производной.\n\nБлагодаря своим свойствам функция ошибки `Huber Loss` позволяет модели быть более устойчивой к выбросам, сочетая преимущества MSE (Mean Squared Error) и MAE (Mean Absolute Error).\n\nРассмотрев понятие предела, мы можем перейти к следующему фундаментальному свойству функций — их `непрерывности`.\n\n## Непрерывность и свойства непрерывных функций\n\n`Непрерывность` обеспечивает стабильность и предсказуемость поведения моделей, позволяя понять, как малейшие изменения входных данных отражаются на результате. Это особенно важно для создания надёжных и эффективных алгоритмов.\n\nПеред тем как углубиться в понятие непрерывности, важно определить ключевые термины, связанные с её практическим применением:\n\n- **Стабильность модели** — это способность алгоритма давать предсказуемые результаты при небольших изменениях входных данных. Малые возмущения на входе не должны приводить к существенным отклонениям на выходе, что особенно важно в условиях работы с шумными или неидеальными данными.\n- **Устойчивость модели** — это её способность сохранять точность и производительность даже при наличии шума, выбросов или изменений в данных. Устойчивая модель демонстрирует хорошую обобщающую способность на новых данных и не склонна к переобучению.\n- **Предсказуемость модели** — это её способность выдавать результаты, которые интуитивно или математически согласуются с изменениями входных данных. Например, если модель прогнозирует температуру на основе времени суток, мы ожидаем, что небольшое изменение времени не приведёт к резким скачкам температуры, как если бы предсказания показывали \\+30°C на рассвете и –10°C через минуту. Предсказуемость повышает доверие к модели.\n\nЭти понятия служат основой для понимания важности непрерывности функций в математическом анализе и их роли в машинном обучении. Вернемся теперь к понятию непрерывности.\n\nИнтуитивно функция называется непрерывной в точке, если небольшие изменения аргумента приводят к небольшим изменениям значения функции. Другими словами, функция ведёт себя предсказуемо, без резких скачков, в каждой точке своей области определения.\n\nБолее формально функция $f(x)$ называется непрерывной в точке $x_0$, если выполняются два условия:\n\n1. Существует значение $f(x_0)$.\n2. Существует предел $\\lim_{x \\to x_0} f(x)$,  и он равен значению функции в этой точке: $\\lim_{x \\to x_0} f(x) = f(x_0)$.\n\nЕсли хотя бы одно из этих условий нарушается, функция называется `разрывной` в точке $x_0$.\n\nРассмотрим пример функции потерь, которая естественным образом возникает в задачах бинарной классификации. Пусть $a = y - \\hat{y}$, где $y$ — истинное значение целевой переменной, а $\\hat{y}$ — предсказание модели. Определим функцию потерь $L(a)$ следующим образом:\n\n$$L(a)= \\begin{cases} \n1, \u0026 \\text{если } a \\neq 0, \\\\\n0, \u0026 \\text{если } a = 0.\n\\end{cases}\n$$\n\nВ интерпретации задач классификации, если предсказание совпало с истиной ($a = 0$), потери равны нулю, иначе — единице. Несмотря на свою интуитивную понятность, такая функция потерь не подходит для оптимизации, так как её производная в точках разрыва не определена.\n\nПолучается проблема: градиентный спуск и другие оптимизационные методы не смогут корректно обновлять параметры модели. Чтобы обойти её, разрывные функции заменяют непрерывными аппроксимациями, которые сохраняют общий смысл задачи, но позволяют использовать математические методы.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Графики различных функций потерь для классификации\" src=\"https://yastatic.net/s3/education-portal/media/3_2_4_68ea84917f.webp\"\u003e\n  \u003cfigcaption\u003e\nГрафики различных функций потерь для классификации\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nРассмотрим одну из функций потерь, на которую заменяют ступенчатую функцию в задачах бинарной классификации, — логистическую функцию:\n\n$$L(a) = \\ln(1 + e^{-a})\n$$\n\n{% cut \"Почему запись именно такая\" %}\n\nЭтот вид записи эквивалентен более привычной форме логистической функции, представляемой через сигмоиду $\\sigma(a) = \\dfrac{1}{1 + e^{-a}}$, где $L(a)$ — логарифм от знаменателя $\\sigma(a)$. Такая запись часто используется в задачах оптимизации, так как она упрощает вычисления градиентов.\n\n{% endcut %}\n\nЭта функция — гладкая, что позволяет эффективно использовать градиентный спуск. Использование таких функций для решения задач оптимизации делает задачу не только решаемой, но и вычислительно более эффективной.\n\nПознакомимся с основными `свойствами непрерывных функций`:\n\n1. **Арифметические операции.** Если функции $f(x)$и $g(x)$ непрерывны в точке $x_0$, то их сумма, разность, произведение и частное (при $g(x_0) \\neq 0$) также будут непрерывны в этой точке. Это свойство делает возможным построение сложных моделей из простых компонентов.\n\n**Пример:** В нейронных сетях непрерывность сохраняется на всех уровнях благодаря тому, что линейные преобразования и функции активации (например, уже известная нам `ReLU`) являются непрерывными.\n\n2. **Составные функции.** Если $f(u)$ непрерывна в точке $u_0$, а $u(x)$ непрерывна в $x_0$ и $u_0 = u(x_0)$, то составная функция $f(u(x))$ также непрерывна в $x_0$.\n\n3. **Теоремы о непрерывности:**\n\n- **Больцано — Коши.** Если функция непрерывна на отрезке $[a, b]$ и значения на концах отрезка имеют противоположные знаки ($f(a) \\cdot f(b) \u003c 0$), то существует хотя бы одна точка $c \\in (a, b)$, где $f(c) = 0$. Эта теорема лежит в основе методов численного поиска корней уравнений.\n- **Вейерштрасс.** Непрерывная функция на отрезке $[a, b]$ достигает своего максимума и минимума. Это свойство важно, например, для определения границ значений в задачах оптимизации.\n\nКогда мы говорим о непрерывных функциях, важно помнить, что непрерывность сама по себе не накладывает ограничений на скорость изменения функции. Функция может быть непрерывной, но при этом резко меняться, что может вызвать сложности в задачах оптимизации и анализа. Именно здесь на помощь приходит понятие `Липшицевой непрерывности`, которое вводит более строгий контроль за тем, насколько сильно значения функции могут изменяться в ответ на изменения входных данных.\n\n### Липшицева непрерывность\n\nФункция $f(x)$ называется `непрерывной по Липшицу` на множестве $D$, если существует константа $L \u003e 0$, такая, что для любых $x_1, x_2 \\in D$ выполняется:\n\n$$|f(x_1) - f(x_2)| \\leq L |x_1 - x_2|.\n\n$$\n\nЛипшицева непрерывность — более строгая форма непрерывности, которая ограничивает скорость изменения функции. Это свойство важно для анализа сходимости и устойчивости алгоритмов.\n\n\u003e **Почему это важно для нас**: Липшицева непрерывность градиента функции потерь позволяет ограничивать шаг обучения в градиентном спуске, обеспечивая стабильность и быструю сходимость алгоритма, то есть уменьшение числа итераций, необходимых для достижения заданного уровня точности.\n\nВ алгоритмах оптимизации градиентного спуска с адаптивным шагом мы будем использовать именно предположение о непрерывности по Липшицу — чтобы обеспечить корректное поведение.\n\nТеперь рассмотрим пример, где градиент функции не является Липшицевым на всей области определения, но становится таким при ограничении интервала — это позволит лучше понять, как непрерывность по Липшицу зависит от области анализа.\n\nВозьмем функцию: $f(x) = e^{x}$, где градиент функции: $\\nabla f(x) = e^{x}.$\n\nПроверим непрерывность градиента по Липшицу:\n\nДля функции $f(x) = e^{x}$ градиент $\\nabla f(x) = e^{x}$ не ограничен на всей числовой прямой, поэтому он не является непрерывным по Липшицу на $\\mathbb{R}$. Однако если мы ограничим область определения, например, интервалом $[a, b]$, то можем найти константу Липшица на этом интервале.\n\nДопустим, рассматриваем $x \\in [0, 1]$.\n\nТогда для любых $x_1, x_2 \\in [0, 1]$:\n\n$$|\\nabla f(x_1) - \\nabla f(x_2)| = |e^{x_1} - e^{x_2}| \\leq e^{b} |x_1 - x_2| = e^{1} |x_1 - x_2|\n$$\n\nТаким образом, на интервале $[0, 1]$ градиент функции $f(x) = e^{x}$ является непрерывным по Липшицу с константой $L = e^{1}$.\n\nКонстанта Липшица: $L = e^{1} \\approx 2.718$.\n\n{% cut \"Как мы нашли константу Липшица?\" %}\n\nВ данном случае мы воспользовались свойством, что если функция дважды дифференцируема и её вторая производная ограничена, то градиент функции является Липшицевым с константой, равной максимальному значению второй производной.\n\nДля функции $f(x) = e^{x}$:\n\n- Первая производная (градиент): $\\nabla f(x) = e^{x}.$\n- Вторая производная: $f''(x) = e^{x}.$\n\nНа интервале $[0, 1]$:\n\n- Максимальное значение второй производной:\n\n  $$\\max_{x \\in [0, 1]} f''(x) = f''(1) = e^{1} = e \\approx 2{.}718.\n  $$\n  \n  \n\nКак итог:\n\n- Градиент $\\nabla f(x)$ является Липшицевым с константой $L = e^{1}$.\n\n{% endcut %}\n\nВыберем шаг обучения: $\\alpha = \\dfrac{1}{L} = \\dfrac{1}{e^{1}} \\approx 0.368$.\n\nИтеративное обновление:\n\n$$x_{k+1} = x_k - \\alpha \\nabla f(x_k) = x_k - \\dfrac{1}{e^{1}} e^{x_k} = x_k - e^{x_k - 1}.\n$$\n\nСходимость:\n\nПоскольку функция $f(x) = e^{x}$ выпукла и её градиент Липшицев с константой $L$ на отрезке $[0, 1]$, последовательность $\\{ x_k \\}$ будет сходиться к точке минимума на этом интервале.\n\nОднако заметим, что функция $f(x) = e^{x}$ не имеет глобального минимума на $\\mathbb{R}$, так как она неограниченно возрастает при $x \\to \\infty$ и стремится к нулю при $x \\to -\\infty$. Поэтому градиентный спуск будет стремиться к $x \\to -\\infty$.\n\nДанный пример демонстрирует важный факт:\n\n\u003e Сходимость градиентного спуска не требует глобальной непрерывности градиента по Липшицу на всей области определения функции. Достаточно, чтобы градиент обладал этим свойством локально, в окрестности оптимального решения.\n\nВедь на практике множество задач оптимизации предполагает, что искомый минимум функции лежит в некоторой компактной области, где условия Липшица выполняются. Например, если шаг обучения $\\alpha$ выбран из интервала $\\left(0, \\frac{1}{L}\\right)$, где $L$ — локальная константа Липшица для градиента, то градиентный спуск будет сходиться к минимуму, даже если глобальная непрерывность градиента по Липшицу нарушается.\n\nПрименение этого подхода:\n\n1. Вместо работы с глобальной областью $\\mathbb{R}^n$ достаточно сосредоточиться на компактной области вокруг решения, где выполняются условия гладкости. Это снижает требования к функциям и упрощает анализ.\n2. В реальных задачах важно понимать, что сходимость алгоритмов оптимизации может быть обеспечена даже для функций, не обладающих глобальной Липшицевой гладкостью, если проблема ограничивается локальной областью, где условия гладкости выполняются.\n\n{% cut \"Другой пример функции, не обладающей непрерывностью по Липшицу\" %}\n\nРассмотрим функцию $f(x) = \\sqrt{x}$ на интервале $[0, 1]$.\n\n**Анализ:**\n\n- Для $x_1 = 0$ и $x_2 = x \u003e 0$ имеем:\n\n  $|f(x) - f(0)| = \\sqrt{x} - 0 = \\sqrt{x}.$\n\n- Разделим это выражение на $|x - 0| = x$:\n\n$\\frac{|f(x) - f(0)|}{|x - 0|} = \\frac{\\sqrt{x}}{x} = \\frac{1}{\\sqrt{x}}.$\n\n- При $x \\to 0$ значение $\\frac{1}{\\sqrt{x}}$ стремится к бесконечности, что означает, что не существует конечной константы $L$, удовлетворяющей условию Липшица.\n\n![3.2.5.webp](https://yastatic.net/s3/education-portal/media/3_2_5_d5e42cd05f.webp)\n\nТеперь посмотрим, как реализуется градиентный спуск для нашей функции. Функция $f(x) = \\sqrt{x}$ определена для $x \\geq 0$, и её градиент (производная) равен $\\nabla f(x) = \\frac{1}{2\\sqrt{x}}$. Однако из-за того, что градиент стремится к бесконечности при $x \\to 0$, градиентный спуск на этой функции может быть нестабильным.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Спуск для $f(x) = \\sqrt{x}$\" src=\"https://yastatic.net/s3/education-portal/media/3_2_6_ec1cc389a4.webp\"\u003e\n  \u003cfigcaption\u003e\n\nСпуск для $f(x) = \\sqrt{x}$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nОсновная проблема, демонстрируемая на графике, связана с тем, что функция $\\sqrt{x}$ определена только для $x \u003e 0$. Однако в процессе градиентного спуска значение $x$ уменьшается и в итоге становится отрицательным, из-за чего дальнейшие вычисления становятся невозможными (градиент $1/(2\\sqrt{x})$ перестаёт существовать при $x \\leq 0$).\n\nВот что мы видим на графике:\n\n1. $x$, расположенный по оси $y$, постепенно уменьшается на каждом шаге (градиентный спуск корректно работает для положительных значений $x$).\n2. Критический момент: значение $x$ становится отрицательным (пересекает красную линию $x = 0$), что не соответствует области определения функции $\\sqrt{x}$.\n3. После пересечения $x = 0$ градиентный спуск не может продолжаться, так как вычисление градиента невозможно.\n\nГрадиентный спуск «ломается», потому что:\n\n- Функция $f(x) = \\sqrt{x}$ не является Липшицевой на всей области, а градиент не определён при $x \\leq 0$.\n- Шаг обучения $\\alpha$ не ограничивает уменьшение $x$ до допустимой области.\n\n{% endcut %}\n\nА теперь пора посмотреть на практическом примере, как концепции предела и непрерывности функции реализуются в анализе сходимости реальных алгоритмов машинного обучения, а также при обеспечении устойчивости моделей.\n\n## Анализ сходимости алгоритмов и устойчивости моделей\n\nИтак, давайте представим, что мы решаем задачу машинного обучения, где необходимо найти такие параметры модели, которые минимизируют ошибку предсказания, — это центральная идея большинства алгоритмов. Эти параметры обновляются на каждом шаге, и анализ того, как они ведут себя со временем, помогает понять, действительно ли алгоритм приближается к цели.\n\nОдин из наиболее известных подходов — это градиентный спуск, целью которого является минимизация функции потерь $L(\\theta)$. Она показывает, насколько текущая модель далека от истинных значений или реальных меток. Параметры $\\theta$, которые определяют модель, обновляются по следующему правилу:\n\n$$\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k),\n$$\n\nгде:\n\n- $\\alpha$ — шаг обучения (контролирует размер шага в направлении уменьшения ошибки),\n- $\\nabla L(\\theta_k)$ — градиент функции потерь, указывающий направление возрастания и скорость изменения $L(\\theta)$ в текущей точке $\\theta_k$.\n\nКак видите, здесь важны предел и непрерывность:\n\n- **Предел последовательности.** Последовательность $\\{\\theta_k\\}$ должна сходиться к оптимальному значению $\\theta^*$. Анализ сходимости позволяет определить, произойдёт ли это при выбранных условиях (например, при правильном шаге обучения $\\alpha$).\n- **Непрерывность функции потерь.** Если функция $L(\\theta)$ непрерывна и её градиент не изменяется резко, алгоритм работает более стабильно и предсказуемо.\n\nДля иллюстрации основного принципа градиентного спуска рассмотрим максимально упрощённую задачу линейной регрессии с одним наблюдением:\n\n$$L(\\theta) = \\dfrac{1}{2} (y -\\hat{y})^2 = \\dfrac{1}{2} (y - \\theta x)^2\n$$\n\n- Градиент $\\nabla L(\\theta) = (\\theta x - y) x$.\n- Итеративное обновление: $\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k) = \\theta_k - \\alpha (\\theta_k x - y) x.$\n\nВ зависимости от значений $x$ и $y$ мы можем проанализировать сходимость алгоритма. Чтобы сосредоточиться исключительно на анализе, мы возьмём $x = 1$ и $y = 0$. При этих значениях задача сводится к следующему:\n\n- Функция потерь: $L(\\theta) = \\dfrac{1}{2} \\theta^2$.\n- Градиент: $\\nabla L(\\theta) = \\theta$.\n- Обновление параметра: $\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k) = (1 - \\alpha) \\theta_k$.\n\nЗдесь параметр $\\alpha$ — это шаг обучения, который контролирует, насколько сильно мы обновляем $\\theta$ на каждом шаге. Для того чтобы последовательность параметров $\\{\\theta_k\\}$ сходилась к минимуму функции потерь ($\\theta^* = 0$), необходимо выполнение условия:\n\n$$|1 - \\alpha| \u003c 1.\n$$\n\nРаскроем это неравенство:\n\n- $1 - \\alpha \u003e -1 \\implies \\alpha \u003c 2$,\n- $1 - \\alpha \u003c 1 \\implies \\alpha \u003e 0$.\n\nТаким образом, градиентный спуск сходится к минимуму, если шаг обучения $\\alpha$ удовлетворяет условию $0 \u003c \\alpha \u003c 2$.\n\nЭто условие связано с непрерывностью градиента функции потерь $L(\\theta)$. Для текущего примера константа Липшица $L = 1$, что позволяет явно установить границы на $\\alpha$. Если $\\alpha$ выходит за пределы этого диапазона, то обновления параметра становятся слишком резкими и последовательность $\\{\\theta_k\\}$ начинает расходиться.\n\nАнализ сходимости показывает, как алгоритмы оптимизации приближаются к желаемому результату, шаг за шагом минимизируя ошибку. При этом ключевую роль играют `пределы` и `непрерывность`: они не только гарантируют, что процесс сходится, но и делают его предсказуемым и стабильным. Однако сходимость — это лишь часть задачи. Чтобы алгоритм был действительно эффективным, важно, чтобы модель оставалась устойчивой, даже если данные содержат шум или небольшие отклонения. Поговорим теперь об устойчивости моделей.\n\n### Устойчивость моделей\n\nУстойчивая модель — это модель, результаты которой не меняются кардинально при небольших изменениях входных данных. Например, если мы добавим небольшой шум к нашим данным, устойчивая модель должна выдавать предсказания, которые остаются близкими к исходным.\n\nКак раз здесь мы и можем оценить роль `непрерывности`:\n\n- Непрерывность функции модели гарантирует, что малые изменения входных данных приводят к небольшим изменениям в предсказаниях модели. Это делает модель предсказуемой и надёжной.\n- Если модель обладает **`непрерывностью по Липшицу`**, можно количественно оценить, насколько сильно изменится результат при небольших изменениях на входе.\n\nВ этот раз в качестве примера устойчивой модели возьмем `линейную регрессию`.  Функция потерь линейной регрессии включает регуляризационный член $\\lambda \\|\\theta\\|^2$, который ограничивает величину параметров модели:\n\n$$L(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\theta^T x_i)^2 + \\frac{\\lambda}{2} \\|\\theta\\|^2,\n\n$$\n\nгде:\n\n- $x_i$ — вектор признаков,\n- $y_i$ — целевая переменная,\n- $\\lambda \u003e 0$ — коэффициент регуляризации.\n\nРегуляризация делает модель устойчивее, так как большие значения параметров подавляются, что предотвращает резкие изменения предсказаний.\n\n***\n\nВ этом параграфе мы увидели, как ключевые понятия математического анализа — пределы и непрерывность — помогают понять и гарантировать сходимость алгоритмов к оптимальному решению. Непрерывность функций и регуляризация способствуют устойчивости моделей, обеспечивая их надёжность даже при наличии шума или отклонений во входных данных.\n\nТеперь, когда мы разобрались с теорией, давайте перейдём к практике. Попробуйте решить задания по этой теме, чтобы закрепить материал и увидеть, как эти концепции работают в реальных задачах.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13716406.e2862518225adc72708426ea8984406394dd9eca/?iframe=1\" frameborder=\"0\" name=\"ya-form-13716406.e2862518225adc72708426ea8984406394dd9eca\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"62:T945c,"])</script><script nonce="">self.__next_f.push([1,"Дифференцирование — это один из ключевых инструментов математического анализа, который позволяет исследовать, как функция изменяется в каждой точке. Это понятие лежит в основе многих процессов в анализе данных и машинном обучении: от оптимизации функций потерь до вычисления градиентов и анализа поведения сложных моделей.\n\nВ этом параграфе мы подробно разберём:\n\n1. **Производную и правила дифференцирования**, чтобы понять, как находить скорость изменения функции и применять основные правила для работы со сложными выражениями.\n2. **Теоремы Ролля и Лагранжа**, которые дадут представление о том, как экстремумы (точки максимума и минимума функций) и изменения функции связаны с её производной.\n3. **Применение производной на практике**, где на конкретных примерах посмотрим, как использовать дифференцирование для нахождения экстремумов и анализа моделей.\n\nВы познакомитесь с основами, которые не только расширят понимание теории, но и помогут разобраться, как применять эти методы в задачах оптимизации и анализа данных.\n\nА начнём мы с понятия производной и основных правил дифференцирования, которые служат фундаментом для дальнейших разделов.\n\n## Производная и правила дифференцирования\n\nКогда мы изучаем поведение функции, нам важно не только знать значения самой функции в различных точках, но и понимать, как они меняются. Производная — это ключевой инструмент, который позволяет количественно описать скорость изменения функции в точке. Она описывает, насколько быстро изменяется значение функции $f(x)$, когда её аргумент $x$ изменяется на бесконечно малую величину.\n\n### Формальное определение\n\nПроизводная функции $f(x)$ в точке $x_0$ определяется как предел отношения приращения функции к приращению аргумента, если этот предел существует:\n\n$$f'(x_0) = \\lim_{\\Delta x \\to 0} \\dfrac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}.\n$$\n\nГде:\n\n- $\\Delta x$ — малое изменение (приращение) аргумента функции,\n- $f(x_0 + \\Delta x) - f(x_0)$ — соответствующее изменение (приращение) значения функции.\n\nФункция $f(x)$ называется `дифференцируемой в точке` $x_0$, если её производная $f'(x_0)$ существует. Это означает, что предел, представленный в формальном определении, существует и конечен.\n\nЕсли функция дифференцируема в каждой точке некоторого интервала, она называется `дифференцируемой на этом интервале`.\n\nВажно отметить, что если функция дифференцируема в точке $x_0$, то она обязательно непрерывна в этой точке. Однако обратное не всегда верно: непрерывность функции не гарантирует её дифференцируемость. Например, функция $f(x) = |x|$ непрерывна во всех точках, но не дифференцируема в точке $x = 0$.\n\n{% cut \"О различных формах нотации производной\" %}\n\nДля обозначения производной существуют разные нотации, каждая из которых находит применение в зависимости от ситуации и удобства. Давайте рассмотрим основные из них.\n\n1. **Лагранжева нотация:**\n\n   $$f'(x), y'\n   $$\n   \n   Эта нотация наиболее часто используется в классическом математическом анализе и кратко указывает на производную функции $f(x)$ или $y$ по её аргументу $x$.\n\n   - Пример: $f'(x_0)$ — производная функции $f(x)$ в точке $x_0$.\n\n2. **Нотация Лейбница:**\n\n   $$\\dfrac{dy}{dx}, \\; \\dfrac{df}{dx} \\text{ или } \\dfrac{d}{dx} f(x)\n   $$\n   \n   Эта нотация подчёркивает изменения зависимой переменной $y$ относительно независимой переменной $x$. Она особенно удобна в приложениях, таких как физика, где важно различать изменение переменных.\n\n   ![3.3.1.webp](https://yastatic.net/s3/education-portal/media/3_3_1_b8d1db86ef.webp)\n\n   - Пример: $\\dfrac{dy}{dx}$ читается как «изменение $y$ относительно изменения $x$».\n\n3. **Ньютонианская нотация:**\n\n   $$\\dot{y}, \\; \\ddot{y}\n   $$\n   \n   Эта форма используется в основном для производных по времени, например в механике. Первая производная обозначается одной точкой над функцией, а вторая — двумя. Такая нотация полезна, например, в задачах динамики.\n\n   - Пример: $\\dot{y}$ — скорость, $\\ddot{y}$ — ускорение.\n\n4. **Операторная нотация**:\n\n   $$Df(x), \\; D^2f(x) \\text{ или } D_y f(x)\n   $$\n   \n   Здесь $D$ используется для обозначения дифференцирования. Такой формат удобен для работы с более сложными операторами, особенно в многомерных задачах.\n\n   - Пример: $D^2f(x)$ обозначает вторую производную функции $f(x)$.\n\nВ каждой нотации скрыта идея предела:\n\n$\\dfrac{dy}{dx} = \\lim_{\\Delta x \\to 0} \\dfrac{\\Delta y}{\\Delta x}, \\quad f'(x) = \\lim_{h \\to 0} \\dfrac{f(x + h) - f(x)}{h}.$\n\nВсе эти формы отражают одно и то же: стремление к локальному описанию изменений функции. Каждое из них подходит для определённого контекста: от глубокого теоретического анализа до прикладных задач в вычислениях и программировании. Важно помнить, что, независимо от выбранной нотации, суть операции остаётся неизменной.\n\n{% endcut %}\n\nТеперь попробуем понять геометрический смысл производной через такие понятия, как `секущая линия` и `касательная`.\n\nСекущая линия — это прямая, которая пересекает график функции в двух точках. Если эти две точки всё ближе смещаются друг к другу, секущая линия начинает «приближаться» к касательной — прямой, которая касается графика функции только в одной точке и имеет с ним одинаковый наклон.\n\n![3.3.2.webp](https://yastatic.net/s3/education-portal/media/3_3_2_448bf4b6e1.webp)\n\nНа иллюстрации мы видим, как формула производной основывается на секущей линии, наклон которой определяется отношением приращений $f(x_0 + \\Delta x) - f(x_0)$ и $\\Delta x$:\n\n$$m_{\\text{secant}} = \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}.\n$$\n\nЗдесь $\\Delta x$ — расстояние между точками по горизонтали, а $f(x_0 + \\Delta x) - f(x_0)$ это изменение функции между этими точками. Это отношение даёт наклон секущей, который описывает среднюю скорость изменения функции на интервале $[x_0, x_0 + \\Delta x]$. Однако это ещё не производная.\n\n![3.3.3.webp](https://yastatic.net/s3/education-portal/media/3_3_3_e4dd9de129.webp)\n\nПроизводная появляется, когда мы «приближаем» точку пересечения секущей с графиком к одной-единственной точке с координатами $(x_0, f(x_0))$, то есть берём предел при $\\Delta x \\to 0$. В этом случае секущая линия становится касательной, а её наклон — это уже точная скорость изменения функции в точке:\n\n$$m_{\\text{tangent}} = f'(x_0) = \\lim_{\\Delta x \\to 0} \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}.\n$$\n\nВ реальных вычислениях (например, в численных методах) мы часто не можем работать с идеальными математическими пределами. Вместо этого мы используем приближения:\n\n- Если у нас есть только конечные данные (например, измеренные точки функции), мы можем построить секущую линию между двумя соседними точками и оценить производную.\n- Это приближение особенно важно для численного дифференцирования в таких областях, как обработка сигналов, оптимизация и машинное обучение.\n\n### Пример вычисления производной\n\nРассмотрим функцию $f(x) = x^2$. Найдем её производную в точке $x_0 = 2$:\n\n$f'(x) = \\lim_{\\Delta x \\to 0} \\dfrac{(x + \\Delta x)^2 - x^2}{\\Delta x}.$\n\nРаскроем скобки:\n\n$$f'(x) = \\lim_{\\Delta x \\to 0} \\dfrac{x^2 + 2x\\Delta x + (\\Delta x)^2 - x^2}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\dfrac{2x\\Delta x + (\\Delta x)^2}{\\Delta x}.\n$$\n\nСокращаем на $\\Delta x$:\n\n$$f'(x) = \\lim_{\\Delta x \\to 0} (2x + \\Delta x).\n$$\n\nПри $\\Delta x \\to 0$:\n\n$$f'(x) = 2x.\n$$\n\nВ точке $x_0 = 2$: $f'(2) = 2 \\cdot 2 = 4.$\n\n![3.3.4.webp](https://yastatic.net/s3/education-portal/media/3_3_4_671fff21bb.webp)\n\n\u003e **Важно:** В примере выше мы использовали формальное определение производной через предел, чтобы показать весь процесс вычисления «с нуля». Однако в большинстве случаев такой подход оказывается избыточно громоздким.\n\nВ повседневной работе часто используются заранее известные производные для наиболее распространённых функций, таких как степенные, экспоненциальные, логарифмические и тригонометрические. Эти производные составляют основу, на которой строится работа с более сложными выражениями.\n\nДавайте рассмотрим производные основных функций, прежде чем перейти к правилам дифференцирования.\n\n### Производные основных функций\n\nДля дальнейшей работы с производными важно знать производные наиболее часто встречающихся функций. Ниже приведены основные из них:\n\n- **Степенная функция:** $f(x) = x^n \\Rightarrow f'(x) = n x^{n-1}$.\n- **Экспоненциальная функция:** $f(x) = e^x \\Rightarrow f'(x) = e^x$.\n- **Логарифм:** $f(x) = \\ln x \\Rightarrow f'(x) = \\frac{1}{x}$.\n- **Синус и косинус:** $\\sin x \\to \\cos x$, $\\cos x \\to -\\sin x$.\n\nПолную таблицу с основными функциями и их производными можно найти в справочной таблице ниже.\n\n{% cut \"Справочная таблица по производным основных функций\" %}\n\n![3.3.5.webp](https://yastatic.net/s3/education-portal/media/3_3_5_37139fbb22.webp)\n\nПримечания:\n\n1. **Сложные функции.** Для функций, которые представляют собой комбинации основных, можно применять правила дифференцирования, такие как правило суммы, произведения, частного и цепное правило.\n2. **Периодические ограничения.** Для тригонометрических функций производные имеют ограничения в точках разрывов (например, $\\tg x$ разрывна в $x = \\frac{\\pi}{2} + k\\pi$).\n\n{% endcut %}\n\nОсвоив их, вы сможете быстро находить производные сложных функций, комбинируя правила дифференцирования, которые мы рассмотрим далее.\n\n### Правила дифференцирования\n\nЧтобы вычислять производные сложных функций, используются специальные правила, которые упрощают процесс.\n\n1. Производная суммы или разности:\n   Если $f(x)$ и $g(x)$ — дифференцируемые функции, то:\n\n$$(f(x) \\pm g(x))' = f'(x) \\pm g'(x)\n$$\n\n**Пример:** Пусть $f(x) = x^2 + 3x$. Тогда: $f'(x) = (x^2)' + (3x)' = 2x + 3$.\n\n2. Производная произведения (правило Лейбница):\n   Если $f(x)$ и $g(x)$ — дифференцируемые функции, то:\n\n$$(f(x) \\cdot g(x))' = f'(x) g(x) + f(x) g'(x)\n$$\n\n**Пример:** Пусть $f(x) = x^2 \\cdot \\sin(x)$. Тогда:\n\n$$f'(x) = (x^2)' \\cdot \\sin(x) + x^2 \\cdot (\\sin(x))' = 2x \\sin(x) + x^2 \\cos(x)$$\n\n3. Производная частного:\n   Если $f(x)$ и $g(x)$ — дифференцируемые функции, и $(x) \\neq 0$, то:\n\n$$\\left(\\frac{f(x)}{g(x)}\\right)' = \\frac{f'(x) g(x) - f(x) g'(x)}{(g(x))^2}\n$$\n\n**Пример:** Пусть $f(x) = \\dfrac{x}{\\sin(x)}$. Тогда:\n\n$$f'(x) = \\dfrac{(x)' \\cdot \\sin(x) - x \\cdot (\\sin(x))'}{\\sin^2(x)} = \\dfrac{\\sin(x) - x \\cos(x)}{\\sin^2(x)}$$\n\n4. Производная сложной функции (правило цепочки):\n\nЕсли $y=f(u)$ и $u=g(x)$, то:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n$$\n\n**Пример:** Пусть $f(x) = \\sin(x^2)$. Тогда:\n\n$$f'(x) = (\\sin(x^2))' = \\cos(x^2) \\cdot (x^2)' = \\cos(x^2) \\cdot 2x$$\n\nИтак, с производными разобрались. Теперь самое время поговорить о том, как изменение функции на некотором интервале связано со значениями её производных внутри этого интервала.\n\nВ этом нам помогут теоремы Ролля и Лагранжа, которые играют ключевую роль в анализе поведения функций и предоставляют теоретическую основу для многих методов оптимизации и решения прикладных задач.\n\n## Теоремы Ролля и Лагранжа\n\nВ дифференциальном исчислении существуют фундаментальные теоремы, которые связывают поведение функции на отрезке с поведением её производных. Две из таких теорем — `теорема Ролля` и `теорема Лагранжа` (также известная как теорема о среднем значении). Они дают теоретическую основу для многих методов оптимизации и решения прикладных задач, например анализа изменений функции потерь в обучении моделей машинного обучения.\n\n### Теорема Ролля\n\nСначала посмотрим на формулировку теоремы Ролля, а затем на её интуитивное понимание:\n\nПусть функция $f(x)$ удовлетворяет следующим условиям на отрезке $[a, b]$:\n\n1. Непрерывность на отрезке $[a,b]$. Функция $f(x)$ непрерывна на всём отрезке от $a$ до $b$.\n2. Дифференцируемость на интервале $(a, b)$. Функция $f(x)$ имеет производную в каждой точке внутри интервала $(a, b)$.\n3. Равенство значений на концах отрезка: $f(a) = f(b)$.\n\nТогда существует хотя бы одна точка $c$ в интервале $(a, b)$ такая, что:\n\n$$f'(c) = 0.$$\n\nТеорема Ролля говорит нам, что если функция начинается и заканчивается на одной и той же высоте (т. е. $f(a) = f(b)$) и она непрерывна и дифференцируема внутри отрезка, то где-то между $a$ и $b$ функция должна иметь горизонтальную касательную, то есть производная должна быть нулевой. Это похоже на то, как если бы вы шли по холму и вернулись в ту же самую точку по высоте — где-то вы должны были либо подниматься, либо спускаться, а в какой-то момент идти по горизонтали.\n\n![3.3.6..webp](https://yastatic.net/s3/education-portal/media/3_3_6_4ab667b52d.webp)\n\n{% cut \"Пример\" %}\n\nРассмотрим функцию $f(x) = \\sin(x)$ на отрезке $[0, \\pi]$:\n\n1. Непрерывность. Функция $sin(x)$ непрерывна на всём множестве действительных чисел, в частности на $[0, \\pi]$.\n2. Дифференцируемость. Производная $f'(x) = \\cos(x)$ существует для всех $x$, значит, функция дифференцируема на $(0, \\pi)$.\n3. Равенство значений на концах отрезка. $f(0) = \\sin(0) = 0$ и $f(\\pi) = \\sin(\\pi) = 0$.\n\nПо теореме Ролля, существует точка $c \\in (0, \\pi)$, такая что $f'(c) = 0$. Найдём эту точку:\n\n$$f'(x) = \\cos(x) = 0 \\quad \\Rightarrow \\quad x = \\dfrac{\\pi}{2}.\n$$\n\nТаким образом, в точке $c = \\dfrac{\\pi}{2}$ производная функции равна нулю.\n\n{% endcut %}\n\nЕсли совсем упростить значимость теоремы Ролля, то она гарантирует наличие экстремума (минимума или максимума) при определённых условиях. Получается, что в задачах минимизации или максимизации функций (например, в машинном обучении при оптимизации функции потерь) именно эта теорема помогает установить условия существования экстремумов.\n\n{% cut \"Прикладной пример\" %}\n\nРассмотрим задачу прогнозирования сезонных продаж, где требуется определить, в какой момент продажи достигли локального пика (максимума) или дна (минимума) за месяц. Пусть значения продаж заданы как функция $f(t)$, где $t$ — дни месяца. Предположим, что продажи в первый и последний день месяца совпадают, т. е. $f(1) = f(30)$.\n\nПрименяя теорему Ролля, можем утверждать, что существует хотя бы один день $t = c \\in (1, 30)$, в который производная $f'(c)$ равна нулю, т. е. продажи не меняются (локальный пик или дно). Этот результат может быть использован для анализа трендов и определения наиболее значимых дней в месяце, когда изменения в продажах могут повлиять на стратегию.\n\nВозьмём $f(t) = \\sin\\left(\\frac{\\pi}{29}(t - 1)\\right)$, где $t \\in [1, 30]$:\n\n1. Непрерывность: $f(t)$ непрерывна на $[1, 30]$.\n2. Дифференцируемость: $f'(t) = \\frac{\\pi}{29} \\cos\\left(\\frac{\\pi}{29}(t - 1)\\right)$ существует на $(1, 30)$.\n3. Равенство значений на концах отрезка: $f(1) = \\sin(0) = 0 \\quad \\text{и} \\quad f(30) = \\sin(\\pi) = 0.$\n\nПо теореме Ролля, существует точка $c \\in (1, 30)$, где $f'(c) = 0$. Найдём эту точку, решив уравнение:\n\n$$\\frac{\\pi}{29} \\cos\\left(\\frac{\\pi}{29}(c - 1)\\right) = 0 \\implies \\cos\\left(\\frac{\\pi}{29}(c - 1)\\right) = 0.\n$$\n\nРешение:\n\n$$\\frac{\\pi}{29}(c - 1) = \\frac{\\pi}{2} + k\\pi, \\quad k \\in \\mathbb{Z}\n$$\n\nНа интервале $(1, 30)$ единственным решением является: $c=15.5$, что соответствует середине между 15-м и 16-м днями месяца.\n\nТаким образом, продажи достигают локального максимума или минимума вблизи 15-го и 16-го дней месяца. Этот результат можно использовать для прогнозирования сезонных изменений продаж или планирования акций, определяя ключевые периоды, когда изменения в продажах наиболее значимы.\n\n{% endcut %}\n\n### Теорема Лагранжа\n\nВзглянем на формулировку теоремы:\n\nПусть функция $f(x)$ удовлетворяет следующим условиям на отрезке $[a, b]$:\n\n1. Непрерывность на отрезке $[a, b]$. Функция $f(x)$ непрерывна на всём отрезке от $a$ до $b$.\n2. Дифференцируемость на интервале $(a, b)$. Функция $f(x)$ имеет производную в каждой точке внутри интервала $(a, b)$.\n\nТогда существует хотя бы одна точка $c$ в интервале $(a, b)$ такая, что:\n\n$$f'(c) = \\dfrac{f(b) - f(a)}{b - a}.$$\n\nТеорема Лагранжа утверждает, что на интервале $[a, b]$ существует точка, в которой мгновенная скорость изменения функции (производная) равна средней скорости изменения функции на всём отрезке.\n\n![3.3.7.webp](https://yastatic.net/s3/education-portal/media/3_3_7_25282e7e31.webp)\n\nЭто означает, что касательная к графику функции в точке $c$ параллельна секущей, которая соединяет точки $(a, f(a))$ и $(b, f(b))$. В какой-то мере теорема обобщает утверждение теоремы Ролля и связывает среднее изменение функции с её производной.\n\n{% cut \"Пример\" %}\n\nВ качестве примера рассмотрим функцию $f(x) = x^2$ на отрезке $[1, 3]$:\n\n1. Непрерывность. Полиномиальные функции, такие как $x^2$, непрерывны на всём множестве действительных чисел.\n2. Дифференцируемость. Производная $f'(x)=2x$ существует для всех $x$, значит, функция дифференцируема на $(1,3)$.\n\nВычислим среднюю скорость изменения функции на отрезке $[1, 3]$:\n\n$$\\dfrac{f(3) - f(1)}{3 - 1} = \\dfrac{9 - 1}{2} = \\dfrac{8}{2} = 4.\n$$\n\nПо теореме Лагранжа, существует точка $c \\in (1, 3)$, такая, что:\n\n$$f'(c) = 4.\n$$\n\nВычислим $f'(x) = 2x$ и решим уравнение:\n\n$$2c = 4 \\quad \\Rightarrow \\quad c = 2.\n$$\n\nТаким образом, в точке $c = 2$ мгновенная скорость изменения функции равна средней скорости изменения на отрезке.\n\n{% endcut %}\n\nТеорема Лагранжа находит применение в оптимизации и анализе производительности функций, показывая, где скорость изменения функции соответствует средней. А также в контексте машинного обучения данная теорема помогает понять и доказать сходимость алгоритмов, поведение функций потерь и градиентов.\n\n{% cut \"Прикладной пример\" %}\n\nПредположим, мы обучаем модель линейной регрессии и используем функцию потерь $L(\\theta)$, зависящую от параметра $\\theta$. Если $L(\\theta)$ непрерывна и дифференцируема на отрезке $[\\theta_1, \\theta_2]$, то, по теореме Лагранжа, существует значение $\\theta = \\theta^*$ между $\\theta_1$ и $\\theta_2$, такое, что:\n\n$$L'(\\theta^*) = \\dfrac{L(\\theta_2) - L(\\theta_1)}{\\theta_2 - \\theta_1}.\n$$\n\nЭто значит, что изменение функции потерь между двумя значениями параметра можно связать с производной в некоторой точке между ними. Это полезно для анализа скорости сходимости алгоритмов оптимизации и выбора шагов изменения параметров.\n\n{% endcut %}\n\nОбе теоремы — мощные инструменты, которые помогают глубже понять поведение функций. Они связывают значение функции с её производной, предоставляя теоретическую основу для анализа изменения функций и задач оптимизации.\n\nВ следующей части мы применим эти результаты для оптимизации простых моделей.\n\n## Оптимизация простых моделей и анализ изменения функций\n\nДавайте разберём основы оптимизации функций — процесса поиска экстремальных значений (минимумов и максимумов), которые играют важную роль в математическом анализе, физике, экономике и особенно в машинном обучении.\n\nМы посмотрим, как производные позволяют находить точки экстремумов и анализировать поведение функций на разных интервалах.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Критические точки функции $f(x)$\" src=\"https://yastatic.net/s3/education-portal/media/3_3_8_75cba1308a.webp\"\u003e\n  \u003cfigcaption\u003e\n\n   Критические точки функции $f(x)$\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nОптимизация — это поиск таких значений, при которых функция принимает своё наименьшее или наибольшее значение. Например:\n\n- В экономике задача может заключаться в максимизации прибыли.\n- В машинном обучении — в минимизации функции потерь для настройки параметров модели.\n\nПроизводные дают точный инструмент для решения этих задач, так как они описывают скорость изменения функции и позволяют выявить её `критические точки` — места, где функция перестаёт возрастать или убывать. В критических точках первая производная функции равна нулю или не существует. То есть такие точки — потенциальные кандидаты на искомые экстремумы.\n\nФункция $f(x)$ имеет `экстремум` в точке $x_0$, если в этой точке она достигает локального минимума или максимума:\n\n- Локальный минимум — значение функции меньше всех остальных значений в небольшой окрестности точки.\n- Локальный максимум — значение функции больше всех остальных значений в небольшой окрестности точки.\n\nВажно отметить, что в точках экстремума $x_0$, где функция $f(x)$ дифференцируема, её первая производная равна нулю: $f'(x_0) = 0$. Однако не все критические точки являются экстремумами — это зависит от поведения функции в окрестности точки.\n\nДля анализа характера этих точек (локальный минимум, максимум или перегиб) используется:\n\n1. Первая производная — для нахождения критических точек.\n2. Вторая производная — для определения выпуклости функции в этих точках.\n\n\u003e **Выпуклость функции** — это свойство, описывающее, как функция изменяется на заданном интервале. Функция называется выпуклой вниз (или просто выпуклой), если её график «лежит ниже» любой прямой, соединяющей две произвольные точки на графике. Аналогично функция выпукла вверх (или вогнута), если её график «лежит выше» такой прямой.\n\n![3.3.9.webp](https://yastatic.net/s3/education-portal/media/3_3_9_7b38edd551.webp)\n\n### Этапы оптимизации функций\n\n1. Нахождение критических точек:\n   - Вычисляем первую производную $f'(x)$.\n   - Находим точки, где $f'(x) = 0$ или $f'(x)$ не существует. Эти точки называются критическими.\n2. Анализ характера критических точек:\n   - С помощью второй производной $f''(x)$:\n     - Если $f''(x_0)\u003e0$, в точке $x_0$ — локальный минимум.\n     - Если $f''(x_0)\u003c0$, в точке $x_0$ — локальный максимум.\n     - Если $f''(x_0)=0$, требуется дополнительный анализ (например, исследование выпуклости функции).\n3. Определение интервалов возрастания и убывания:\n   - Если $f'(x) \u003e 0$ на интервале, функция возрастает.\n   - Если $f'(x) \u003c 0$, функция убывает.\n4. Анализ выпуклости функции:\n   - Если $f''(x) \u003e 0$ на интервале, функция выпукла вниз.\n   - Если $f''(x) \u003c 0$, функция выпукла вверх.\n5. Оценка на границах:\n   - Если функция определена на отрезке $[a,b]$, нужно также проверить её значения в точках $a$, $b$ и критических точках.\n\n{% cut \"Пример\" %}\n\nРассмотрим функцию $f(x) = x^2 - 4x + 3$. Найдём её экстремумы.\n\n1. Находим первую производную: $f'(x) = 2x - 4$.\n\n2. Решаем уравнение $f'(x) = 0$:\n\n   $$2x - 4 = 0 \\quad \\Rightarrow \\quad x = 2.\n   $$\n   \n   \n\n3. Находим вторую производную:\n\n$$    f''(x) = 2 \u003e 0.\n$$\n\nСледовательно, точка $x = 2$ является локальным минимумом.\n\n1. Вычисляем значение функции в этой точке:\n\n   $$f(2) = 2^2 - 4 \\cdot 2 + 3 = -1.\n   $$\n   \n   \n\n2. Интервалы возрастания и убывания:\n\n- $f'(x) \u003e 0$ для $x\u003e2$: функция возрастает.\n- $f'(x) \u003c 0$ для $x\u003c2$: функция убывает.\n\n![3.3.10.webp](https://yastatic.net/s3/education-portal/media/3_3_10_69b752eb43.webp)\n\nТаким образом, функция $f(x)$ достигает локального минимума в точке $x = 2$, где её значение равно $-1$. На интервале $(-\\infty, 2)$ функция убывает, а на интервале $(2, \\infty)$ — возрастает.\n\n{% endcut %}\n\nПроизводные — это инструмент, который помогает понять, как ведёт себя функция, находить её максимумы и минимумы, а также принимать взвешенные решения.\n\nВы уже узнали, что такое производная функции одной переменной, каков её геометрический и физический смысл, познакомились с основными правилами дифференцирования и научились их применять.\n\nЭти знания — основа для понимания сложных алгоритмов и моделей, которые используются в анализе данных и машинном обучении. Оптимизация с помощью производных — это не просто математика, а универсальный метод решения задач в разных сферах.\n\nПереходите к квизу и задачам, чтобы закрепить изученный материал. Это поможет вам лучше понять, как производные и оптимизация применяются в реальных сценариях.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13717003.27e1b173e1423d58469b2d7d9ae042665a64d8ce/?iframe=1\" frameborder=\"0\" name=\"ya-form-13717003.27e1b173e1423d58469b2d7d9ae042665a64d8ce\" width=\"650\"\u003e\u003c/iframe\u003e"])</script><script nonce="">self.__next_f.push([1,"63:T1cec,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы погрузимся в линейную алгебру — язык, на котором говорят данные.\n\nМы сделаем акцент не на строгой теории, а на геометрической интуиции и практическом применении её инструментов в анализе данных и машинном обучении. Вы увидите, как абстрактные векторы и матрицы превращаются в мощные инструменты для представления данных, выявления скрытых закономерностей и построения предсказательных моделей.\n\nВ следующих параграфах этой главы мы обсудим ключевые концепции и их применение:\n\n- **Фундаментальные объекты: векторы и матрицы.** Мы начнём с основ: повторим, как векторы используются для описания признаков, а матрицы — для представления датасетов и линейных преобразований. Разберёмся с понятием линейной независимости, чтобы избегать избыточности в данных, и поработаем с Python-библиотеками `numpy` и `pandas`.\n- **Ключевые инструменты: системы линейных уравнений (СЛУ) и определитель.** Далее мы рассмотрим, как СЛУ помогают решать практические задачи, и поймём глубокий геометрический смысл определителя — как он описывает изменение объёма пространства, что важно для понимания работы многих алгоритмов.\n- **Геометрия признакового пространства: нормы, расстояния и углы.** Здесь мы перейдём от объектов к измерениям. Вы узнаете, что такое нормы ($L_1$, $L_2$) и как они определяют длину векторов. Мы разберём, как скалярное произведение и косинусная мера помогают оценить сходство и как из ортогональных проекций выводится метод наименьших квадратов.\n- **Построение эффективных базисов: ортогонализация и QR-разложение.** Мы изучим, как с помощью метода Грама — Шмидта превратить обычный набор признаков в идеальный — ортонормированный, где все направления независимы. Это приведёт нас к QR-разложению — мощному инструменту для повышения численной устойчивости алгоритмов.\n- **Снижение размерности и латентные факторы.** Мы научимся находить скрытую структуру в данных высокой размерности. Вы увидите, как метод главных компонент (PCA) и SVD-разложение извлекают главные направления изменчивости, а их аналоги — LSA и NMF — используются для тематического моделирования текстов и построения рекомендательных систем.\n- **Линейные модели и регуляризация.** В завершение мы применим все полученные знания для построения предсказательных моделей. Мы разберём линейную и логистическую регрессии, изучим метод опорных векторов (SVM) и ядровой трюк. А главное — поймём, как регуляризация (Ridge, Lasso) и препроцессинг признаков помогают бороться с переобучением и делают модели по-настоящему робастными, то есть устойчивыми к выбросам, шуму в данных и способными хорошо работать на новых примерах.\n\nМы ожидаем, что вы уже знакомы с базовыми операциями и понятиями, такими как векторы и матрицы. Но даже если нет, не переживайте — мы разберём всё с нуля. Вы увидите, как эти концепции из учебников оживают при решении реальных задач.\n\n\u003e 💡Наша цель — провести вас по полному циклу работы с данными с точки зрения линейной алгебры: от того, как представить объекты в виде векторов, до того, как построить на их основе устойчивую и интерпретируемую модель.\n\nЧто вам даст эта глава:\n\n- **Понимание базовых объектов.** Вы научитесь представлять данные в виде векторов и матриц и применять основные операции для их преобразования.\n- **Навыки измерения в многомерном пространстве.** Вы освоите различные нормы и метрики, поймёте их геометрический смысл и научитесь выбирать подходящие для конкретных задач: от регуляризации до кластеризации.\n- **Умение извлекать скрытую структуру.** Вы освоите методы снижения размерности для выявления латентных факторов, сжатия данных и построения рекомендательных систем.\n- **Навыки построения и стабилизации моделей.** Вы научитесь строить, интерпретировать и настраивать линейные модели, включая линейную и логистическую регрессии, а также SVM. Вы поймёте, как регуляризация и грамотный препроцессинг делают модели устойчивыми и эффективными.\n- **Практические навыки.** Используя Python-библиотеки `numpy` и `pandas`, вы освоите все ключевые операции: от матричных разложений до обучения и оценки моделей.\n\nА теперь давайте погрузимся в мир линейной алгебры!"])</script><script nonce="">self.__next_f.push([1,"64:T8fab,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы будем работать с векторами — одним из фундаментальных объектов линейной алгебры, который играет ключевую роль в анализе данных и машинном обучении.\n\nМы повторим основные понятия, связанные с векторами, но с акцентом на их применение для решения реальных задач в области обработки данных и обучения моделей. Также покажем, как работать с векторами в Python, используя такие библиотеки, как `numpy` и `pandas`.\n\nА начнём мы с определения вектора.\n\n### **Определение**\n\n`Вектор` — это математический объект, который в зависимости от области применения может иметь различные интерпретации и свойства.\n\nТак векторы можно встретить:\n\n- в геометрии,\n- компьютерных науках,\n- линейной алгебре.\n\n**В геометрии**\n\nГеометрически вектор — это направленный отрезок, задаваемый начальной точкой и конечной точкой. Для вектора, заданного точками $A(x_1, y_1, \\dots, z_1)$ и $B(x_2, y_2, \\dots, z_2)$, координаты вычисляются следующим образом:\n\n$$\\vec{AB} = (x_2 - x_1, y_2 - y_1, \\dots, z_2 - z_1)\n$$\n\nНапример, вектор, соединяющий точки $A(1, 2)$ и $B(3, 4)$, имеет координаты $\\vec{AB} = (2, 2)$.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Вектор\" src=\"https://yastatic.net/s3/education-portal/media/4_2_1_Vektor_soedinyayushhij_tochki_nad_AB_2_2_strelka_7b31766521.webp\"\u003e\n  \u003cfigcaption\u003e\n\nВектор $\\vec{AB}$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**В компьютерных науках**\n\nВ программировании и компьютерной графике векторы используются для представления данных в виде упорядоченного списка чисел для последующих вычислений над ними.\n\nТак, в машинном обучении наблюдение может быть представлено в виде вектора, состоящего из чисел, где каждое число соответствует значению соответствующего признака.\n\nАналогично в NLP (Natural Language Processing, обработка естественного языка) слова или части слов кодируются векторами, чтобы компьютер мог понять их семантику, а затем применить к ним математические операции.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Пример кодирования слов векторами\" src=\"https://yastatic.net/s3/education-portal/media/4_2_2_Embeddingi_slov_9bda332b6c.webp\"\u003e\n  \u003cfigcaption\u003e\n\nПример кодирования слов векторами\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**В линейной алгебре**\n\nАлгебраическое определение вектора в некотором роде объединяет два определения выше, но описывает его более абстрактно. Чтобы с ним разобраться, необходимо изучить понятие векторного пространства.\n\n`Векторным пространством` называется множество $V$ с операциями сложения и умножения на скаляр, удовлетворяющими определённым свойствам. И элементы этого множества называются `векторами`.\n\nГоворя по-простому, `векторное пространство` состоит из двух частей:\n\n- множество элементов,\n- операции над этими элементами, удовлетворяющие заданным свойствам.\n\nПримеры:\n\n- векторы-столбцы длины $n$;\n- многочлены степени не выше $n$;\n- непрерывные функции, определённые на отрезке $[a, b]$;\n- и так далее.\n\nНас в первую очередь будут интересовать именно векторы-столбцы. Как вы помните из курса школьной геометрии, точку на прямой можно задать с помощью одного числа $(x)$. Множество всех таких точек есть $\\mathbb{R}$ — то есть вся числовая прямая.\n\nАналогично, чтобы задать точку на плоскости, нужна уже пара чисел $(x, y)$, а множество всех таких пар есть $\\mathbb{R}^2$ плоскость. В трёхмерном пространстве всё описывается тремя числами $(x, y, z)$, и мы получаем $\\mathbb{R}^3$.\n\nПродолжая увеличивать количество измерений, мы приходим к пространству $\\mathbb{R}^n$ — это обобщение на случай $n$-измерений, и, чтобы задать точку, нам нужно указать $n$ координат. Таким образом, в $n$-мерном пространстве вектор представляет собой упорядоченный набор чисел:\n\n$$v =\n\\begin{pmatrix}\nv_1\\\\\nv_2\\\\\n\\vdots\\\\\nv_n\\\\\n\\end{pmatrix},\n$$\n\nгде каждое $v_i$ — это  $i$ -я координата вектора.\n\nНапример: в двумерном пространстве вектор $v$ имеет координаты $2$ по оси $x$ и $5$ по оси $y$:\n\n$$v = \n\\begin{pmatrix}\n2\\\\\n5\\\\\n\\end{pmatrix}.\n$$\n\nПредставим эту запись на графике:\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Вектор $v$\" src=\"https://yastatic.net/s3/education-portal/media/4_2_3_ca461dee8b.webp\"\u003e\n  \u003cfigcaption\u003e\n\nВектор $v$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**Примечание:** Вектор можно записывать как вектор-столбец или как вектор-строку. Разницы между этими объектами нет, это, скорее, вопрос удобного формализма. Для согласованности мы по умолчанию будем считать, что все векторы — это именно векторы-столбцы. Перейти от вектора-столбца к вектору-строке можно с помощью операции транспонирования, которую мы разберём чуть позже.\n\nДля разбора теоретических концептов нам удобнее смотреть на векторы именно с точки зрения алгебры, но в прикладных задачах мы чаще всего будем будем работать с векторами как упорядоченным набором чисел. Поэтому рассмотрим, как можно создавать векторы с использованием библиотеки `numpy`.\n\n{% cut \"**Представление векторов в Python**\" %}\n\nВ NumPy вектора представлены в виде массивов, позволяющих эффективно выполнять математические операции.\n\n```python\nimport numpy as np\n\n# Для создания вектора с заданными значениями\n# используется функция `np.array()`\nv = np.array([2, 5])  # array([2, 5])\n\n# Также можно создавать специальные векторы.\n# Например, вектор нулей заданного размера,\n# используя функцию `np.zeros()`\nzeros = np.zeros(3)  # array([0., 0., 0.])\n\n# Или вектор с равномерным шагом (аналог функции range),\n# используя функцию `np.arange()`\na = np.arange(0, 10, 2)  # array([0, 2, 4, 6, 8])\n```\n\nЕсть много и других полезных функций и методов в этой библиотеке — о них мы поговорим позже.\n\n{% endcut %}\n\nТеперь, для полного представления о векторном пространстве, давайте разберём операции, которые мы можем выполнять над векторами, и свойства, которым должны удовлетворять эти операции.\n\n### Операции с векторами\n\n**Сложение**\n\nДля векторов $u$ и $v$ сумма определяется как:\n\n$$u + v = \n\\begin{pmatrix}\nu_1 + v_1\\\\\nu_2 + v_2\\\\\n\\vdots\\\\\nu_n + v_n\\\\\n\\end{pmatrix}.\n$$\n\n**Свойства:**\n\n- Коммутативность: $u + v = v + u$.\n- Ассоциативность: $(u + v) + w = u + (v + w)$.\n- Существование нулевого вектора: $u + 0 = u$.\n- Существование противоположного вектора:  $u + (-u) = 0$.\n\nГде:\n\n$u = \n\\begin{pmatrix}\nu_1\\\\\nu_2\\\\\n\\vdots\\\\\nu_n\\\\\n\\end{pmatrix}\n,\\quad\nv = \n\\begin{pmatrix}\nv_1\\\\\nv_2\\\\\n\\vdots\\\\\nv_n\\\\\n\\end{pmatrix}\n,\\quad\nw = \n\\begin{pmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_n\\\\\n\\end{pmatrix}\n,\\quad\n0 = \n\\begin{pmatrix}\n0\\\\\n0\\\\\n\\vdots\\\\\n0\\\\\n\\end{pmatrix}.$\n\n**Умножение на скаляр**\n\nДля вектора $u$ умножение на число (скаляр)  $\\lambda$ определяется как:\n\n$$\\lambda u = \n\\begin{pmatrix}\n\\lambda u_1\\\\\n\\lambda u_2\\\\\n\\vdots\\\\\n\\lambda u_n\n\\end{pmatrix}.\n$$\n\n**Свойства умножения на скаляр**:\n\n- $(\\lambda + \\mu) u = \\lambda u + \\mu u$.\n- $\\lambda (u + v) = \\lambda u + \\lambda v$.\n- $(\\lambda \\mu) v = \\lambda (\\mu v)$.\n- $1 \\cdot v = v$.\n\nГде $u = \n\\begin{pmatrix}\nu_1\\\\\nu_2\\\\\n\\vdots\\\\\nu_n\\\\\n\\end{pmatrix}\n,\\quad\nv = \n\\begin{pmatrix}\nv_1\\\\\nv_2\\\\\n\\vdots\\\\\nv_n\\\\\n\\end{pmatrix}\n,\\quad\n\\lambda \\in \\mathbb{R}\n,\\quad\n\\mu \\in \\mathbb{R}$\n\nСвойства, приведённые выше для операций сложения и умножения, — ключевые в определении векторного пространства. Мы рассмотрели, как реализованы данные операции на векторах, которые отождествляются с $\\mathbb{R}^n$, так как в каком-то смысле это самые полезные и наглядные объекты, удовлетворяющие определению вектора.\n\nДалее мы будем рассматривать и другие объекты, которые также являются векторами, — многочлены, функции, матрицы, бесконечные последовательности. Важно, что все разумные свойства, к которым мы привыкли в $\\mathbb{R}^n$, будут выполняться и для этих объектов тоже. Поэтому интуитивно всегда можно ориентироваться на  $\\mathbb{R}^n$ как на главный пример.\n\nВекторы можно не только складывать и умножать на числа, но также, например, транспонировать или считать скалярное произведение между ними. Эти операции уже не участвуют в определении векторного пространства, однако могут быть очень полезны при практической работе с векторами.\n\n**Транспонирование**\n\nДля вектора $v$ операция транспонирования меняет форму записи: вектор представленный в виде столбца, преобразуется в вектор, представленный в виде строки, и наоборот.\n\n$$v = \n\\begin{pmatrix}\nv_1\\\\\nv_2\\\\\n\\vdots\\\\\nv_n\\\\\n\\end{pmatrix}\n,\\quad\nv^{t}= \n\\begin{pmatrix}\nv_1\u0026v_2\u0026\\ldots\u0026 v_n\\\\\n\\end{pmatrix}.\n$$\n\n**Стандартное скалярное произведение**\n\nДля векторов $u$ и $v$, заданных в координатах, скалярное произведение — это операция, которая отображает пару векторов в скаляр и определяется как:\n\n$$\\langle u, v \\rangle=\nu \\cdot v=\nu_{1}v_{1} + u_{2}v_{2} + \\ldots + u_{n}v_{n}= \n\\sum_{i=1}^n u_{i}v_{i}.\n$$\n\n**Свойства скалярного произведения**:\n\n- $\\langle u, v \\rangle= \\langle v, u \\rangle.$\n- $\\langle v, v \\rangle \\geq 0$ и $\\langle v, v \\rangle = 0 \\iff v=0.$\n- $\\langle \\lambda u, v \\rangle= \\lambda \\langle u, v \\rangle.$\n- $\\langle u + v, w \\rangle= \n  \\langle u, w \\rangle +\n  \\langle v, w \\rangle.$\n\nСкалярное произведение играет ключевую роль в машинном обучении, особенно в методах нахождения ближайших соседей и методах классификации, таких как метод опорных векторов (SVM).\n\nПокажем, как выглядит скалярное произведение, на примере.\n\nПредположим, что у нас есть два объекта с более сложными признаками:\n\n- Вектор признаков для объекта №1:\n\n$$u = \n\\begin{pmatrix} \n3\\\\\n-5\\\\\n2\\\\\n0\\\\ \n1\n\\end{pmatrix}\n$$\n\n- Вектор признаков для объекта №2:\n\n$$v = \n\\begin{pmatrix}\n-1 \\\\ \n4 \\\\ \n0 \\\\ \n2 \\\\ \n-3 \\\\\n\\end{pmatrix}\n$$\n\nСкалярное произведение этих двух векторов рассчитывается как:\n\n$\\langle u, v \\rangle = 3 \\cdot (-1) + (-5) \\cdot 4 + 2 \\cdot 0 + 0 \\cdot 2 + 1 \\cdot (-3) = -3 - 20 - 3 = -26.$\n\nЧто это нам говорит? Поскольку скалярное произведение дало отрицательное значение, значит, векторы направлены в противоположные стороны в многомерном пространстве. В контексте, например, метода опорных векторов это может означать, что объекты, представленные этими векторами, принадлежат к разным классам.\n\nЕсли бы скалярное произведение было положительным, это означало бы, что объекты, вероятно, принадлежат к одному классу (например, они оба «ближе» друг к другу в многомерном пространстве признаков).\n\nТакже со скалярным произведением тесно связаны следующие полезные определения:\n\n- `Длина (норма)` вектора $u$ может быть выражена как:\n\n  $$\\begin{Vmatrix}u\\end{Vmatrix}=\n  \\sqrt{\\langle u, u \\rangle}.\n  $$\n  \n  \n\n- `Косинусная мера` для векторов $u$ и $v$ — это нормированное скалярное произведение, которое показывает, насколько близки направления двух векторов независимо от их длин. Вычисляется как:\n\n  $$\\cos \\theta = \\frac{\\langle u, v \\rangle}{\\begin{Vmatrix}u\\end{Vmatrix}\\begin{Vmatrix}v\\end{Vmatrix}}.\n  $$\n  \n  - $\\cos \\theta=1$, если векторы направлены в одну сторону;\n  - $\\cos \\theta=0$, если векторы `ортогональны` (перпендикулярны);\n  - $\\cos \\theta=-1$, если векторы направлены в противоположные стороны;\n\nГде $u = \n\\begin{pmatrix}\nu_1\\\\\nu_2\\\\\n\\vdots\\\\\nu_n\\\\\n\\end{pmatrix}\n,\\quad\nv = \n\\begin{pmatrix}\nv_1\\\\\nv_2\\\\\n\\vdots\\\\\nv_n\\\\\n\\end{pmatrix}\n,\\quad\nw = \n\\begin{pmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_n\\\\\n\\end{pmatrix}\n,\\quad\n\\lambda \\in \\mathbb{R}$\n.\n\nКосинусная мера очень часто применяется в NLP-моделях для оценки близости слов. Сейчас покажем на примере.\n\nРанее мы упомянули, что в NLP-моделях слова (для простоты будем считать, что токен — это одно слово) кодируются векторами, которые отражают их семантические связи между собой.\n\nТакие векторы часто называют эмбеддингами (англ. word embeddings). Пусть у нас есть два векторных представления слов:\n\n- Эмбеддинг слова «гитара»: $v_{\\text{гитара}} = \\begin{pmatrix} 0.809 \\ 0.437 \\ 0.173 \\ 0.604 \\end{pmatrix}^{t}$.\n- Эмбеддинг слова «пианино»: $v_{\\text{пианино}} = \\begin{pmatrix} 0.370 \\ 0.198 \\ 0.100 \\ 0.844 \\end{pmatrix}^{t}$.\n\nМы можем вычислить косинусную меру между этими векторами с помощью следующей формулы:\n\n$$\\cos \\theta = \\frac{\\langle v_{\\text{гитара}}, v_{\\text{пианино}} \\rangle}{\\|v_{\\text{гитара}}\\| \\|v_{\\text{пианино}}\\|}.\n$$\n\nСначала находим скалярное произведение этих векторов:\n\n$$\\langle v_{\\text{гитара}}, v_{\\text{пианино}} \\rangle = 0.809 \\cdot 0.370 + 0.437 \\cdot 0.198 + 0.173 \\cdot 0.100 + 0.604 \\cdot 0.844=0.299+0.087+0.017+0.510=0.913.\n$$\n\nТеперь вычислим нормы этих векторов:\n\n$$\\|v_{\\text{гитара}}\\| = \\sqrt{0.809^2 + 0.437^2 + 0.173^2 + 0.604^2} = \\sqrt{0.654 + 0.191 + 0.030 + 0.365} = \\sqrt{1.240} \\approx 1.114. \n$$\n\n$$\\|v_{\\text{пианино}}\\| = \\sqrt{0.370^2 + 0.198^2 + 0.100^2 + 0.844^2} = \\sqrt{0.137 + 0.039 + 0.010 + 0.713} = \\sqrt{0.899} \\approx 0.948.\n$$\n\nТеперь вычислим косинусную меру:\n\n$$\\cos \\theta = \\frac{0.913}{1.114 \\cdot 0.948} \\approx \\frac{0.913}{1.056} \\approx 0.865.\n$$\n\nЭто значение довольно близко к 1, что означает, что слова «гитара» и «пианино» схожи в семантическом смысле. Это подтверждается тем, что они часто встречаются в похожих контекстах, что делает их векторные представления близкими.\n\nЕщё пример: в рекомендательных системах мы можем использовать косинусную меру для сравнения пользователей или товаров. Например, если два пользователя имеют похожие предпочтения, векторные представления таких пользователей будут схожи, и это отразится в высоком значении косинусной меры. Это поможет системе рекомендовать товары или фильмы, которые понравятся похожим пользователям.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Векторные представления пользователей\" src=\"https://yastatic.net/s3/education-portal/media/4_2_4_Primer_kosinusnogo_rasstoyaniya_v_rekomendatelnyh_sistemah_f03727667a.webp\"\u003e\n  \u003cfigcaption\u003e\n\nВекторные представления пользователей\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nДавайте рассмотрим применение операций с векторами в библиотеке `numpy`, которые там представлены в виде векторизованных массивов. Это ускоряет вычисления и позволяет эффективно работать с многомерными данными.\n\n{% cut \"**Операции с векторами в Python**\" %}\n\nРазмер векторов может быть различен в зависимости от используемой модели. Например:\n\n- BERT — 768.\n- DeepSeek-V3 — 7 168.\n- ChatGPT-3 — 12 288.\n- Llama 3 — 16 384.\n\nДавайте разберём несколько наглядных примеров с эмбеддингами намного меньшего размера, равного 5. Данные примеры также будут отражать суть семантической близости. Для этого будем выполнять операции сложения и вычитания векторов и оценивать косинусное сходство результирующего вектора.\n\n```python\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -\u003e float:\n    \"\"\"Вычисляет косинусную меру двух векторов.\"\"\"\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\ndef evaulate_result(embeddings: np.array, result_embedding: np.array) -\u003e None:\n    \"\"\"Вычисляет косинусное сходство результата с каждым словом в словаре.\"\"\"\n    print(\"Косинусное сходство результата с каждым словом:\")\n    for word, vec in embeddings.items():\n\t\t    print(f\"{word}: {cosine_similarity(result_embedding, vec):.3f}\")\n\n# Пример 1: ebook ≈ book - pages + digital\nembeddings = {\n    \"book\":     np.array([0.5, 0.7, 0.6, 0.4, 0.3]),\n    \"pages\":    np.array([0.2, 0.3, 0.1, 0.0, 0.2]),\n    \"digital\":  np.array([0.1, 0.2, 0.4, 0.3, 0.4]),\n    \"ebook\":    np.array([0.3, 0.6, 1.0, 0.7, 0.5]),\n    \"magazine\": np.array([0.3, 0.5, 0.5, 0.2, 0.1])\n}\n\nresult = embeddings[\"book\"] - embeddings[\"pages\"] + embeddings[\"digital\"]\nevaulate_result(embeddings, result)\n\n# Output:\n# Косинусное сходство результата с каждым словом:\n# book: 0.951\n# pages: 0.737\n# digital: 0.953\n# ebook: 0.996\n# magazine: 0.921\n```\n\nВ результате получаем, что $ebook \\approx book - pages + digital$.\n\nРассмотрим ещё один пример:\n\n```python\n# Пример 2: hybrid ≈ motor - gasoline + electricity\n\nembeddings = {\n    \"motor\":       np.array([0.6, 0.7, 0.4, 0.2, 0.3]),\n    \"gasoline\":    np.array([0.2, 0.3, 0.1, 0.0, 0.1]),\n    \"electricity\": np.array([0.4, 0.5, 0.3, 0.4, 0.3]),\n    \"hybrid\":      np.array([0.8, 0.9, 0.7, 0.6, 0.5]),\n    \"battery\":     np.array([0.1, 0.4, 0.2, 0.5, 0.2])\n}\n\nresult = embeddings[\"motor\"] - embeddings[\"gasoline\"] + embeddings[\"electricity\"]\nevaulate_result(embeddings, result)\n\n# Output:\n# Косинусное сходство результата с каждым словом:\n# motor: 0.975\n# gasoline: 0.896\n# electricity: 0.995\n# hybrid: 0.998\n# battery: 0.873\n\n```\n\nВ результате получаем, что $hybrid \\approx motor - gasoline + electricity$.\n\nИ, наконец, последний пример:\n\n```python\n# Пример 3:  profit ≈ investment - risk + opportunity\n\nembeddings = {\n    \"investment\":   np.array([0.7, 0.8, 0.6, 0.7, 0.5]),\n    \"risk\":         np.array([0.3, 0.4, 0.2, 0.2, 0.3]),\n    \"opportunity\":  np.array([0.5, 0.6, 0.4, 0.5, 0.6]),\n    \"profit\":       np.array([0.9, 0.9 , 0.8, 1. , 0.8]),\n    \"loss\":         np.array([-0.6, -0.7, -0.5, -0.6, -0.4])\n}\n\nresult = embeddings[\"investment\"] - embeddings[\"risk\"] + embeddings[\"opportunity\"]\nevaulate_result(embeddings, result)\n\n# Output:\n# Косинусное сходство результата с каждым словом:\n# investment: 0.997\n# risk: 0.969\n# opportunity: 0.989\n# profit: 0.999\n# loss: -0.995\n\n```\n\nВ результате получаем, что $profit \\approx investment - risk + opportunity$ .\n\nСуществуют и другие интересные применения векторных операций над эмбеддингами. То есть кодировать векторами можно не только тексты, но и другую информацию. Так, в компьютерном зрении используют векторы изображений (англ. image embeddings), полученные, например, из промежуточных слоёв свёрточных нейронных сетей, чтобы описывать изображения для классификации, поиска или кластеризации.\n\nИзучив векторы и базовые операции над ними, мы можем исследовать их взаимосвязи через понятие линейной зависимости. Оно поможет нам определить, насколько один набор векторов достаточен для описания всего пространства.\n\n{% endcut %}\n\n### Линейная зависимость\n\nПусть $V$ — векторное пространство над $\\mathbb R$. Тогда мы можем взять некоторые элементы этого пространства (векторы) $v_{1}, \\ldots, v_{n} \\in V$и применить к ним операции сложения и умножения, описанные выше. В результате получим новый вектор, который называется `линейной комбинацией`:\n\n$$ r_{1}v_{1} + \\ldots + r_{n}v_{n} \\in V, \\text{где } r_{i} \\in \\mathbb R.\n$$\n\nЛинейная комбинация называется `тривиальной`, если все $r_{i} = 0$, в противном случае она `нетривиальная`.\n\n- Векторы $v_{1}, \\ldots, v_{n} \\in V$ называются линейно зависимыми, если существует **нетривиальная** линейная комбинация этих векторов, равная нулевому вектору. То есть  $v_{1}, \\ldots, v_{n}$ — `линейно зависимы`, если существуют $r_{1}, \\ldots, r_{n} \\in \\mathbb R$, такие, что хотя бы один из $r_{i}$ не равен нулю и $r_{1}v_{1} + \\ldots + r_{n}v_{n} = 0$.\n- Если никакая нетривиальная комбинация не может дать ноль, то векторы `линейно независимы`.\n\n**Примеры**\n\n1. Рассмотрим $V = \\mathbb{R}^3$, и пусть:\n\n   $$v_1 = \n   \\begin{pmatrix} \n   1 \\\\ \n   0 \\\\ \n   0 \n   \\end{pmatrix}\n   , \\quad\n   v_2 = \n   \\begin{pmatrix}\n   0 \\\\ \n   1 \\\\ \n   0 \n   \\end{pmatrix}\n   , \\quad\n   v_3 = \n   \\begin{pmatrix} \n   0 \\\\\n   0 \\\\\n   1 \n   \\end{pmatrix}\n   , \\quad\n   v_4 =\n   \\begin{pmatrix} \n   1 \\\\ \n   1 \\\\ \n   1 \n   \\end{pmatrix}\n   , \\quad\n   v_5 = \n   \\begin{pmatrix} \n   1 \\\\ \n   1 \\\\ \n   0 \n   \\end{pmatrix}\n   $$\n   \n   Тогда векторы:\n\n   - $v_1, v_2, v_3$ — линейно независимы;\n   - $v_1, v_2, v_4$ — линейно независимы;\n   - $v_1, v_2, v_3, v_4$ — линейно зависимы ($v_1 + v_2 + v_3 - v_4 = 0$);\n   - $v_1, v_2, v_5$ — линейно зависимы ($v_1 + v_2 - v_5 = 0$).\n\n2. Один вектор $v \\in V$линейно зависим тогда и только тогда, когда он равен нулю.\n\n3. Два вектора $u, v \\in V$линейно зависимы тогда и только тогда, когда они пропорциональны, то есть $u = \\lambda v$, где $\\lambda \\in \\mathbb R$.\n\nС точки зрения анализа данных, проверить линейную зависимость признаков в датасете — значит понять, нет ли там избыточной информации (коллинеарности). Если один признак почти пропорционален другому, это может вызывать проблемы в обучении регрессий и нейронных сетей, а также приводить к переобучению. Как вы думаете почему?\n\n{% cut \"Ответ (не открывайте сразу, сначала подумайте сами!)\" %}\n\n**Нестабильность оценок коэффициентов.** В линейной регрессии матрица признаков становится почти вырождённой, и коэффициенты модели могут приобретать очень большие значения, сильно реагировать на малейшие изменения в данных, что снижает их интерпретируемость и надёжность.\n**Переобучение.** Избыточная информация в виде повторяющихся (почти одинаковых) признаков увеличивает сложность модели без добавления новых сведений. Модель может начать подгонять шум в тренировочных данных, что приводит к ухудшению обобщающей способности на новых данных.\n\n{% endcut %}\n\nПосле того как мы подробно рассмотрели векторы, освоили базовые операции над ними и исследовали понятие линейной зависимости, следующим логическим шагом будет ввести понятие базиса.\n\nБазис позволяет описывать любое векторное пространство с помощью минимального набора линейно независимых векторов, обеспечивая таким образом удобные способы представления данных и решений в выбранном пространстве.\n\n### Базис\n\n`Базис` векторного пространства $V$ — это система линейно независимых векторов $\\{v_1, v_2, \\dots, v_n\\}$, которые порождают всё пространство $V$. Это означает, что любой вектор $v \\in V$ можно представить в виде линейной комбинации базисных векторов:\n\n$$v = \\lambda_1 v_1 + \\lambda_2 v_2 + \\dots + \\lambda_n v_n,\n$$\n\nгде $\\lambda_i \\in \\mathbb R$ — координаты вектора $v$ в выбранном базисе.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.2.5: Канонический базис\" src=\"https://yastatic.net/s3/education-portal/media/4_2_5_Bazis_fokus_na_j_so_strelkoj_i_i_so_strelkoj_9d40ff07aa.webp\"\u003e\n  \u003cfigcaption\u003e\n\nКанонический базис в $\\mathbb R^2$\n\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nСвойства базиса:\n\n- Всякое векторное пространство $V$обладает базисом.\n- Любой вектор в $V$ единственным образом представляется через базис.\n- Любые два базиса $V$содержат одно и то же число векторов.\n\nПримеры:\n\n- В $\\mathbb R^2$: стандартный базис\n\n$$v_1 = \n\\begin{pmatrix} \n1 \\\\ \n0\n\\end{pmatrix}\n, \\quad\nv_2 = \n\\begin{pmatrix}\n0 \\\\ \n1\n\\end{pmatrix}.\n$$\n\n- В $\\mathbb R^3$: стандартный базис\n\n$$v_1 = \n\\begin{pmatrix} \n1 \\\\ \n0 \\\\\n0\n\\end{pmatrix}\n, \\quad\nv_2 = \n\\begin{pmatrix}\n0 \\\\ \n1 \\\\\n0\n\\end{pmatrix}\n, \\quad\nv_3 = \n\\begin{pmatrix}\n0 \\\\ \n0 \\\\\n1\n\\end{pmatrix}.\n$$\n\n\u003e Смысл базиса заключается в том, что после его выбора можно отождествить $V$ с $\\mathbb R^n$. То есть, если $v = \\lambda_1 v_1 + \\lambda_2 v_2 + \\dots + \\lambda_n v_n$ , тогда ему соответствует единственный столбец:\n\u003e\n\u003e $$\\begin{pmatrix}\n\u003e \\lambda_1\\\\\n\u003e \\lambda_2\\\\\n\u003e \\vdots\\\\\n\u003e \\lambda_n\\\\\n\u003e \\end{pmatrix}\n\u003e $$\n\u003e \n\u003e \n\nТаким образом, про любое векторное пространство, когда это необходимо, можно думать как просто про $\\mathbb R^n$.\n\nВыбор базиса очень важен при работе с данными — например, методы снижения размерности (PCA и другие) фактически ищут «хороший» базис, в котором данные выглядят проще (часто это значит, что большинство координат становятся близкими к нулю).\n\nПоследнее, о чём поговорим в этом параграфе, — как раз о размерности.\n\n### Размерность\n\n`Размерность` векторного пространства $V$ — это число векторов в базисе этого пространства. Обозначается как $\\operatorname{dim}(V)$. Размерность показывает минимальное число векторов, достаточное для порождения пространства. Говоря по-простому, $\\operatorname{dim}(V)$ — это величина, показывающая, насколько векторное пространство большое, она характеризует «количество степеней свободы» в пространстве.\n\n**Примеры:**\n\n- В $\\mathbb R^n$: размерность равна $n$.\n- В пространстве всех многочленов степени $\\leq n$: размерность равна $n+1$ (по числу коэффициентов многочлена).\n\n\u003e В контексте анализа данных размерность отражает, сколько координат нужно, чтобы описать объекты. А если признак вносит мало информации, иногда его можно исключить или заменить, уменьшая размерность и сохраняя достаточную полноту описания.\n\n***\n\nИтак, векторы — это не просто набор чисел, а мощное средство для описания и преобразования данных. Именно через векторы мы представляем наборы признаков, вычисляем расстояния и ищем оптимальные решения.\n\nВ следующих параграфах мы углубимся в тему линейных преобразований и посмотрим, как матричная форма записей упрощает и ускоряет многие вычислительные задачи. Это позволит нам перейти к рассмотрению методов решения систем линейных уравнений — ключевого инструмента в моделировании и оптимизации.\n\u003c/br\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13814388.9e5feae780b1cace0477153bc60aaecc2682c9f1?iframe=1\" frameborder=\"0\" name=\"ya-form-13814388.9e5feae780b1cace0477153bc60aaecc2682c9f1\" width=\"650\"\u003e\u003c/iframe\u003e"])</script><script nonce="">self.__next_f.push([1,"65:Tcf46,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы вспомним матрицы и увидим, почему они — основа для большинства вычислительных операций в анализе данных и машинном обучении.\n\nВ работе с данными строки матрицы часто соответствуют объектам, а столбцы — признакам. В машинном обучении операции с матрицами лежат в основе линейных моделей, нейронных сетей и преобразований признаков.\n\nМы рассмотрим основные типы матриц, повторим ключевые операции, такие как сложение, умножение и транспонирование, и разберём их практическое применение — от загрузки датасетов с помощью `pandas` до реализации матричных операций в `numpy`.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.1: Представление исходных данных в виде матрицы признаков и вектора целевой переменной\" src=\"https://yastatic.net/s3/education-portal/media/Novoe_v_glavu_4_3_a0cd58a3c6.webp\"\u003e\n  \u003cfigcaption\u003e\n  Представление исходных данных в виде матрицы признаков и вектора целевой переменной\n     \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n### Определение\n\nМатрицей размера $m \\times n$ называется прямоугольная таблица из элементов, у которой $m$ строк и $n$ столбцов. Элементы матрицы обозначаются одной и той же буквой с двумя индексами, первый из которых — номер строки, а второй — номер столбца.\n\nНапример, элементы матрицы $A$ обычно обозначают как $a_{ij}$, где $i$ — номер строки, а $j$ — номер столбца.\n\n$$A= \n\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n,\\quad \\text{где } \na_{ij} \\in \\mathbb R.\n$$\n\nПримеры интерпретации матриц:\n\n- **Табличные данные.** B машинном обучении, когда у нас есть $m$ объектов и $n$ признаков, мы храним данные как матрицу $(m \\times n)$.\n\n- **Изображения.** Каждый пиксель черно-белого изображения представляет собой число, которое отражает степень насыщенности цвета. Тогда изображение можно представить в виде сетки из пикселей — матрицы чисел.\n  \n  \u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.2\" src=\"https://yastatic.net/s3/education-portal/media/4_3_1_4db69e2e91.webp\"\u003e\n  \u003cfigcaption\u003e\n    Для наглядности матрицы яркостей исходное изображение было уменьшено до размера $28 х 28$\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n### Типы матриц\n\nМножество всех матриц с $m$ строками и $n$ столбцами будем обозначать через $\\operatorname{M}_{m \\thinspace n} (\\mathbb R)$.\n\n1. **Прямоугольная матрица.** Матрица $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$, у которой число строк не равно числу столбцов ($m \\neq n$). Пример:\n\n$$A=\n\\begin{pmatrix}\n1 \u0026 2 \u0026 3\\\\ \n4 \u0026 5 \u0026 6\n\\end{pmatrix}.\n$$\n\n2. **Квадратная матрица.** Матрица $B \\in \\operatorname{M}_{n} (\\mathbb R)$, у которой число строк равно числу столбцов ($m = n$). Пример:\n\n$$B=\n\\begin{pmatrix}\n1 \u0026 2 \\\\ \n3 \u0026 4 \n\\end{pmatrix}.\n$$\n\n3. **Диагональная матрица.** Квадратная матрица, у которой все элементы вне главной диагонали равны нулю. `Главной диагональю` называют мысленную линию, которую можно провести через элементы $a_{ij}$, где $i=j$. В примере ниже такая линия пройдёт через элементы $1, 2, 3.$\n   Пример:\n\n$$D=\n\\begin{pmatrix}\n1 \u0026 0 \u0026 0\\\\\n0 \u0026 2 \u0026 0\\\\\n0 \u0026 0 \u0026 3\n\\end{pmatrix}.\n$$\n\n4. **Верхнетреугольная матрица.** Квадратная матрица, у которой все элементы **ниже** главной диагонали равны нулю, то есть $a_{ij} = 0$  при $i \u003e j$. А если все элементы **выше** главной диагонали равны нулю ($i \u003c j$), то такая матрица называется `нижнетреугольной`.\n   Пример:\n\n$$U=\n\\begin{pmatrix}\n1 \u0026 2 \u0026 0\\\\\n0 \u0026 5 \u0026 6\\\\\n0 \u0026 0 \u0026 9\n\\end{pmatrix}\n, \\quad\nL=\n\\begin{pmatrix}\n1 \u0026 0 \u0026 0\\\\\n0 \u0026 5 \u0026 0\\\\\n7 \u0026 8 \u0026 9\n\\end{pmatrix}.\n$$\n\n5. **Единичная матрица.** Диагональная матрица, у которой все элементы главной диагонали равны 1. Часто обозначается как $E$ или $I$. Пример:\n\n$$E=\n\\begin{pmatrix}\n1 \u0026 0\\\\\n0 \u0026 1\n\\end{pmatrix}.\n$$\n\nПрежде чем перейти к рассказу о свойствах матриц, давайте ненадолго остановимся и поговорим, как создавать матрицы в Python — и с помощью каких инструментов.\n\nДля работы с матрицами чаще всего используются библиотеки `numpy`, `pandas` и `scipy`. Выбор в пользу одной из них зависит от типа решаемой задачи. Но они в некотором смысле совместимы друг с другом, что позволяет использовать их одновременно.\n\nДля удобства мы сравнили библиотеки в табличке:\n\n#|\n||\n\n**Название**\n\n|\n\n**Основная** **цель**\n\n|\n\n**Применение**\n\n|\n\n**Матрицы**\n\n|\n\n**Преимущества**\n\n||\n||\n\nNumPy\n\n|\n\nРабота с n-мерными массивами и выполнение быстрых векторизованных операций.\n\n|\n\nБазовая матричная математика, численные расчёты, подготовка данных для более высокоуровневых библиотек.\n\n|\n\nПредоставляет объект `ndarray`, который можно использовать как матрицу (двумерный массив) и над которым можно выполнять арифметические и линейно-алгебраические операции.\n\n|\n\nВысокая производительность, обширный набор функций для базовой линейной алгебры, статистики, генерации случайных чисел и пр.\n\n||\n||\n\nPandas\n\n|\n\nРабота с табличными данными (например, данными из CSV, баз данных) с богатым набором средств для фильтрации, агрегации и анализа.\n\n|\n\nАнализ и манипуляция данными, где важна структурированность и метаданные, а не только чистые числовые операции.\n\n|\n\nСтруктуры `DataFrame` и `Series` можно рассматривать как матрицы с индексами и метками столбцов, что упрощает обработку и анализ данных.\n\n|\n\nУдобная индексация, возможность работы с разными типами данных в одном объекте, интеграция с другими источниками данных и мощные инструменты для группировки и агрегации.\n\n||\n||\n\nSciPy\n\n|\n\nПредоставление расширенного набора алгоритмов для научных и инженерных вычислений.\n\n|\n\nРешение сложных задач в области физики, инженерии, статистики и других областях, требующих специализированных методов обработки матриц и численных расчётов.\n\n|\n\nСтроится поверх NumPy, дополняя его более специализированными модулями, такими как `scipy.linalg` (расширенные методы линейной алгебры), `scipy.sparse` (работа с разрежёнными матрицами), а также функциями для оптимизации, интегрирования, интерполяции и др.\n\n|\n\nШирокий спектр готовых к использованию алгоритмов, оптимизированных для научных задач, которые часто выходят за рамки базовых операций NumPy.\n\n||\n|#\n\nЧтобы упростить вам задачу выбора, вот короткое и ёмкое саммари, когда и какую библиотеку применять:\n\n- Если нужно работать с массивами и выполнять базовые матричные операции — `numpy`.\n- Если задача — анализ табличных данных с метками и сложной агрегацией, то `pandas`.\n- Если требуются более продвинутые методы обработки матриц, специализированные алгоритмы и работа с разрежёнными матрицами — `scipy`.\n\nДля начала рассмотрим, как выглядят матрицы в каждой из этих библиотек.\n\n**Матрицы в NumPy**\n\n```python\nimport numpy as np\n\n# Для создания матрицы с заданными значениями\n# используется функция `np.array()`, как и для векторов,\n# но, в отличие от векторов, матрицы могут быть многомерными\n\nA = np.array([[1, 2], [3, 4], [5, 6]])  # матрица размера 3x2\nprint(A)\n\n# Output:\n# [[1 2]\n# [3 4]\n# [5 6]]\n```\n\nТакже можно создавать специальные матрицы. Например, матрицу нулей заданного размера, используя функцию `np.zeros()` по аналогии с векторами (об этом мы говорили в предыдущем параграфе):\n\n```python\nO = np.zeros(shape=(3, 2))\nprint(O)\n\n# Output:\n# [[0. 0.]\n# [0. 0.]\n# [0. 0.]]\n```\n\nАналогично для матрицы единиц:\n\n```python\nE = np.ones(shape=(3, 2))\nprint(E)\n\n# Output:\n# [[1. 1.]\n# [1. 1.]\n# [1. 1.]]\n```\n\nИ для случайной матрицы:\n\n```python\nrng = np.random.default_rng()\nR = rng.random(shape=(3, 2))\nprint(R)\n\n# Output:\n# [[0.47821453 0.28421059]\n# [0.2178972  0.07891322]\n# [0.63259903 0.4502506 ]]\n```\n\nТакже полезно знать, как индексируются матрицы:\n\n```python\nprint(A[0, 1])\nprint(A[1:3])\nprint(A[0:2, 0])\n\n# Output:\n# 2\n# array([[3, 4],\n#       [5, 6]])\n# array([1, 3])\n```\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.3\" src=\"https://yastatic.net/s3/education-portal/media/4_3_2_Indeksirovanie_matricz_2dcb03317e.webp\"\u003e\n  \u003cfigcaption\u003e\n  Индексирование элементов матрицы A в NumPy\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТеперь рассмотрим, как создавать матрицы в `pandas`.\n\n**Матрицы в Pandas**\n\n```python\nimport pandas as pd\n\n# Создаём матрицу в виде списка списков\nA = [\n    [1, 2],\n    [3, 4],\n    [5, 6]\n]\n\n# Преобразуем список списков в DataFrame и задаём имена столбцов\ndf = pd.DataFrame(A, columns=[\"Column1\", \"Column2\"])\n\n# Выводим DataFrame (матрицу)\nprint(df)\n\n# Output:\n#     Column1  Column2\n#  0        1        2\n#  1        3        4\n#  2        5        6\n```\n\nНа первый взгляд кажется, что больших отличий с `numpy` нет. Но использовать `pandas` намного удобнее, когда мы работаем с табличными данными, так как мы можем более гибко манипулировать ими.\n\n```python\nimport pandas as pd\n\n# Группировка и агрегация данных по категориальному признаку\n# Допустим, у нас есть данные о зарплатах по отделам\ndata_group = {\n    'Отдел': ['HR', 'IT', 'HR', 'IT', 'HR'],\n    'Зарплата': [40000, 70000, 45000, 72000, 42000]\n}\ndf_group = pd.DataFrame(data_group)\n# Группируем по столбцу 'Отдел' и \n# вычисляем среднюю зарплату в каждом отделе\ngrouped = df_group.groupby('Отдел').mean()\nprint(\"Средняя зарплата по отделам:\")\nprint(grouped)\n\n# Output:\n# Средняя зарплата по отделам:\n#            Зарплата\n# Отдел              \n# HR     42333.333333\n# IT     71000.000000\n```\n\nБиблиотека `pandas` — очень мощный инструмент, используемый в анализе данных, и в рамках линейной алгебры мы будем иногда обращаться к нему. Теперь рассмотрим библиотеку `scipy`.\n\n**Матрицы в SciPy**\n\nВыше мы выделили пять типов матриц, но на практике часто встречается ещё один тип матриц — разрежённые.\n\n`Разрежённая матрица` — это матрица, в которой большая часть элементов равна нулю. В таких случаях хранение и обработка исходной матрицы может быть неэффективной по памяти и времени. Для разрежённых матриц используются специальные форматы хранения, которые сохраняют только ненулевые элементы и их индексы.\n\nБиблиотека `scipy` как раз содержит методы для хранения таких матриц и работы с ними. Далее мы ещё более плотно поработаем с такими матрицами, а сейчас лишь поймём, как их можно задавать. Создадим разрежённую матрицу в формате `COO (Coordinate format)`, он позволяет задавать ненулевые элементы матрицы через координаты (индексы строк и столбцов) и значения.\n\n```python\nimport numpy as np\nfrom scipy.sparse import coo_matrix\n\n# Определяем ненулевые элементы: их значения и позиции (строки и столбцы)\nrows = np.array([0, 1, 2, 0])\ncols = np.array([1, 2, 0, 2])\ndata = np.array([10, 20, 30, 40])\n\n# Создаём матрицу размером 3x3\nsparse_coo = coo_matrix((data, (rows, cols)), shape=(3, 3))\n\nprint(\"Исходная матрица:\")\nprint(sparse_coo.toarray())\n\nprint(\"\\nРазрежённая матрица в формате COO:\")\nprint(sparse_coo)\n\n# Output:\n# Исходная матрица:\n# [[ 0 10 40]\n#  [ 0  0 20]\n#  [30  0  0]]\n \n# Разрежённая матрица в формате COO:\n#   (0, 1)\t10\n#   (1, 2)\t20\n#   (2, 0)\t30\n#   (0, 2)\t40\n```\n\nВажно заметить, что NumPy, Pandas и SciPy — это взаимодополняющие инструменты, где объект [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) служит общей основой.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import coo_matrix\n\n# Шаг 1: Создаём базовый NumPy array\ndata = np.array([[0, 0, 3],\n                 [4, 0, 0],\n                 [0, 5, 6]])\nprint(\"Исходный NumPy array:\")\nprint(data)\n\n# Шаг 2: Преобразуем NumPy array в разрежённую матрицу в формате COO\nsparse_matrix = coo_matrix(data)\nprint(\"\\nРазрежённая матрица (COO формат):\")\nprint(sparse_matrix)\n\n# Шаг 3: Преобразуем разрежённую матрицу обратно в плотный NumPy array\ndense_array = sparse_matrix.toarray()\nprint(\"\\nПлотный NumPy array, полученный из разрежённой матрицы:\")\nprint(dense_array)\n\n# Шаг 4: Создаём DataFrame Pandas на основе плотного массива\ndf = pd.DataFrame(dense_array, columns=['Column1', 'Column2', 'Column3'])\nprint(\"\\nDataFrame (Pandas):\")\nprint(df)\n\n# Output:\n# Исходный NumPy array:\n# [[0 0 3]\n#  [4 0 0]\n#  [0 5 6]]\n\n# Разрежённая матрица (COO формат):\n#   (0, 2)\t3\n#   (1, 0)\t4\n#   (2, 1)\t5\n#   (2, 2)\t6\n\n# Плотный NumPy array, полученный из разрежённой матрицы:\n# [[0 0 3]\n#  [4 0 0]\n#  [0 5 6]]\n\n# DataFrame (Pandas):\n#    Column1  Column2  Column3\n# 0        0        0        3\n# 1        4        0        0\n# 2        0        5        6\n```\n\nВ задачах линейной алгебры использование библиотеки NumPy выглядит более удобным и эффективным, так как данная библиотека предоставляет оптимизированные функции для работы с многомерными массивами. В то время как Pandas и SciPy ориентированы на структурированный анализ данных и специализированные численные методы соответственно.\n\nТакже полезно понимать, что в NumPy можно работать не только с векторами и матрицами, но и с объектами произвольного размера, которые называются `тензорами`.\n\n`Тензор` — это математический объект, который обобщает понятия скаляров, векторов и матриц. Можно представить его как многомерный массив чисел, где:\n\n- тензор нулевого порядка — это скаляр (одно число);\n- тензор первого порядка — это вектор (одномерный массив);\n- тензор второго порядка — это матрица (двумерный массив);\n- тензоры более высокого порядка — это массивы с тремя и более индексами.\n\nОтлично, с этим разобрались. Теперь поговорим про операции над матрицами.\n\n### Операции над матрицами\n\n**Сложение**\n\nПусть $A, B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$, тогда сумма $(A + B) \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ определяется поэлементно, то есть $A + B = C$, где $c_{ij} = a_{ij} + b_{ij}$, или\n\n$$\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\nb_{11}\u0026\\ldots\u0026 b_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\nb_{m1}\u0026 \\ldots \u0026b_{mn}\n\\end{pmatrix}\n= $$\n\n$$ = \\begin{pmatrix}\na_{11}+b_{11}\u0026\\ldots\u0026 a_{1n} + b_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}+b_{m1}\u0026 \\ldots \u0026a_{mn} + b_{mn}\n\\end{pmatrix}\n$$\n\nПример:\n\n$$\\begin{pmatrix}\n1 \u0026 2\\\\ \n3 \u0026 4 \n\\end{pmatrix} \n+ \n\\begin{pmatrix} \n5 \u0026 6\\\\ \n7 \u0026 8 \n\\end{pmatrix} \n= \n\\begin{pmatrix} \n6 \u0026 8\\\\ \n10 \u0026 12 \n\\end{pmatrix}.\n$$\n\n\u003e **Важно:** складывать можно только матрицы одинакового размера. При этом на практике можно выполнять арифметические операции над объектами уже разного размера, автоматически «расширяя» меньший объект так, чтобы его размеры стали совместимы с размерами большего объекта.\n\u003e То есть мы «выравниваем» размеры матриц, чтобы они по факту стали одинакового размера. В NumPy данный механизм называется `broadcasting`.\n\n**Умножение на скаляр**\n\nПусть $\\lambda \\in \\mathbb{R}$ и $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$, тогда произведение $\\lambda A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ определяется поэлементно, то есть $\\lambda A = C$, где $c_{ij} = \\lambda a_{ij}$, или\n\n$$\\lambda\n\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\lambda a_{11}\u0026\\ldots\u0026 \\lambda a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\n\\lambda a_{m1}\u0026 \\ldots \u0026\\lambda a_{mn}\n\\end{pmatrix}\n$$\n\nПример:\n\n$$3 \n\\cdot \n\\begin{pmatrix} \n1 \u0026 2\\\\\n3 \u0026 4 \n\\end{pmatrix} \n= \n\\begin{pmatrix}\n3 \u0026 6\\\\\n9 \u0026 12 \n\\end{pmatrix}.\n$$\n\n**Умножение**\n\nПусть $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$, тогда произведение $AB \\in \\operatorname{M}_{m \\thinspace k} (\\mathbb R)$ определяется как $AB = C$, где $c_{ij} = \\sum_{t=1}^n a_{it}b_{tj}$, или\n\n$$\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{11}\u0026\\ldots\u0026 b_{1k}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\nb_{n1}\u0026 \\ldots \u0026b_{nk}\n\\end{pmatrix}\n= $$\n\n$$ = \\begin{pmatrix}\n\\sum_{t=1}^n a_{1t}b_{t1}\u0026\\ldots\u0026 \\sum_{t=1}^n a_{1t}b_{tk}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\n\\sum_{t=1}^n a_{mt}b_{t1}\u0026\\ldots\u0026\n\\sum_{t=1}^n a_{mt}b_{tk}\n\\end{pmatrix}.\n$$\n\nПроще говоря, чтобы получить значение $c_{ij}$, нужно из матрицы $A$ взять $i$-ю строку, а из матрицы $B$ — $j$-й столбец и скалярно их перемножить.\n\n\u003e **Важно:** произведение двух матриц определено только тогда, когда их размеры согласованы, а именно когда число столбцов первой матрицы равно числу строк второй.\n\nПример:\n\n$$A= \n\\begin{pmatrix} \n1 \u0026 -1 \u0026 0 \\\\ \n0 \u0026 2 \u0026 -1 \n\\end{pmatrix}\n, \\quad \nB=\n\\begin{pmatrix} \n2 \u0026 -1 \\\\ \n-1 \u0026 1 \\\\ \n0 \u0026 -2 \n\\end{pmatrix}\n\\\\\nC=AB=\n\\begin{pmatrix}\n3 \u0026 -2 \\\\\n-2 \u0026 4\n\\end{pmatrix}.\n$$\n\n{% cut \"**Вычисление по шагам**\" %}\n\n1. Вычислим $c_{11}$:\n\n$$c_{11} = (1 \\cdot 2) + (-1 \\cdot -1) + (0 \\cdot 0) = 2 + 1 + 0 = 3\n$$\n\n2. Вычислим $c_{12}$:\n\n$$c_{12} = (1 \\cdot -1) + (-1 \\cdot 1) + (0 \\cdot -2) = -1 - 1 + 0 = -2\n$$\n\n3. Вычислим $c_{21}$:\n\n$$c_{21} = (0 \\cdot 2) + (2 \\cdot -1) + (-1 \\cdot 0) = 0 - 2 + 0 = -2\n$$\n\n4. Вычислим $c_{22}$:\n\n$$c_{22} = (0 \\cdot -1) + (2 \\cdot 1) + (-1 \\cdot -2) = 0 + 2 + 2 = 4\n$$\n\n{% endcut %}\n\nВ нейронных сетях, особенно в полносвязных (fully-connected), каждый слой можно рассматривать как матричное преобразование.\n\nНапример, если на вход слоя приходит вектор $\\mathbf{h}^{(l-1)}$ размерности $n$, он умножается на матрицу весов $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{m \\times n}$ и складывается со смещением (bias) $\\mathbf{b}^{(l)} \\in \\mathbb{R}^{m}$, в результате чего получается новый вектор $\\mathbf{h}^{(l)}$размерности $m$.\n\nСхематично:\n\n$$\\mathbf{h}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}.\n$$\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.4\" src=\"https://yastatic.net/s3/education-portal/media/4_3_3_Umnozhenie_matricz_v_nejronnyh_setyah_3c1f212d81.webp\"\u003e\n  \u003cfigcaption\u003e\n  Матричные операции в нейронных сетях\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТаким образом, в нейронных сетях матричные операции играют ключевую роль при обработке данных, где веса и смещения (bias) обновляются в процессе обучения, что позволяет сети обучаться на данных.\n\nВ процессе обучения для эффективного вычисления и обновления параметров используются специализированные алгоритмы и библиотеки глубокого обучения (например, `PyTorch`, `TensorFlow`), которые могут распараллеливать вычисления на графических процессорах (GPU), значительно ускоряя обработку матриц.\n\n**Интересные особенности матричного умножения**\n\nПосле освоения базовых операций над матрицами становится ясно, что их умножение обладает рядом уникальных свойств, отличающих его от обычного умножения чисел. Давайте рассмотрим эти свойства.\n\n1. Умножение матриц некоммутативно.\n\n   1. Пусть $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$, при этом $m \\neq k$. Тогда определено произведение $AB \\in \\operatorname{M}_{m \\thinspace k} (\\mathbb R)$, но произведение $BA$ уже не определено из-за несогласованности размеров матриц.\n\n   2. Пусть $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$. Тогда определено как произведение $AB \\in \\operatorname{M}_{m} (\\mathbb R)$, так и $BA \\in \\operatorname{M}_{n} (\\mathbb R)$. Однако матрицы $AB$ и $BA$ имеют разные размеры, следовательно, не могут быть одинаковыми.\n\n   3. Пусть $A, B \\in \\operatorname{M}_{n} (\\mathbb R)$. Тогда определено как произведение $AB \\in \\operatorname{M}_{n} (\\mathbb R)$, так и $BA \\in \\operatorname{M}_{n} (\\mathbb R)$, и у матриц совпадают размеры. Однако даже это не даёт им быть одинаковыми. Пример:\n\n      $$\\begin{pmatrix}\n      0 \u0026 1\\\\\n      0 \u0026 0\n      \\end{pmatrix}\n      \\begin{pmatrix}\n      0 \u0026 0\\\\\n      1 \u0026 0\n      \\end{pmatrix}=\n      \\begin{pmatrix}\n      1 \u0026 0\\\\\n      0 \u0026 0\n      \\end{pmatrix}\n      ,\\quad\n      \\text{но}\n      \\quad\n      \\begin{pmatrix}\n      0 \u0026 0\\\\\n      1 \u0026 0\n      \\end{pmatrix}\n      \\begin{pmatrix}\n      0 \u0026 1\\\\\n      0 \u0026 0\n      \\end{pmatrix}=\n      \\begin{pmatrix}\n      0 \u0026 0\\\\\n      0 \u0026 1\n      \\end{pmatrix}\n      $$\n      \n      \n\n2. В матрицах есть `делители нуля`.\n   То есть существуют ненулевые матрицы $A$ и $B$, такие, что: $AB=0$. Пример:\n\n   $$\\begin{pmatrix}\n   1 \u0026 0\\\\\n   0 \u0026 0\n   \\end{pmatrix}\n   \\begin{pmatrix}\n   0 \u0026 0\\\\\n   0 \u0026 1\n   \\end{pmatrix}\n   = 0\n   $$\n   \n   \n\n3. В матрицах есть `нильпотенты`.\n   То есть можно найти такую ненулевую матрицу $A$, что $A^{n} = 0$. Пример:\n\n   $$\\begin{pmatrix}\n   0 \u0026 1\\\\\n   0 \u0026 0\n   \\end{pmatrix}^{2}=\n   \\begin{pmatrix}\n   0 \u0026 1\\\\\n   0 \u0026 0\n   \\end{pmatrix}\n   \\begin{pmatrix}\n   0 \u0026 1\\\\\n   0 \u0026 0\n   \\end{pmatrix}\n   = 0\n   $$\n   \n   \n\n**Транспонирование**\n\nПусть $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$, тогда определим транспонированную матрицу $A^{t} \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ следующим образом:\n\n$$A=\n\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n, \\quad\nA^t=\n\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026 a_{m1}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{1n}\u0026 \\ldots \u0026a_{mn}\n\\end{pmatrix}\n$$\n\nТо есть строками транспонированной матрицы $A^t$ служат столбцы матрицы $A$, а столбцами — строки матрицы $A$. На уровне элементов это выглядит так: $a_{ij} = a^{t}_{ji}$.\n\nПримеры:\n\n$$A^t=\n{\n\\begin{pmatrix}\n1 \u0026 2 \u0026 3\\\\\n4 \u0026 5 \u0026 6\n\\end{pmatrix}\n}^{t}=\n\\begin{pmatrix}\n1 \u0026 4 \\\\\n2 \u0026 5 \\\\\n3 \u0026 6\n\\end{pmatrix}\n\\\\\nC^t=\n{\n\\begin{pmatrix}\n7\\\\\n6\n\\end{pmatrix}\n}^{t}=\n\\begin{pmatrix}\n7 \u0026 6 \\\\\n\\end{pmatrix}\n$$\n\nКак вы думаете, чему равно $(A^{t})^{t}$?\n\n{% cut \"Ответ (не открывайте сразу; сначала подумайте сами!)\" %}\n\n1. После первого транспонирования получаем матрицу $A^t$, где элементы $a_{ij}$ становятся $a_{ji}$.\n\n$$A^t = (a_{ij})^t = (a_{ji})\n$$\n\n2. После второго транспонирования получаем матрицу $(A^{t})^{t}$, где элементы $a_{ji}$ становятся $a_{ij}$.\n\n$$(A^{t})^{t} = (a_{ji})^t = (a_{ij})\n$$\n\n3. Таким образом:\n\n$$(A^{t})^{t} = A\n$$\n\n{% endcut %}\n\n**След матрицы**\n\n`След` — это сумма диагональных элементов матрицы. Пусть $A \\in \\operatorname{M}_{n} (\\mathbb R)$, тогда определим след матрицы $\\operatorname{tr}A$ так:\n\n$$\\operatorname{tr}A = \\sum_{i=1}^n a_{ii}\n$$\n\nОтметим важные `свойства следа` матрицы:\n\n1. Для любых матриц $A, B \\in \\operatorname{M}_{n} (\\mathbb R)$ верно $\\operatorname{tr}(A + B) = \\operatorname{tr}(A) + \\operatorname{tr}(B)$.\n2. Для любой матрицы $A \\in \\operatorname{M}_{n} (\\mathbb R)$ и $\\lambda \\in \\mathbb R$ выполнено $\\operatorname{tr}(\\lambda A) = \\lambda \\operatorname{tr}(A)$.\n3. Для любых матриц $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ выполнено $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$.\n\nТретье свойство особо важно в анализе данных и линейной алгебре. Например, при упрощении выражений в оптимизационных задачах: регуляризация в линейной регрессии или оценка суммы квадратов через произведение матриц.\n\n{% cut \"Пример №1\" %}\n\nПосмотрим на регуляризацию в классической постановке линейной регрессии. В текущем примере мы продемонстрируем, как [L2-регуляризация](https://education.yandex.ru/knowledge/trenirovki-po-ml-lektsiia-2-lineinaia-regressiia-i-reguliarizatsiia) (или ridge-регрессия), которая добавляет штраф за большие значения весов, используется в контексте матричных выражений и как свойство следа помогает упрощать записи для аналитических и численных методов.\n\nПусть:\n\n- $\\mathbf{X} \\in \\mathbb R^{m \\times n}$ — матрица признаков (строки соответствуют объектам, столбцы — признакам),\n- $\\mathbf{w} \\in \\mathbb R^{n \\times 1}$ — вектор весов (параметры модели),\n- $\\mathbf{y} \\in \\mathbb R^{m \\times 1}$ — вектор целевых значений.\n\nТогда вектор предсказаний:\n\n$$\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} \\quad (\\text{размер } m \\times 1).\n$$\n\nПри L2-регуляризации к функционалу суммы квадрата ошибки добавляется член $\\lambda \\|\\mathbf{w}\\|^2$:\n\n$$L(\\mathbf{w}) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 + \\lambda \\|\\mathbf{w}\\|^2,\n$$\n\nгде $\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\mathsf{T}\\mathbf{w}$.\n\nИногда, когда эта регуляризация записывается в матричном (или «следовом») виде, мы можем встретить выражения наподобие:\n\n$$\\lambda \\,\\mathbf{w}^\\mathsf{T}\\mathbf{w} = \\lambda \\,\\operatorname{tr}\\bigl(\\mathbf{w}^\\mathsf{T}\\mathbf{w}\\bigr) = \\lambda \\,\\operatorname{tr}\\bigl(\\mathbf{w}\\mathbf{w}^\\mathsf{T}\\bigr),\n$$\n\nи снова свойство $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$ позволяет объединять такие слагаемые с другими матричными выражениями в удобной форме. Например, при анализе функций потерь или выведении их градиентов.\n\n{% endcut %}\n\n{% cut \"Пример №2\" %}\n\nИногда (особенно в задачах, связанных с статистикой и PCA) появляется сумма вида $\\sum_{i} x_i^\\mathsf{T} A \\, x_i,$ где $x_i$ — строки некоторой матрицы $\\mathbf{X}$.\n\nЗаписав $\\mathbf{X}$ целиком, мы можем перейти к выражениям наподобие:\n\n$$\\sum_{i=1}^m x_i^\\mathsf{T} A \\, x_i \\;=\\; \\operatorname{tr}\\Bigl( \\mathbf{X}^\\mathsf{T} A \\,\\mathbf{X}\\Bigr),\n$$\n\nА затем использовать:\n\n$$\\operatorname{tr}\\bigl(\\mathbf{X}^\\mathsf{T} A \\,\\mathbf{X}\\bigr) = \\operatorname{tr}\\bigl(A \\,\\mathbf{X} \\mathbf{X}^\\mathsf{T}\\bigr).\n$$\n\nЗдесь чётко видно, как перестановка факторов под следом (из-за $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$) упрощает анализ ковариационных матриц $\\mathbf{X}\\mathbf{X}^\\mathsf{T}$ или при поиске оптимальных $A$.\n\nДанное свойство также крайне удобно при дифференцировании «следовых» выражений. В теории оптимизации часто встречаются производные вида  $\\tfrac{\\partial}{\\partial X} \\operatorname{tr}(X^\\mathsf{T}AX)$, а благодаря свойству $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$ мы можем свободно переставить множители, что даёт в итоге более лаконичную формулу для градиента:\n\n$$\\tfrac{\\partial}{\\partial X} \\operatorname{tr}(X^\\mathsf{T} A X) = A^\\mathsf{T} X + A X \\quad (\\text{если } A \\text{ симметрична, то } A^\\mathsf{T}=A).\n$$\n\nЭто упрощение позволяет легко получать результат без разворачивания сумм по индексам, что крайне полезно в машинном обучении и методах оптимизации, особенно при работе с большими матрицами и векторно-ориентированными библиотеками вроде `pytorch`, `numpy` и другими.\n\n{% endcut %}\n\nПосле того как мы изучили, как задаются матрицы и выполняются над ними операции (сложение, умножение, транспонирование и так далее), рассмотрим свойства этих операций. Они определяют их алгебраическую структуру и играют ключевую роль в решении как теоретических, так и практических задач.\n\n### Свойства операций\n\n 1. **Ассоциативность сложения**\n    $(A+B)+C=A+(B+C)$ для любых $A,B,C \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$.\n 2. **Существование нейтрального элемента для сложения**\n    Существует единственная матрица $0$, такая, что: $A + 0 = 0 + A = A$ для всех $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$. Такая матрица целиком заполнена нулями.\n 3. **Коммутативность сложения**\n    $A + B = B + A$  для любых $A,B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$.\n 4. **Наличие обратного по сложению**\n    Для любой матрицы $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ существует матрица $-A$, такая, что: $A + (-A) = (-A) + A = 0$. Такая матрица единственная и состоит из элементов $-a_{ij}$.\n 5. **Ассоциативность умножения**\n    Для любых матриц $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R), B \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$ и $C \\in \\operatorname{M}_{k \\thinspace t} (\\mathbb R)$ верно $(AB)C=A(BC)$.\n 6. **Существование нейтрального элемента для умножения**\n    Для каждого $k$ существует единственная матрица $E \\in \\operatorname{M}_{k} (\\mathbb R)$, такая, что для любой матрицы $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно  $EA=AE=A$. У такой матрицы $E_{ii}=1$, а $E_{ij}=0$.\n 7. **Дистрибутивность умножения относительно сложения**\n    Для любых матриц $A, B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $C \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$ верно ($A+B)C=AC+BC$. Аналогично для любых $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B,C \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$ верно $A(B+C)=AB+AC$.\n 8. **Умножение на числа ассоциативно**\n    Для любых $\\lambda, \\mu \\in \\mathbb R$ и любой матрицы $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $\\lambda(\\mu A)=(\\lambda \\mu)A$. Аналогично для любого $\\lambda \\in \\mathbb R$ и любых $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$ верно $\\lambda (AB)=(\\lambda A)B$.\n 9. **Умножения на числа дистрибутивно относительно сложения матриц и относительно сложения чисел**\n    Для любых $\\lambda, \\mu \\in \\mathbb R$ и $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $(\\lambda + \\mu)A = \\lambda A + \\mu A$. Аналогично для любого $\\lambda \\in \\mathbb R$ и $A,B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $\\lambda (A+B) = \\lambda A + \\lambda B$.\n10. **Умножение на скаляр нетривиально**\n    Если $1 \\in \\mathbb R$, то для любой матрицы $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $1A=A$.\n11. **Умножение на скаляр согласовано с умножением матриц**\n    Для любого $\\lambda \\in \\mathbb R$ и любых $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и $B \\in \\operatorname{M}_{n \\thinspace k} (\\mathbb R)$ верно $\\lambda (AB) = (\\lambda A) B= A(\\lambda B) = (AB) \\lambda$.\n12. **Транспонирование согласовано с суммой**\n    Для любых матриц $A,B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $(A+B)^{t} = A^t + B^t$.\n13. **Транспонирование согласовано с умножением на скаляр**\n    Для любой матрицы $A \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ и любого $\\lambda \\in \\mathbb R$ верно $(\\lambda A)^t = \\lambda A^t$.\n14. **Транспонирование согласовано с умножением**\n    Для любых матриц $A,B \\in \\operatorname{M}_{m \\thinspace n} (\\mathbb R)$ верно $(AB)^t = B^{t}A^{t}$.\n\nА теперь мы можем посмотреть, как операции над матрицами реализованы в Python.\n\n### **Операции над матрицами в Python**\n\nОбычно для выполнения арифметических операций с матрицами они должны быть одинакового размера. Однако, как мы сказали выше, в NumPy можно оперировать матрицами различного размера за счёт механизма `broadcasting`. При этом действуют следующие правила:\n\n1. Если объекты имеют разное число измерений, к объекту с меньшим числом измерений добавляются измерения длины 1 слева.\n2. Для каждого соответствующего измерения длины должны совпадать или одна из них должна быть равна 1.\n\nНапример, если один тензор имеет размер $(2, 1)$, а другой $(3, 1, 2)$, то размер первого тензора при операциях будет приведён к $(1, 2, 1)$. При этом если их сложить, то размер получившегося тензора будет уже $(3, 2, 2)$: по каждому из измерений тензоров мы взяли максимальное значение длины. Это позволяет писать компактный и эффективный код, не прибегая к явному копированию данных для согласования размеров объектов.\n\nРассмотрим несколько примеров.\n\n```python\nimport numpy as np\n\nr = 4  # скаляр\nv = np.arange(3)  # вектор \na = np.arange(9).reshape(3, 3)  # матрица\n\n# Сумма скаляра и вектора\nprint(r + v)\n\n# Сумма вектора и матрицы\nprint(v + a)\n\n# Сумма векторов разного размера\nprint(v.reshape(3, 1) + v) \n\n# Output:\n# array([4, 5, 6])\n\n# array([[ 0,  2,  4],\n#        [ 3,  5,  7],\n#        [ 6,  8, 10]])\n       \n# array([[0, 1, 2],\n#        [1, 2, 3],\n#        [2, 3, 4]])\n```\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.5: Иллюстрация механизма broadcasting\" src=\"https://yastatic.net/s3/education-portal/media/4_3_4_broadcasting_d36504175f.webp\"\u003e\n  \u003cfigcaption\u003e\n\nИллюстрация механизма broadcasting\n\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nОтдельно стоит рассмотреть, как устроено умножение матриц и векторов в NumPy. Можно выделить два основных способа умножения, которые выглядят похоже, но выполняют совершенно разные операции.\n\n1\\.**Поэлементное умножение (`*`)**\n\nОператор `*` выполняет поэлементное умножение двух тензоров, которые должны иметь совместимые размеры (согласно правилам `broadcasting`). То есть если у вас есть два тензора одинакового размера (или они могут быть «расширены» до одного размера), то операция `a * b` перемножает соответствующие элементы:\n\n$$(a * b)_{ij} = a_{ij} \\cdot b_{ij}\n$$\n\nПример:\n\n```python\nimport numpy as np\n\n# Задаём два вектора\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Поэлементное умножение: возвращает массив тех же размеров\nelementwise = a * b \nprint(\"Поэлементное умножение a * b:\")\nprint(elementwise)\n\n# Задаём две матрицы\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Поэлементное умножение: перемножаются соответствующие элементы\nelementwise = A * B  \nprint(\"Поэлементное умножение A * B:\")\nprint(elementwise)\n\n# Output:\n# Поэлементное умножение a * b:\n# [ 4 10 18]\n# Поэлементное умножение A * B:\n# [[ 5 12]\n#  [21 32]]\n```\n\n2\\.**Матричное умножение** (`@` или `np.dot`/`np.matmul`)\n\nОператор `@` (введённый в Python 3.5) предназначен для матричного умножения согласно правилам линейной алгебры. При этом для двух векторов $a$ и $b$ операция `a @ b` вычисляет их скалярное произведение:\n\n$$a^t \\cdot b = \\sum_{i=1}^n a_i \\, b_i\n$$\n\nА для матриц $A$ и $B$ (при условии, что число столбцов $A$ равно числу строк $B$) результатом будет матрица $C$, где:\n\n$$(AB)_{ij} = \\sum_{t=1}^n A_{it} \\, B_{tj}\n$$\n\nПример:\n\n```python\nimport numpy as np\n\n# Задаём два вектора\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Матричное умножение: возвращает скаляр\ndot_product = a @ b  # Результат: 1*4 + 2*5 + 3*6 = 32\nprint(\"Скалярное произведение a @ b:\")\nprint(dot_product)\n\n# Задаём две матрицы\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Матричное умножение (стандартное): используется оператор @\nmatrix_product = A @ B  \nprint(\"Матричное умножение A @ B:\")\nprint(matrix_product)\n\n# Output:\n# Скалярное произведение a @ b:\n# 32\n# Матричное умножение A @ B:\n# [[19 22]\n#  [43 50]]\n```\n\nЕсли массивы имеют более двух измерений, оператор `*` всё равно делает поэлементное умножение, а `@` или `np.matmul` применяют матричное умножение к последним двум осям с поддержкой `broadcasting` для остальных осей.\n\n```python\nimport numpy as np\n\n# Создаём два 3D массива (тензора)\nX = np.random.rand(2, 3, 4)\nY = np.random.rand(2, 4, 5)\n\n# np.matmul (или оператор @) умножает X и Y по последним двум измерениям\nmatmul_result = X @ Y  \nprint(\"Форма результата матричного умножения:\", matmul_result.shape)\n\n# Поэлементное умножение (*) здесь не сработает напрямую, \n# т. к. размеры (2,3,4) и (2,4,5) несовместимы для \n# поэлементного умножения без явного преобразования.\nelementwise_result = X * Y\n\n# Output:\n# Форма результата матричного умножения: (2, 3, 5)\n# ValueError: operands could not be broadcast together with shapes (2,3,4) (2,4,5)\n```\n\nТеперь потренируемся с операциями над матрицами.\n\nРанее мы уже видели, как с помощью матрицы задаётся изображение. Давайте рассмотрим на наглядном примере, как операции с матрицами могут влиять на практический результат. Для этого возьмём изображение и определённым образом изменим его матрицу.\n\n```python\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Путь к изображению\nimage_path = \"image.jpg\"\n\n# Считываем изображение и переводим его в оттенки серого\nimage = Image.open(image_path).convert(\"L\")\nimage_np = np.array(image, dtype=float)\n\n# Фильтр понижения яркости: отнимаем константу от всех элементов матрицы\nbrightness_offset = -50\nresult_brightness = np.clip(image_np + brightness_offset, 0, 255)\n\n# Фильтр инверсии: вычисляем отрицательное изображение\nresult_inversion = 255 - image_np\n\n# Пороговая фильтрация: если значение пикселя больше порога, ставим 255, иначе 0\nthreshold = 128\nresult_threshold = np.where(image_np \u003e threshold, 255, 0)\n\n# Создаём фигуру с заданными параметрами\nfig, axes = plt.subplots(2, 2, figsize=(15, 8), dpi=200)\n\naxes[0, 0].imshow(image_np, cmap='gray')\naxes[0, 0].set_title(\"Исходное изображение\")\naxes[0, 0].axis('off')\n\naxes[0, 1].imshow(result_brightness, cmap='gray')\naxes[0, 1].set_title(\"Понижение яркости\")\naxes[0, 1].axis('off')\n\naxes[1, 0].imshow(result_inversion, cmap='gray')\naxes[1, 0].set_title(\"Инверсия изображения\")\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(result_threshold, cmap='gray')\naxes[1, 1].set_title(\"Пороговая фильтрация\")\naxes[1, 1].axis('off')\n\nplt.subplots_adjust(wspace=-0.35, hspace=0.1)\nplt.show()\n```\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Рисунок 4.3.6: Иллюстрация операций с матрицами\" src=\"https://yastatic.net/s3/education-portal/media/4_3_4_Filtracziya_izobrazhenij_851ff19426.webp\"\u003e\n  \u003cfigcaption\u003e\n  Иллюстрация операций с матрицами\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nКак видите, матрицы — это мощный инструмент не только для чисто теоретических преобразований в линейной алгебре, но и для прикладных задач анализа данных и машинного обучения.\n\nВ каждой задаче, где нужно компактно описать и эффективно обработать многомерные данные, матрицы позволяют переводить реальную постановку в форму линейного или нелинейного уравнения.\n\nВ следующем параграфе мы обсудим, как с помощью матриц формулируются системы линейных уравнений и почему умение эффективно их решать столь важно для построения современных моделей машинного обучения и анализа больших датасетов.\n\nА пока советуем пройти квиз.\n\n\u003c/br\u003e\n\u003c/br\u003e\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13737804.6e78e1bead9f08af2fc17239554f6bfffc145441/?iframe=1\" frameborder=\"0\" name=\"ya-form-13737804.6e78e1bead9f08af2fc17239554f6bfffc145441\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"66:Tbc8c,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы разберём, что такое `системы линейных уравнений` (`СЛУ`), каких видов они бывают и как их можно эффективно решать с помощью алгоритма Гаусса.\n\nНо самое главное — мы разберёмся, *зачем* вообще всё это нужно. Ведь системы линейных уравнений лежат в основе огромного числа прикладных задач анализа данных: от обучения линейной регрессии до оптимизации параметров в нейросетях.\n\nКаждый раз, когда мы хотим наилучшим образом подогнать модель под данные, минимизируя ошибку, задача в итоге сводится к решению `СЛУ`. Без них было бы невозможно эффективно решать задачи прогноза, факторизации, даже построения графов и вероятностных моделей.\n\n\u003e 👉 Поэтому понять, как работают `СЛУ`, — значит научиться видеть скелет большинства алгоритмов машинного обучения.\n\n### Определение\n\n`Система линейных уравнений (СЛУ)` — это набор $m$ уравнений и $n$ неизвестных переменных, где в каждом уравнении каждая переменная входит *только* в первой степени и не перемножается с другими переменными.\n\n$$\\begin{cases}\n\\begin{aligned}\na_{11}x_1 + \u0026\\ldots + a_{1n}x_n = b_1\\\\\n\u0026\\ldots \\\\\na_{m1}x_1 + \u0026\\ldots + a_{mn}x_n = b_m\n\\end{aligned}\n\\end{cases}\n$$\n\nТакже выделяют однородную систему линейных уравнений (`ОСЛУ`), где вектор свободных членов (число без переменных в правой части уравнений) состоит из нулей.\n\n$$\\begin{cases}\n\\begin{aligned}\na_{11}x_1 + \u0026\\ldots + a_{1n}x_n = 0\\\\\n\u0026\\ldots \\\\\na_{m1}x_1 + \u0026\\ldots + a_{mn}x_n = 0\n\\end{aligned}\n\\end{cases}\n$$\n\nВ предыдущей главе мы уже познакомились с векторами и матрицами, поэтому удобно будет представить `СЛУ` с их помощью: $Ax=b$, где $A$ — это матрица коэффициентов $m \\times n$, $x$ — вектор неизвестных, $b$ — вектор свободных членов. Также выделяют расширенную запись системы $(A|b)$.\n\n$$A=\n\\begin{pmatrix}\na_{11}\u0026\\ldots\u0026a_{1n}\\\\\n\\vdots\u0026\\ddots\u0026\\vdots\\\\\na_{m1}\u0026\\ldots \u0026a_{mn}\n\\end{pmatrix},\n\\quad\nx =\n\\begin{pmatrix}\nx_1\\\\\\vdots\\\\x_n\n\\end{pmatrix},\n\\quad\nb =\n\\begin{pmatrix}\nb_1\\\\\\vdots\\\\b_m\n\\end{pmatrix},\n\\quad\n$$\n\n$$(A|b) =\\left(\\,\\left.\\begin{matrix}a_{11} \u0026 \\ldots \u0026 a_{1n}\\\\\\vdots \u0026 \\ddots \u0026 \\vdots\\\\a_{m1} \u0026 \\ldots \u0026 a_{mn}\\end{matrix}\\:\\right|\\:\\begin{matrix}b_1\\\\\\vdots\\\\b_m\\end{matrix}\\right)\n$$\n\nРассмотрим, какими бывают системы линейных уравнений с точки зрения количества решений, а также приведём конкретные примеры для каждого из видов систем.\n\n### Виды систем линейных уравнений\n\nВсе системы линейных уравнений можно разделить на три вида относительно их количества решений:\n\n1. Нет решений.\n2. Одно решение.\n3. Бесконечно много решений.\n\nДавайте рассмотрим базовый случай, когда `СЛУ` содержит одно уравнение и одну неизвестную:\n\n- $0x=1$ (нет решений)\n- $x=0$ (одно решение)\n- $0x=0$ (бесконечно много решений)\n\nТеперь рассмотрим более интересный случай: в пространстве $\\mathbb R^2$ каждое уравнение задаёт прямую, а решение `СЛУ` — точка их пересечения. Рассмотрим несколько численных примеров для наглядности.\n\n`Несовместная СЛУ` — нет решений:\n\n$$\\begin{cases}\n\\begin{aligned}\nx_1 + 3x_2 = 5\\\\\nx_1 + 3x_2 = 0\n\\end{aligned}\n\\end{cases}\n$$\n\n![Рисунок 4.4.1: Пример СЛУ без решений](https://yastatic.net/s3/education-portal/media/4_4_1_1531cd7a60.webp \"Пример СЛУ без решений\")\n\n`Совместная СЛУ (определённая)` — имеет ровно одно решение:\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + 3x_2 = 5\\\\\n\u0026x_1 - 2x_2 = -5\n\\end{aligned}\n\\end{cases}\n$$\n\n![Рисунок 4.4.2: Пример СЛУ с одним решением](https://yastatic.net/s3/education-portal/media/4_4_2_1c5740a36f.webp \"Пример СЛУ с одним решением\")\n\n`Совместная СЛУ (неопределённая)` — имеет бесконечно много решений:\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + 3x_2 = 5\\\\\n\u00262x_1 + 6x_2 = 10\n\\end{aligned}\n\\end{cases}\n$$\n\n![Рисунок 4.4.3: Пример СЛУ с бесконечным количеством решений](https://yastatic.net/s3/education-portal/media/4_4_3_f1d3c86208.webp \"Пример СЛУ с бесконечным количеством решений\")\n\nВ машинном обучении с каждой из трёх ситуаций мы сталкиваемся постоянно.\n\n- **Нет решений (несовместная `СЛУ`)** ­— классический признак переопределённой модели с жёсткими ограничениями. Например, когда мы пытаемся подобрать *точно* совпадающее значение параметров под все заказанные бизнес‑метрики сразу.\n- **Одно решение** ­— идеальный сценарий в обычной линейной регрессии без мультиколлинеарности, то есть без линейно зависимых признаков в матрице.\n- **Бесконечно много решений** появляется, когда признаков сильно больше наблюдений (типичная широкая матрица в NLP или CV). Здесь и рождаются методы регуляризации\u0026nbsp;(L2, L1, [Dropout‑like шумы](https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya#:~:text=%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%87%D0%B5%D1%80%D0%B5%D0%B7%20%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D1%8B%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8))\u0026nbsp;— они выбирают единственное «наиболее простое» решение из бесконечного семейства.\n\nПрежде чем двинуться дальше, давайте чуть остановимся. Как вы думаете, возможен ли случай, когда `СЛУ` имеет ровно два решения?\n\n{% cut \"Ответ (не открывайте сразу, сначала подумайте сами!)\" %}\n\nНет, так как в примере выше решением является пересечение двух прямых, а они не могут пересекаться строго в двух точках.\n\n{% endcut %}\n\nДвигаемся дальше. Теперь поговорим о том, как решать системы линейных уравнений. Основной метод — это алгоритм Гаусса. Давайте разберём его подробнее.\n\n### Алгоритм Гаусса\n\nЦель данного алгоритма заключается в том, чтобы с помощью последовательности элементарных преобразований привести систему к такому «упрощённому» виду, в котором легко описать все её решения или понять, что решений нет.\n\nЧтобы раскрыть метод Гаусса, нужно рассмотреть три аспекта:\n\n- элементарные преобразования, применяемые в этом алгоритме;\n- вид, к которому приводится система;\n- и сам алгоритм.\n\nДавайте приступим.\n\n**Элементарные преобразования**\n\nЧтобы уметь переходить от одной системы линейных уравнений к другой без изменения множества решений — то есть так, чтобы все старые решения оставались решениями новой системы и наоборот, — используют следующие элементарные преобразования:\n\n1. Прибавление одной строки к другой с множителем ($i \\neq j$)\n\n$$\\left(\\,\\left.\n  \\begin{matrix}\n    a_{11}\u0026\\ldots\u0026a_{1n}\\\\\n    a_{i1}\u0026\\ldots\u0026a_{in}\\\\\n    a_{j1}\u0026\\ldots\u0026a_{jn}\\\\\n    a_{m1}\u0026\\ldots\u0026a_{mn}\n  \\end{matrix}\n  \\:\\right|\\:\n  \\begin{matrix}\n    b_1\\\\\n    b_i\\\\\n    b_j\\\\\n    b_m\n  \\end{matrix}\n\\right)\n\\mapsto\n\n\\left(\\,\\left.\n  \\begin{matrix}\n    a_{11}\u0026\\ldots\u0026a_{1n}\\\\\n    a_{i1}\u0026\\ldots\u0026a_{in}\\\\\n    a_{j1} + \\lambda a_{i1}\u0026\\ldots\u0026a_{jn}+ \\lambda    a_{in}\\\\\n    a_{m1}\u0026\\ldots\u0026a_{mn}\n  \\end{matrix}\n  \\:\\right|\\:\n  \\begin{matrix}\n    b_1\\\\\n    b_i\\\\\n    b_j+ \\lambda b_i\\\\\n    b_m\\\\\n  \\end{matrix}\n\\right)\n\n$$\n\n2. Перестановка двух строк\n\n$$\\left(\\left.\n\\begin{matrix}\na_{11}\u0026\\ldots\u0026a_{1n}\\\\\na_{i1}\u0026\\ldots\u0026a_{in}\\\\\na_{j1}\u0026\\ldots\u0026a_{jn}\\\\\na_{m1}\u0026\\ldots\u0026a_{mn}\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_i\\\\\nb_j\\\\\nb_m\n\\end{matrix}\n\\right)\n\\mapsto\n\\left(\\left.\n\\begin{matrix}\na_{11}\u0026\\ldots\u0026a_{1n}\\\\\na_{j1}\u0026\\ldots\u0026a_{jn}\\\\\na_{i1}\u0026\\ldots\u0026a_{in}\\\\\na_{m1}\u0026\\ldots\u0026a_{mn}\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_j\\\\\nb_i\\\\\nb_m\n\\end{matrix}\n\\right)\n$$\n\n3. Умножение строки на ненулевую константу ($\\lambda \\neq 0$)\n\n$$\\left(\\left.\n\\begin{matrix}\na_{11}\u0026\\ldots\u0026a_{1n}\\\\\na_{i1}\u0026\\ldots\u0026a_{in}\\\\\na_{m1}\u0026\\ldots\u0026a_{mn}\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_i\\\\\nb_m\n\\end{matrix}\n\\right)\n\\mapsto\n\\left(\\left.\n\\begin{matrix}\na_{11}\u0026\\ldots\u0026a_{1n}\\\\\n\\lambda a_{i1}\u0026\\ldots\u0026\\lambda a_{in}\\\\\na_{m1}\u0026\\ldots\u0026a_{mn}\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\n\\lambda b_i\\\\\nb_m\n\\end{matrix}\n\\right)\n$$\n\n{% cut \"Элементарные преобразования в ML‑фреймворках\" %}\n\nВ `pytorch` и `tensorflow` те же три типа операций — перестановка строк, прибавление одной строки к другой и умножение строки на число — лежат в основе низкоуровневых функций, таких как `torch.linalg.lu`, `tf.linalg.set_diag` или `scatter_add`.\n\nПонимание того, *какое именно* преобразование выполняет матричный блок, помогает лучше ориентироваться в вычислениях и понимать, где можно отключить лишние градиенты с помощью `torch.no_grad()` — например, чтобы сократить использование памяти.\n\n{% endcut %}\n\nНа практике подобные преобразования часто применяют одновременно (за один шаг) с целью ускорения вычислений. Хороший вопрос заключается в том, почему такие преобразования не меняют множество решений?\n\n{% cut \"Ответ (не открывайте сразу, сначала подумайте сами!)\" %}\n\nЕсли $x$ — решение исходной системы, то:\n\n- При **прибавлении одной строки к другой с множителем** левая и правая части нового уравнения становятся суммой двух истинных равенств, значит, снова истинны.\n\n$$a_{i1}x_1 + \\ldots + a_{in}x_n = b_{i}\n\\\\\na_{j1}x_1 + \\ldots + a_{jn}x_n = b_{j}\n\\\\\n(a_{j1} + \\lambda a_{i1})x_1+\\ldots +(a_{jn}+ \\lambda a_{in})x_n=( b_{j} + \\lambda b_{i})\n$$\n\n- При **перестановке строк** меняется только порядок уравнений, а каждое по-прежнему выполняется.\n- При **умножении строки на ненулевую константу** $\\lambda \\neq 0$ все коэффициенты и свободный член умножаются на $\\lambda$, но исходное равенство остаётся верным.\n\n$$a_{i1}x_1 + \\ldots + a_{in}x_n = b_{i} \\longmapsto (\\lambda a_{i1})x_1 + \\ldots + (\\lambda a_{in})x_n = \\lambda b_{i}\n$$\n\nКаждая из этих операций имеет «противоположную» того же типа:\n\n- Прибавление одной строки к другой с множителем обращается вычитанием этой же строки с множителем.\n\n$$(a_{j1} + \\lambda a_{i1})x_1+\\ldots +(a_{jn}+ \\lambda a_{in})x_n=( b_{j} + \\lambda b_{i})\n\\\\\n(a_{j1} + \\lambda a_{i1} - \\lambda a_{i1})x_1+\\ldots +(a_{jn}+ \\lambda a_{in} - \\lambda a_{in})x_n=( b_{j} + \\lambda b_{i} - \\lambda b_{i})\n\\\\\na_{j1}x_1 + \\ldots + a_{jn}x_n = b_{j}\n$$\n\n- Перестановку строк $i \\leftrightarrow j$ обращает перестановка тех же строк.\n- Умножение на $\\lambda$ отменяется умножением на $1 / \\lambda$.\n\nКаждое из этих преобразований обратимо и приводит систему линейных уравнений к эквивалентной системе. Это означает, что любые изменения, выполненные при помощи элементарных преобразований (перестановка строк, умножение строки на ненулевой скаляр, прибавление строки, умноженной на скаляр, к другой строке), могут быть «отменены», и решения исходной системы и преобразованной системы совпадают.\n\n{% endcut %}\n\nРассмотренные элементарные преобразования можно представить и как умножение исходной матрицы на матрицу слева. Это очень удобно, когда нам нужно структурировано представить операции над матрицами.\n\n1. Прибавление одной строки к другой с множителем ($i \\neq j$). Пусть $S_{ij}(\\lambda) \\in \\operatorname{M}_{n} (\\mathbb R)$ — матрица, полученная из единичной, где на позиции элемента $i$ строки и $j$ столбца стоит $\\lambda \\in \\mathbb R$.\n\n   $$\\begin{array}{cc}\n     \u0026 \\quad\\quad j \\\\[2mm]\n   \\begin{array}{c}\n     \\\\[1mm]\n     i \\\\[5mm]\n     \\\\\n   \\end{array}\n   \u0026\n   \\begin{pmatrix}\n     1 \u0026 0 \u0026 \\ldots \u0026 0 \\\\\n     0 \u0026 \\ddots \u0026 \\lambda \u0026 \\vdots \\\\\n     \\vdots \u0026  \u0026 \\ddots \u0026 0 \\\\\n     0 \u0026 \\ldots \u0026 0 \u0026 1\n   \\end{pmatrix}\n   \\end{array}\n   $$\n   \n   Тогда умножение $A \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ на $S_{ij}(\\lambda)$ слева прибавляет $j$ строку, умноженную на $\\lambda$, к $i$ строке матрицы $A$.\n\n2. Перестановка двух строк. Пусть $P_{ij} \\in \\operatorname{M}_{n} (\\mathbb R)$ — матрица, полученная из единичной путём перестановки местами $i$ и $j$ столбцов.\n\n   $$\\begin{array}{cc}\n     \u0026 {i}\\quad\\quad\\quad{j} \\\\[2mm]\n     \u0026\n     \\begin{pmatrix}\n       {1} \u0026 {} \u0026 {} \u0026 {} \u0026 {} \\\\\n       {}  \u0026 {0} \u0026 {} \u0026 {1} \u0026 {} \\\\\n       {}  \u0026 {}  \u0026 {\\ddots} \u0026 {} \u0026 {} \\\\\n       {}  \u0026 {1} \u0026 {} \u0026 {0} \u0026 {} \\\\\n       {}  \u0026 {}  \u0026 {} \u0026 {} \u0026 {1}\n     \\end{pmatrix}\n   \\end{array}\n   $$\n   \n   Тогда умножение $A \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ на $P_{ij}$ слева переставляет $i$ и $j$ строки матрицы $A$.\n\n3. Умножение строки на ненулевую константу ($\\lambda \\neq 0$). Пусть $D_{i}(\\lambda) \\in \\operatorname{M}_{n} (\\mathbb R)$ — матрица, полученная из единичной умножением $i$ строки на $\\lambda \\in \\mathbb R$.\n\n   $$\\begin{array}{cc}\n     \u0026 i \\\\[3mm]\n     \\begin{array}{c}\n       {} \\\\[1mm]\n       {} \\\\[1mm]\n       i \\\\[1mm]\n       {} \\\\[1mm]\n       {}\n     \\end{array}\n     \u0026\n     \\begin{pmatrix}\n       1 \u0026 {} \u0026 {} \u0026 {} \u0026 {} \\\\\n       {} \u0026 \\ddots \u0026 {} \u0026 {} \u0026 {} \\\\\n       {} \u0026 {} \u0026 \\lambda \u0026 {} \u0026 {} \\\\\n       {} \u0026 {} \u0026 {} \u0026 \\ddots \u0026 {} \\\\\n       {} \u0026 {} \u0026 {} \u0026 {} \u0026 1\n     \\end{pmatrix}\n   \\end{array}\n   $$\n   \n   Тогда умножение $A \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ на $D_{i}$ слева умножает $i$ строку матрицы $A$ на $\\lambda$.\n\nМы рассмотрели, какие бывают элементарные преобразования и как их можно задавать с помощью матриц. Важно отметить, что элементарные преобразования можно выполнять как над строками, так и над столбцами матрицы.\n\nНа практике обычно делают преобразования строк, так как в этом случае множество решений `СЛУ` не меняется. При преобразовании столбцов `СЛУ` задаётся уже в новых переменных, поэтому и исходное множество решений может измениться.\n\nДалее мы увидим, как элементарные преобразования используются для упрощения структуры матрицы.\n\n**Ступенчатый вид**\n\nРассмотрим вид, к которому мы хотим привести матрицу. Важным элементом в матрице при приведении к ступенчатому виду является первый (если смотреть слева направо) ненулевой элемент в каждой строке, который называется `ведущим` ,`опорным` или `главным` элементом. Он используется при исключении неизвестных из остальных строк.\n\n`Ступенчатый вид матрицы` — это форма записи матрицы, которая удовлетворяет следующим условиям:\n\n- Все ненулевые строки располагаются выше строк, состоящих только из нулей.\n- В каждой ненулевой строке ведущий элемент располагается правее ведущего элемента предыдущей строки.\n\nВ общем виде матрица после приведения к ступенчатому виду выглядит так:\n\n$$\\left(\\left.\n\\begin{matrix}\n*\u0026*\u0026*\u0026\\ldots\u0026*\\\\\n0\u0026*\u0026*\u0026\\ldots\u0026*\\\\\n0\u00260\u0026*\u0026\\ldots\u0026*\\\\\n\n\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\n0\u00260\u00260\u0026\\ldots\u0026*\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n*\\\\\n\n*\\\\\n*\\\\\n\\vdots\\\\\n\n*\\\\\n\\end{matrix}\n\\right)\n$$\n\nГде `*` — это ненулевые элементы в общем случае. Может показаться, что матрица приведенная к ступенчатому виду, — это верхнетреугольная матрица. На самом деле это не совсем так. Отличия заключаются в следующем:\n\n#|\n||\n\n**Критерии**\n\n|\n\nВерхнетреугольная матрица\n\n|\n\n**Ступенчатый вид**\n\n||\n||\n\nДопустимые размеры\n\n|\n\nОбязательно квадратная\n\n|\n\nВозможен для произвольных прямоугольных матриц\n\n||\n||\n\nТребования к расположению нулевых элементов\n\n|\n\nВсе элементы ниже главной диагонали обнуляются\n\n|\n\nОбнуляются только элементы ниже каждого ведущего элемента в соответствующем ему столбце\n\n||\n||\n\nПозиции ведущих элементов\n\n|\n\nОтсутствует требование строгого смещения ведущих элементов вправо: номера столбцов ведущих элементов в разных строках могут не возрастать\n\n|\n\nДля ведущих элементов каждой последующей строки номер столбца всегда больше номера столбца ведущего элемента предыдущей строки\n\n||\n||\n\nРасположение нулевых строк\n\n|\n\nНулевые строки могут быть на любой строковой позиции\n\n|\n\nВсе нулевые строки находятся внизу матрицы\n\n||\n|#\n\nПриведем конкретный пример верхнетреугольной матрицы, которая при этом не соответствует ступенчатому виду:\n\n$$\\left(\\left.\n\\begin{matrix}\n0\u00261\u00261\\\\\n0\u00262\u00263\\\\\n0\u00260\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n1\\\\\n2\\\\\n3\n\\end{matrix}\n\\right)\n$$\n\nПояснение:\n\n- Матрица верхнетреугольная, потому что все элементы ниже главной диагонали равны нулю.\n- Матрица не ступенчатая, поскольку в первой строке первый ненулевой элемент стоит во втором столбце, но во второй строке первый ненулевой элемент также стоит во втором столбце. По определению ступенчатого вида, в каждой следующей ненулевой строке ведущий элемент должен идти строго правее ведущего элемента в предыдущей строке — здесь это условие не выполняется.\n\nНо важно отметить, что с помощью элементарных преобразований её легко можно привести к ступенчатому виду — попробуйте сделать это самостоятельно.\n\n{% cut \"Практическая ценность ступенчатого вида\" %}\n\nОказывается, знание свойств ступенчатого вида может сильно помочь в задачах масштабной матричной факторизации — например, в рекомендательных задачах. В `ALS‑алгоритме` ([Alternating Least Squares для матричной факторизации](https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij)) на каждом шаге приходится тысячи раз решать СЛУ вида:\n\n$$(Q^\\top Q + \\lambda I)\\,u = Q^\\top r,\n$$\n\nгде $Q$ — матрица уже найденных признаков, а $u$ — вектор, который мы пересчитываем.\n\nЕсли отсортировать пользователей по количеству рейтингов, то матрица $Q^\\top Q$ приобретает структуру, близкую к верхнетреугольной. Это позволяет ускорить алгоритмы решения СЛУ — например, `LU`‑разложение или метод Гаусса — с $O(k^3)$ до $O(k^2)$, что критично при обучении моделей на больших данных. Понимание ступенчатого вида помогает увидеть, когда такая оптимизация будет работать.\n\n{% endcut %}\n\n`Улучшенный ступенчатый вид матрицы` — это форма записи матрицы, которая удовлетворяет условиям ступенчатого вида матрицы, а также дополнительно:\n\n- каждый ведущий элемент равен 1;\n- в каждом столбце, где находится ведущий элемент, все остальные элементы как выше, так и ниже равны нулю.\n\nВ общем виде матрица после приведения к улучшенному ступенчатому виду может выглядеть так (это не полный список всех вариантов):\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u00260\u00260\u0026\\ldots\u00260\\\\\n0\u00261\u00260\u0026\\ldots\u00260\\\\\n0\u00260\u00261\u0026\\ldots\u00260\\\\\n\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\n0\u00260\u00260\u0026\\ldots\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n*\\\\\n*\\\\\n*\\\\\n\\vdots\\\\\n*\\\\\n\\end{matrix}\n\\right),\n\n\\left(\\left.\n\\begin{matrix}\n1\u0026*\u00260\u0026\\ldots\u00260\\\\\n0\u00260\u00261\u0026\\ldots\u00260\\\\\n0\u00260\u00260\u0026\\ldots\u00260\\\\\n\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\n0\u00260\u00260\u0026\\ldots\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n*\\\\\n*\\\\\n*\\\\\n\\vdots\\\\\n\n*\\\\\n\\end{matrix}\n\\right),\n\n\\left(\\left.\n\\begin{matrix}\n1\u00260\u0026*\u0026\\ldots\u00260\\\\\n0\u00261\u0026*\u0026\\ldots\u00260\\\\\n0\u00260\u00260\u0026\\ldots\u00260\\\\\n\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\n0\u00260\u00260\u0026\\ldots\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n*\\\\\n*\\\\\n*\\\\\n\\vdots\\\\\n\n*\\\\\n\\end{matrix}\n\\right).\n$$\n\nГде `*` — это ненулевые элементы в общем случае.\n\nТеперь разберёмся, в чем заключается сам алгоритм, который позволяет с помощью рассмотренных выше элементарных преобразований перейти сперва к ступенчатому, а потом и к улучшенному ступенчатому виду.\n\n\u003e 👉 На этом этапе у вас уже есть всё необходимое, чтобы составить алгоритм самостоятельно.\n\u003e\n\u003e Так что, если вам любопытно, советуем сделать паузу и попробовать, а затем сравнить результат.\n\n**Алгоритм**\n\nВ алгоритме Гаусса обычно выделяют два этапа: прямой ход и обратный ход. При этом иногда алгоритмом Гаусса называют только прямой ход, после которого для получения решений из ступенчатого вида системы пользуются обратной подстановкой.\n\nТакже иногда отдельно выделяют алгоритм Гаусса — Жордана, в котором прямой и обратный ход реализуются одновременно. Далее, чтобы избежать коллизии в терминологии, давайте будем рассматривать именно алгоритм Гаусса, в котором реализуются прямой и обратный ход за два отдельных шага.\n\n**Прямой ход** — мы зануляем элементы в столбцах, приводя матрицу системы к `ступенчатому виду`.\n\n1. Выбирается первый столбец, в котором среди ещё не обработанных строк обнаруживается ненулевой элемент.\n2. Строка с таким ненулевым элементом меняется местами с первой из ещё не обработанных строк и далее считается текущей строкой.\n3. Из каждой из оставшихся ещё не обработанных строк (стоящих ниже текущей строки) вычитают текущую строку, умноженную на соответствующий коэффициент, чтобы занулить все элементы в выбранном столбце под ведущим элементом.\n4. Шаги 1–3 повторяются для каждого следующего столбца справа. Если в каком-то столбце среди непройденных строк ненулевых элементов нет, сразу перейти к следующему.\n\nРазберем прямой ход алгоритма на примере системы из трёх уравнений и четырёх неизвестных, записанной в общем виде.\n\n$$\\left(\\left.\n\\begin{matrix}\na_{11}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\na_{21}\u0026 a_{22}\u0026a_{23}\u0026 a_{24}\\\\\na_{31}\u0026 a_{32}\u0026a_{33}\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\n**Примечание:**\n\n1. При переходе от одной матрицы к другой у нас будут меняться некоторые коэффициенты, но давайте оставим им прежние имена, чтобы не усложнять запись.\n2. Будем считать, что подчёркнутые элементы не равны нулю, чтобы можно было на них делить.\n3. Будем обозначать $i$-ю строку как $R_i$ для обозначения элементарных операций со строками. Например, запись $R_2 \\gets R_2 - \\frac{a_{21}}{a_{11}} \\cdot R_1$ означает, что из второй строки вычитается первая строка, умноженная на $\\frac{a_{21}}{a_{11}}$ , и полученный результат записывается в качестве второй строки.\n\nДействие №1: $R_2 \\gets R_2 - \\frac{a_{21}}{a_{11}} \\cdot R_1$\n\n$$\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 \\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\na_{31}\u0026 a_{32}\u0026\\underline{a_{33}}\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №2: $R_3 \\gets R_3 - \\frac{a_{31}}{a_{11}} \\cdot R_1$\n\n$$\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 \\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 a_{32}\u0026\\underline{a_{33}}\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №3: $R_3 \\gets R_3 - \\frac{a_{32}}{a_{22}} \\cdot R_2$\n\n$$\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 \\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 0\u0026\\underline{a_{33}}\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nВ результате получаем расширенную матрицу, записанную в ступенчатом виде. Ранее мы сказали, что в каждой ненулевой строке первый ненулевой элемент называется `ведущим`. В данном случае такими элементами будут: $a_{11}, a_{22}, a_{33}$.\n\nОстальные ненулевые элементы (переменные и коэффициенты при них) называются `свободными`. Также важно понимать, что в ходе операций со строками некоторые из ведущих элементов могли занулиться, — например, возможны такие случаи:\n\n$$\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 \\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 0\u00260\u0026 \\underline{a_{34}}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right),\n\\;\n\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 \\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 0\u00260\u00260\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\n\\underline{b_3}\\\\\n\\end{matrix}\n\\right).\n$$\n\nВ первом случае ведущими элементами будут $a_{11}, a_{22}, a_{34}$, а во втором — $a_{11}, a_{22}$. При этом после выполнения элементарных операций возможны и другие случаи, кроме приведённых выше.\n\n**Обратный ход** — после прямого хода матрица системы оказывается в `ступенчатом виде`, и теперь мы приводим её к более простому виду, который называют `улучшенным ступенчатым видом`. На этом этапе легко найти все решения или показать, что их нет.\n\nПрименяя прямой ход алгоритма, мы получили:\n\n$$\\left(\\left.\n\\begin{matrix}\n\\underline{a_{11}}\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026\\underline{a_{22}}\u0026a_{23}\u0026 a_{24}\\\\\n0\u00260\u0026\\underline{a_{33}}\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №1: $R_{i} \\gets R_{i} / a_{ii}  \\;(\\text{разделим }i\\text{-ю строку на ведущий элемент})$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 1\u0026a_{23}\u0026 a_{24}\\\\\n0\u00260\u00261\u0026a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №2: $R_2 \\gets R_2 - a_{23}\\cdot R_3$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 a_{12}\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 1\u00260\u0026 a_{24}\\\\\n0\u00260\u00261\u0026a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №3: $R_1 \\gets R_1 - a_{13} \\cdot R_3$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 a_{12}\u00260\u0026 a_{14}\\\\\n0\u0026 1\u00260\u0026 a_{24}\\\\\n0\u00260\u00261\u0026a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nДействие №4: $R_1 \\gets R_1 - a_{12} \\cdot R_2$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u00260\u0026 a_{14}\\\\\n0\u0026 1\u00260\u0026 a_{24}\\\\\n0\u0026 0\u00261\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n$$\n\nСтоит отметить, что при некоторых действиях какие-то коэффициенты могли занулиться. Но мы осознанно считали их ненулевыми, чтобы рассмотреть наиболее общий случай. Для двух других специальных случаев, полученных после прямого хода, мы бы получили соответственно:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u0026a_{13}\u0026 0\\\\\n0\u0026 1\u0026a_{23}\u0026 0\\\\\n0\u0026 0\u00260\u0026 1\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right),\n\\;\n\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 1\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 0\u00260\u00260\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n0\\\\\n0\\\\\n1\\\\\n\\end{matrix}\n\\right).\n$$\n\nОстался последний вопрос, который мы не раскрыли в алгоритме Гаусса, — это как из полученной матрицы в улучшенном ступенчатом виде получить решения системы.\n\n**Получение решений**\n\nВ полученной системе у нас есть свободная переменная — $x_4$, то есть мы можем назначить ей любое значение. Поэтому выберем переменную $x_4$ как параметр и выразим главные переменные через неё:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u00260\u0026 a_{14}\\\\\n0\u0026 1\u00260\u0026 a_{24}\\\\\n0\u0026 0\u00261\u0026 a_{34}\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + a_{14}x_4 = b_1\\\\\n\u0026x_2 + a_{24}x_4 = b_2\\\\\n\u0026x_3 + a_{34}x_4 = b_3\n\\end{aligned}\n\\end{cases}\n$$\n\nТогда решения имеют вид:\n\n$$\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\n\\end{pmatrix}\n-x_4\n\\begin{pmatrix}\na_{14}\\\\\na_{24}\\\\\na_{34}\n\\end{pmatrix}.\n$$\n\nДля двух других специальных случаев, полученных после обратного хода, мы бы получили соответственно:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u0026a_{13}\u0026 0\\\\\n0\u0026 1\u0026a_{23}\u0026 0\\\\\n0\u0026 0\u00260\u0026 1\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + a_{13}x_3 = b_1\\\\\n\u0026x_2 + a_{23}x_3 = b_2\\\\\n\u0026x_4 = b_3\n\\end{aligned}\n\\end{cases}\n$$\n\n$$\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\n\\end{pmatrix}\n-x_3\n\\begin{pmatrix}\na_{13}\\\\\na_{23}\\\\\n0\n\\end{pmatrix}.\n$$\n\nГлавные переменные —  $x_1, x_2, x_4$, а $x_3$ — свободная.\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\u0026a_{13}\u0026 a_{14}\\\\\n0\u0026 1\u0026a_{23}\u0026 a_{24}\\\\\n0\u0026 0\u00260\u00260\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n0\\\\\n0\\\\\n1\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + a_{13}x_3 + a_{14}x_4 = 0\\\\\n\u0026x_2 + a_{23}x_3 + a_{24}x_4= 0\\\\\n\u00260 = 1\n\\end{aligned}\n\\end{cases}\n$$\n\n$$0=1 \\implies \\text{Нет решений}.\n$$\n\nПолучили противоречие $0 = 1$, то есть система несовместна и решений нет.\n\n{% cut \"«Гаусс» в обучении нейросетей\" %}\n\nЕсть целый класс методов оптимизации нейросетей, которые учитывают не только градиенты, но и *вторые производные функции ошибки* — это так называемые методы второго порядка. К ним относятся, например, [Shampoo](https://arxiv.org/pdf/1802.09568).\n\nВ этих методах обновление весов сводится к решению системы линейных уравнений:\n\n$$H\\,\\Delta w = -g,\n$$\n\nгде $H$ — аппроксимированный гессиан (матрица вторых производных), $g$ — градиент, $\\Delta w$ — обновление весов. Прямая инверсия $H$ слишком дорогая, поэтому вместо этого:\n\n- строят факторизацию, например `LU`;\n- или выполняют одну-две итерации методов типа спрямлённого Гаусса — Зайделя.\n\nИ в том и в другом случае сначала матрицу приводят к ступенчатому или почти треугольному виду, чтобы быстрее найти решение. Так что даже в современных нейросетевых библиотеках (вроде `pytorch` или `tensorflow`) знание линейной алгебры помогает понимать, почему какие-то оптимизации работают, а другие — нет.\n\n{% endcut %}\n\nОбщее правило следующее:\n\n1. Если на этапе прямого хода получилась строка вида $(0 \\dots 0|b_i)$, где $b_i \\neq 0$, то **решений не существует**, система несовместна.\n2. Иначе (система совместна):\n   1. Если количество главных переменных равно количеству неизвестных, тогда получаем одно **единственное решение**, так как все переменные восстанавливаются однозначно.\n   2. Если есть свободные переменные, то **решений бесконечно много** — каждую свободную переменную можно выбрать произвольно, а остальные выразить через неё.\n\nДавайте рассмотрим по одному конкретному примеру для каждого случая.\n\n**Пример №1**\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + 2x_2 - x_3 = 1\\\\\n\u00262x_1 + 4x_2 - 2x_3 = 2\\\\\n\u0026x_1 +3x_2 - x_3 = 2\n\\end{aligned}\n\\end{cases}\n$$\n\nВыполним алгоритм Гаусса:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u00262\u0026-1\\\\\n2\u00264\u0026-2\\\\\n1\u00263\u0026-1\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n1\\\\\n2\\\\\n2\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\left(\\left.\n\\begin{matrix}\n1\u00260\u0026-1\\\\\n0\u00261\u00260\\\\\n0\u00260\u00260\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n-1\\\\\n1\\\\\n0\\\\\n\\end{matrix}\n\\right)\n$$\n\nЗапишем решение:\n\n$$\\begin{pmatrix}\nx_1\\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1\\\\\n1\n\\end{pmatrix}\n-x_3\n\\begin{pmatrix}\n-1\\\\\n0\n\\end{pmatrix}.\n$$\n\nПолучили, что $x_3$ является свободной переменной. Поэтому система имеет бесконечное множество решений.\n\n**Пример №2**\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + x_2 + x_3 = 6\\\\\n\u00262x_1 - x_2 + 3x_3 = 8\\\\\n\u0026-x_1 + 4x_2 + 2x_3 = 10\n\\end{aligned}\n\\end{cases}\n$$\n\nВыполним алгоритм Гаусса:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u00261\u00261\\\\\n2\u0026-1\u00263\\\\\n-1\u00264\u00262\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n6\\\\\n8\\\\\n10\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\left(\\left.\n\\begin{matrix}\n1\u00260\u00260\\\\\n0\u00261\u00260\\\\\n0\u00260\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n2\\\\\n2\\\\\n2\\\\\n\\end{matrix}\n\\right)\n$$\n\nЗапишем решение:\n\n$$\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2\\\\\n2\\\\\n2\n\\end{pmatrix}\n.\n$$\n\nПолучили, что количество главных переменных равно количеству неизвестных, поэтому все переменные восстанавливаются однозначно.\n\n**Пример №3**\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_1 + x_2 + x_3 = 3\\\\\n\u00262x_1 + 2x_2 + 2x_3 = 6\\\\\n\u0026x_1 + x_2 + x_3 = 4\n\\end{aligned}\n\\end{cases}\n$$\n\nВыполним алгоритм Гаусса:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u00261\u00261\\\\\n2\u00262\u00262\\\\\n1\u00261\u00261\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n3\\\\\n4\\\\\n6\\\\\n\\end{matrix}\n\\right)\n\\mapsto\n\\left(\\left.\n\\begin{matrix}\n1\u00261\u00261\\\\\n0\u00260\u00260\\\\\n0\u00260\u00260\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n3\\\\\n0\\\\\n1\n\\end{matrix}\n\\right)\n$$\n\nНа этапе прямого хода мы получили строку вида $\\left(\\left.\n\\begin{matrix}\n0\u00260\u00260\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n1\n\\end{matrix}\n\\right)$, то есть система несовместная, поэтому решений нет.\n\n### Алгоритм Гаусса в Python\n\nМы рассмотрели, как можно решать `СЛУ`, используя для этого алгоритм Гаусса. Давайте также разберёмся, как это можно делать в Python c помощью уже знакомых нам библиотек `numpy` и `scipy`.\n\n{% cut \"Вычисление в `numpy`\" %}\n\nДля решения СЛУ используется функция `np.linalg.solve`. Рассмотрим пример:\n\n```python\nimport numpy as np\n\n# Матрица коэффициентов\nA = np.array([\n    [1, 1, 1],\n    [2, -1, 3],\n    [-1, 4, 2]\n], dtype=float)\n\n# Вектор свободных членов\nb = np.array([6, 8, 10], dtype=float)\n\n# Решаем систему Ax = b\nx = np.linalg.solve(A, b)\n\nprint(\"Решение системы (x, y):\", x)\n\n# Output:\n# Решение системы: [2. 2. 2.]\n```\n\n{% endcut %}\n\n{% cut \"Вычисление в `scipy`\" %}\n\nВ `scipy` для решения СЛУ используется функция `scipy.linalg.solve`, рассмотрим пример:\n\n```python\nimport numpy as np\nimport scipy\n\n# Матрица коэффициентов\nA = np.array([\n    [1, 1, 1],\n    [2, -1, 3],\n    [-1, 4, 2]\n], dtype=float)\n\n# Вектор свободных членов\nb = np.array([6, 8, 10], dtype=float)\n\n# Решаем систему уравнений Ax = b\nx = scipy.linalg.solve(A, b)\n\nprint(\"Решение системы:\", x)\n\n# Output:\n# Решение системы: [2. 2. 2.]\n```\n\n{% endcut %}\n\nКак мы видим, код для решения `СЛУ` в этих двух библиотеках очень похожий и, более того, внутри библиотеки используют одни и те же алгоритмы из базовой библиотеки [LAPACK](https://github.com/Reference-LAPACK). Однако библиотека `scipy` в сравнении с numpy предлагает больше возможностей, например поддержка разреженных матриц (`scipy.sparse.linalg.spsolve`) или выбор [LAPACK-рутины](https://github.com/Reference-LAPACK/lapack) для соответствующего типа матрицы.\n\n---\n\nИтак, мы разобрались, как работает алгоритм Гаусса и как с его помощью можно находить решения систем линейных уравнений. Важно заметить, что, несмотря на простоту реализации данного алгоритма, вычислительно он является далеко не самым эффективным.\n\nПоэтому на практике при работе с большими объёмами данных алгоритм Гаусса обычно не используется в явном виде. Однако он очень тесно связан с другими более быстрыми алгоритмами (например, [LU-разложением](https://en.wikipedia.org/wiki/LU_decomposition#:~:text=LU%20decomposition%20can%20be%20viewed%20as%20the%20matrix%20form%20of%20Gaussian%20elimination)) и полезен для понимания работы с матрицами.\n\nВ следующем параграфе мы углубим понимание `СЛУ`: разберём LU-разложения и другие продвинутые методы решения, познакомимся с понятием обратимой матрицы и увидим, как современные библиотеки ускоряют расчёты, сохраняя при этом математическую точность.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13776304.403e19600e577f6a6bbfadc1914a1607e1f16144?iframe=1\" frameborder=\"0\" name=\"ya-form-13776304.403e19600e577f6a6bbfadc1914a1607e1f16144\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"67:Te9b8,"])</script><script nonce="">self.__next_f.push([1,"Теперь углубим понимание и перейдём к методам, которые лежат в основе практических вычислений: обратимые матрицы, LU‑разложение и разложение Холецкого. Все эти техники развивают идеи алгоритма Гаусса, позволяя быстрее и надёжнее решать задачи, где матрицы играют ключевую роль.\n\nА начнём мы с понятия обратимой матрицы — одного из фундаментальных кирпичиков линейной алгебры.\n\n### **Обратимые матрицы**\n\nКогда мы работаем с числами, то на операцию деления можно смотреть как на операцию умножения на обратный элемент. Рассмотрим два числа $a, b \\in \\mathbb R$ и $b \\neq 0$, тогда деление $a/b$ можно представить как $a \\cdot b^{-1}$, где $b^{-1}$ — обратный элемент для $b$, который удовлетворяет свойству $b \\cdot b^{-1}=1$.\n\nПодобным образом реализуется «деление» в матрицах, но в качестве аналога единицы выступает уже единичная матрица $E$. То есть для матрицы $B \\in \\operatorname{M}_{n} (\\mathbb R)$ матрица $B^{-1}$ называется `обратной`, если выполняется равенство $BB^{-1}=B^{-1}B=E$, а сама матрица $B$ в таком случае называется `обратимой`.\n\nСледовательно, «деление» матрицы $A \\in \\operatorname{M}_{n} (\\mathbb R)$ на матрицу $B$ (при условии, что она обратима) реализуется как умножение $A$ на $B^{-1}$. Формально операцию деления в матрицах вообще не вводят, поэтому пример выше нужен больше для интуитивного понимания. Также, в отличие от чисел, далеко не каждая матрица является `обратимой`.\n\n\u003e 💡Для произвольной матрицы $B \\in \\operatorname{M}_{n} (\\mathbb R)$ не гарантировано существование обратной матрицы, но если матрица $B$ обратима, то существует матрица $B^{-1}$, удовлетворяющая $BB^{-1}=B^{-1}B=E$, и эта матрица $B^{-1}$ единственна.\n\n{% cut \"Доказательство единственности обратной матрицы\" %}\n\nПусть $B_1 \\in \\operatorname{M}_{n} (\\mathbb R)$ и $B_2 \\in \\operatorname{M}_{n} (\\mathbb R)$ — различные обратные матрицы к матрице $B \\in \\operatorname{M}_{n} (\\mathbb R)$, то есть:\n\n$$B_1 \\neq B_2,\n\\\\\nBB_1=B_1B=E,\n\\\\\nBB_2=B_2B=E\n$$\n\nТогда рассмотрим равенство:\n\n$$BB_1=E\n$$\n\nУмножим его слева на $B_2$:\n\n$$B_2BB_1=B_2E\n$$\n\nПолучим противоречие:\n\n$$EB_1=B_2,\n\\\\\nB_1=B_2\n$$\n\nСледовательно:\n\n$$B_1=B_2=B^{-1}.\n$$\n\n{% endcut %}\n\n**Свойства:**\n\nПусть $A \\in \\operatorname{M}_{n} (\\mathbb R)$ является обратимой матрицей, тогда верно:\n\n1. $(A^{-1})^{-1}=A$\n2. $(AB)^{-1}=B^{-1}A^{-1}$\n3. $(A^t)^{-1}=(A^{-1})^{t}$\n4. $(kA)^{-1}=k^{-1}A^{-1},$ для $k \\neq 0$\n5. $E^{-1}=E$\n6. `СЛУ` $Ax=0$ имеет только нулевое решение $x \\in \\mathbb R^n$ . Данное свойство является эквивалентным, то есть если оно выполняется, то из этого следует, что и матрица $A$ обратима.\n\nСтоит отметить, что обратимую матрицу также называют `невырожденной` и обычно это определяется через некоторый инвариант — `определитель`, с ним мы познакомимся в следующем параграфе. Поскольку термины «обратимая» и «невырожденная» обозначают одно и то же свойство квадратной матрицы — существование обратной, — в дальнейшем мы будем использовать термин `обратимая матрица`.\n\nТеперь, когда мы поняли, как выглядят обратные матрицы и какими свойствами они обладают, самое время научиться их находить. В этом нам поможет алгоритм Гаусса, который мы [рассмотрели]() в предыдущем параграфе.\n\n**Нахождение обратной матрицы методом Гаусса**\n\nДанный алгоритм обычно не используется на практике для нахождения обратной матрицы, так как он менее эффективен в сравнении с другими методами, однако он достаточно простой для понимания и может быть полезен в учебных целях. Итак, пусть нам дана матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$ и мы хотим понять, обратима ли она, и если обратима, то найти матрицу $A^{-1}$.\n\nАлгоритм:\n\n1. На задачу поиска обратной матрицы можно смотреть как на поиск решения системы линейных уравнений вида: $AX=E$, где $X \\in \\operatorname{M}_{n} (\\mathbb R)$ — это искомая обратная матрица, а $E \\in \\operatorname{M}_{n} (\\mathbb R)$ — единичная матрица. Тогда составим расширенную матрицу системы вида $(A|E)$.\n2. Для матрицы $(A|E)$ выполним сперва прямой ход алгоритма Гаусса — приведем её к ступенчатому виду, а затем обратный ход алгоритма Гаусса — приведем её к улучшенному ступенчатому виду.\n3. После применения алгоритма Гаусса к матрице $(A|E)$ возможны два случая:\n   1. Получили матрицу вида $(E|B)$, тогда матрица $A$ обратима, а матрица $B=A^{-1}$.\n   2. Получили матрицу вида $(D|B)$, где $D \\neq E$ и у матрицы $D$ есть свободные переменные (нулевая строка). Тогда матрица $A$ не обратима.\n\nПрименим на примере. Рассмотрим матрицу $A \\in \\operatorname{M}_{2} (\\mathbb R)$:\n\n$$A=\n\\begin{pmatrix}\n2\u0026 3\\\\\n1\u0026 2\\\\\n\\end{pmatrix}\n$$\n\nЗапишем её расширенный вид:\n\n$$\\left(\\left.\n\\begin{matrix}\n2\u0026 3\\\\\n1\u0026 2\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n1\u0026 0\\\\\n0\u0026 1\\\\\n\\end{matrix}\n\\right)\n$$\n\nПоменяем строки между собой, так как удобнее выбрать в качестве первой строки ту, где ведущий элемент меньше, чтобы вычитать её из остальных строк:\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 2\\\\\n2\u0026 3\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n0\u0026 1\\\\\n1\u0026 0\\\\\n\\end{matrix}\n\\right)\n$$\n\nВыполним алгоритм Гаусса, сделав следующие элементарные преобразования:\n\nШаг №1: $R_2 \\gets R_2 - 2 \\cdot R_1$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 2\\\\\n0\u0026 -1\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n0\u0026 1\\\\\n1\u0026 -2\\\\\n\\end{matrix}\n\\right)\n$$\n\nШаг №2: $R_2 \\gets (-1) \\cdot R_2$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 2\\\\\n0\u0026 1\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n0\u0026 1\\\\\n-1\u0026 2\\\\\n\\end{matrix}\n\\right)\n$$\n\nШаг №3: $R_1 \\gets R_1 -2 \\cdot R_2$\n\n$$\\left(\\left.\n\\begin{matrix}\n1\u0026 0\\\\\n0\u0026 1\\\\\n\\end{matrix}\n\\:\\right|\\:\n\\begin{matrix}\n2\u0026 -3\\\\\n-1\u0026 2\\\\\n\\end{matrix}\n\\right)\n$$\n\nМы получили матрицу вида $(E|B)$, то есть матрица $A$ обратима и $B=A^{-1}$. Выполним проверку:\n\n$$AA^{-1} = \\begin{pmatrix}\n2\u0026 3\\\\\n1\u0026 2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n2\u0026 -3\\\\\n-1\u0026 2\\\\\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\u0026 0\\\\\n0\u0026 1\\\\\n\\end{pmatrix}=E\n$$\n\nИтак, мы рассмотрели, как с помощью алгоритма Гаусса можно решать системы линейных уравнений, а также находить обратные матрицы. Но есть более удобные способы, которые не требуют доводить матрицу до полного единичного вида. Один из них — `LU`-разложение, на котором мы теперь подробно остановимся.\n\n### LU-разложение\n\nДанный алгоритм заключается в разложении исходной матрицы $A \\in \\operatorname{M}_{n} (\\mathbb R)$ на произведение двух треугольных матриц: нижнетреугольной матрицы $L \\in \\operatorname{M}_{n} (\\mathbb R)$ и верхнетреугольной матрицы $U \\in \\operatorname{M}_{n} (\\mathbb R)$.\n\n$$A=LU\n$$\n\n**Cхема Дулитла**\n\nСуществуют различные виды самого `LU-разложения`, которые отличаются исходной структурой матриц $L$ и $U$. Наиболее популярной является схема Дулитла (Doolittle decomposition), где матрицы $L$ и $U$ имеют следующую структуру:\n\n$$L=\n\\left(\\left.\n\\begin{matrix}\n1\u00260\u0026\\ldots\u00260\\\\\nl_{21}\u00261\u0026\\ldots\u00260\\\\\n\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\nl_{n1}\u0026l_{n2}\u0026\\ldots\u00261\n\\end{matrix}\n\\right)\\right.\n,\n\\quad\nU=\n\\left(\\left.\n\\begin{matrix}\nu_{11}\u0026u_{12}\u0026\\ldots\u0026u_{1n}\\\\\n0\u0026u_{22}\u0026\\ldots\u0026u_{2n}\\\\\n\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\n0\u00260\u0026\\ldots\u0026u_{nn}\n\\end{matrix}\n\\right)\\right.\n$$\n\nЗаполнение единицами главной диагонали матрицы $L$ ($l_{ii} = 1$) позволяет уменьшить число вычисляемых коэффициентов. Данная схема используется также и в программных пакетах, например в `scipy.linalg.lu`.\n\n**Способы нахождения LU-разложения**\n\nРассмотрим конкретный пример. Пусть дана следующая матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$:\n\n$$A =\n\\begin{pmatrix}\n1\u0026 2\u0026 4\\\\\n3\u0026 8\u0026 14\\\\\n2\u0026 6\u0026 13\n\\end{pmatrix}\n$$\n\nТогда в общем виде матрицы $L$ и $U$ выглядят так:\n\n$$L =\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\nl_{21}\u0026 1\u0026 0\\\\\nl_{31}\u0026 l_{32}\u0026 1\n\\end{pmatrix},\n\\quad\nU =\n\\begin{pmatrix}\nu_{11}\u0026 u_{12}\u0026 u_{13}\\\\\n0\u0026 u_{22}\u0026 u_{23}\\\\\n0\u0026 0\u0026 u_{33}\n\\end{pmatrix}\n$$\n\nПеремножая $L$ и $U$, получаем следующую матрицу:\n\n$$LU=\n\\begin{pmatrix}\nu_{11}\u0026 u_{12}\u0026 u_{13}\\\\\nl_{21}u_{11}\u0026 l_{21}u_{12} + u_{22}\u0026 l_{21}u_{13} + u_{23}\\\\\nl_{31}u_{11}\u0026 l_{31}u_{12} + l_{32}u_{22}\u0026 l_{31}u_{13} + l_{32}u_{23} + u_{33}\n\\end{pmatrix}\n$$\n\nСразу можем найти значения следующих элементов:\n\n$$u_{11}=1, \\; u_{12}=2, \\; u_{13}=4\n$$\n\nДалее будем шаг за шагом находить значения всех последующих элементов, то есть:\n\n$$l_{21}u_{11}=3, \\; u_{11}=1 \\implies \\; l_{21}=3\n\\\\\nl_{21}u_{12}+u_{22}=8, \\; l_{21}=3, \\; u_{12}=2 \\; \\implies u_{22}=2\n$$\n\nИ так далее. Проделав последующие вычисления, получим матрицы:\n\n$$L =\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n3\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1\n\\end{pmatrix},\n\\quad\nU =\n\\begin{pmatrix}\n1\u0026 2\u0026 4\\\\\n0\u0026 2\u0026 2\\\\\n0\u0026 0\u0026 3\n\\end{pmatrix}\n$$\n\nВыполнив проверку, получим:\n\n$$A = LU =\n\\begin{pmatrix}\n1\u0026 2\u0026 4\\\\\n3\u0026 8\u0026 14\\\\\n2\u0026 6\u0026 13\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n3\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 2\u0026 4\\\\\n0\u0026 2\u0026 2\\\\\n0\u0026 0\u0026 3\n\\end{pmatrix}\n$$\n\nТеперь давайте вспомним рассмотренный алгоритм Гаусса и матрицы элементарных преобразований. На самом деле с их помощью также можно найти `LU`-разложение!\n\nКогда мы выполняем алгоритм Гаусса над матрицей (прямой ход), то путём элементарных преобразований ($g_i$) приводим её к ступенчатому верхнетреугольному виду $U$:\n\n$$A \\xrightarrow{g_1}\nA_1 \\xrightarrow{g_2}\n\\;\n\\dotso\n\\;\nA_{k-1} \\xrightarrow{g_k}\nU\n$$\n\nНо мы знаем, что любое элементарное преобразование можно представить как умножение слева на соответствующую матрицу $G_i$:\n\n$$U=G_k \\cdot G_{k-1} \\cdot \\dotso \\cdot G_2 \\cdot G_1 \\cdot A\n$$\n\nДалее давайте подробнее разберём, как можно задать матрицу $G_i$. Рассмотрим конкретный пример, чтобы логика действий была более наглядна. Пусть дана следующая матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$:\n\n$$A =\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1\n\\end{pmatrix}\n$$\n\nДля того чтобы прийти к верхнетреугольному ступенчатому виду, нам нужно поменять местами первую и вторую строки. Тогда матрица, задающая данное элементарное преобразование, выглядит так:\n\n$$P =\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n$$\n\nПолучим:\n\n$$A_1=PA=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1\n\\end{pmatrix}\n=\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1  \n\\end{pmatrix}\n$$\n\nДалее, выполняя элементарные преобразования, будем использовать только прибавление $j$ строки, умноженной на $\\lambda$, к $i$ строке. Тогда матрица, задающая данное элементарное преобразование, получается из единичной, где на позиции элемента $i$ строки и $j$ столбца стоит $\\lambda \\in \\mathbb R$, и в общем виде выглядит так:\n\n$$\\begin{array}{cc}\n  \u0026 \\quad\\quad j \\\\[2mm]\n\\begin{array}{c}\n  \\\\[1mm]\n  i \\\\[5mm]\n  \\\\\n\\end{array}\n\u0026\n\\begin{pmatrix}\n  1 \u0026 0 \u0026 \\ldots \u0026 0 \\\\\n  0 \u0026 \\ddots \u0026 \\lambda \u0026 \\vdots \\\\\n  \\vdots \u0026  \u0026 \\ddots \u0026 0 \\\\\n  0 \u0026 \\ldots \u0026 0 \u0026 1\n\\end{pmatrix}\n\\end{array}\n$$\n\nИ давайте добавим дополнительное условие $i \u003e j$, чтобы данная матрица была нижнетреугольной. То есть мы всегда будем прибавлять вышестоящие строки, умноженные на $\\lambda$, к нижестоящим.\n\nТогда, чтобы сделать следующий шаг алгоритма Гаусса, нам нужно занулить все элементы в столбце ведущего элемента, стоящие ниже него. Для рассматриваемого примера нужно прибавить первую строку, умноженную на $-2$, к третьей строке, что равносильно умножению слева на следующую матрицу $L_1$:\n\n$$L_{1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n-2\u0026 0\u0026 1  \n\\end{pmatrix},\n$$\n\n$$A_{2}=\nL_{1}A_{1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n-2\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1  \n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 -1\u0026 1  \n\\end{pmatrix}\n$$\n\nТеперь ведущим стал элемент $a_{22}$, поэтому матрицы $L_2$ и $A_3$ выглядят следующим образом:\n\n$$L_{2}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 1  \n\\end{pmatrix},\n$$\n\n$$A_{3}=\nL_{2}A_{2}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 -1\u0026 1  \n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n$$\n\nМы получили, что $A_3$ — уже верхнетреугольная ступенчатая матрица, и на этом прямой ход алгоритма завершен. Запишем его в матричном виде:\n\n$$U=\nL_{2} \\cdot L_{1} \\cdot P \\cdot A\n$$\n\nТеперь, если мы умножим обе части выражения на матрицу $L_{1}^{-1} \\cdot L_{2}^{-1}$, то получим:\n\n$$L_{1}^{-1} \\cdot L_{2}^{-1} \\cdot U\n=\nL_{1}^{-1} \\cdot L_{2}^{-1} \\cdot\nL_{2} \\cdot L_{1} \\cdot P \\cdot A\n\\\\\nL_{1}^{-1} \\cdot L_{2}^{-1} \\cdot U\n=\nP \\cdot A\n$$\n\nНо, строго говоря, мы ранее нигде не говорили, что матрицы $L_{1}^{-1}, L_{2}^{-1}$ обязательно существуют. Попытайтесь самостоятельно доказать, что они существуют, и подумать, как могут выглядеть эти матрицы, а потом загляните в ответ.\n\n{% cut \"Ответ\" %}\n\nДоказательство обратимости матриц элементарных преобразований основано на том, что каждая элементарная операция обратима. Рассмотрим на примере прибавления одной строки к другой с множителем, а для других элементарных преобразований можно будет показать по аналогии.\n\nПусть $S_{ij}(\\lambda) \\in \\operatorname{M}_{n} (\\mathbb R)$ — матрица, полученная из единичной, где на позиции элемента $i$ строки и $j$ столбца стоит $\\lambda \\in \\mathbb R$. Тогда умножение $A \\in \\operatorname{M}_{n \\thinspace m} (\\mathbb R)$ на $S_{ij}(\\lambda)$ слева прибавляет $j$ строку, умноженную на $\\lambda$, к $i$ строке матрицы $A$. Для того чтобы отменить данную операцию, нужно к $i$ строке матрицы $A$ прибавить $j$ строку, умноженную на $(-\\lambda)$. То есть достаточно просто поменять знак у $\\lambda$. Рассмотрим на примере с матрицей $L_1$, где \\$i=3, j=1, \\\\lambda=-\\$2:\n\n$$L_{1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n-2\u0026 0\u0026 1  \n\\end{pmatrix},\n\\;\nL_{1}^{-1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 0\u0026 1  \n\\end{pmatrix},\n$$\n\n$$L_{1}L_{1}^{-1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n-2\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 0\u0026 1  \n\\end{pmatrix}\n=\nE\n$$\n\n{% endcut %}\n\nНайдем матрицы  $L_{1}^{-1}$, $L_{2}^{-1}$ и их произведение:\n\n$$ L_{1}^{-1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 0\u0026 1  \n\\end{pmatrix},\n\\;\nL_{2}^{-1}=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 -1\u0026 1  \n\\end{pmatrix},\n$$\n\n$$L =\nL_{1}^{-1} \\cdot L_{2}^{-1} =\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 -1\u0026 1\n\\end{pmatrix}\n$$\n\nЗаметим, что $L_{1}^{-1} \\cdot L_{2}^{-1}$ — это также нижнетреугольная матрица, так как $L_{1}^{-1}$ и  $L_{2}^{-1}$ — это нижнетреугольные матрицы, а следовательно, и их произведение будет нижнетреугольной матрицей, то есть $L_{1}^{-1} \\cdot L_{2}^{-1} = L$. Тогда итоговое выражение, полученное вследствие прямого хода алгоритма Гаусса, выглядит так:\n\n$$L \\cdot U=P\\cdot A\n$$\n\nгде:\n\n- $L$ — нижнетреугольная матрица;\n- $U$ — верхнетреугольная матрица;\n- $P$ — матрица перестановки строк;\n- $A$ — исходная матрица.\n\nУмножив обе части равенства на $P^{-1}$, слева получим:\n\n$$P^{-1} \\cdot L \\cdot U=A\n$$\n\nПоэтому часто `LU`-разложение называют `PLU`-разложением, так как использование матрицы $P$ для перестановки строк помогает обойти случай, когда ведущий элемент оказался равен нулю. Поэтому `PLU`-разложение более универсально, и далее в качестве `LU`-разложения будем рассматривать именно его.\n\nНайдем итоговое разложение матрицы $A$:\n\n$$P^{-1}=P=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix},\n$$\n\n$$L =\nL_{1}^{-1} \\cdot L_{2}^{-1} =\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 -1\u0026 1\n\\end{pmatrix},\n$$\n\n$$U=\nL_{2} \\cdot L_{1} \\cdot P \\cdot A=\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix},\n$$\n\n$$P^{-1} \\cdot L \\cdot U=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n2\u0026 -1\u0026 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026 1\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 1\u0026 0\\\\\n2\u0026 1\u0026 1\n\\end{pmatrix}\n=\nA\n$$\n\nИтак, мы рассмотрели как можно найти `LU`-разложение. Теперь давайте попробуем понять, всегда ли можно представить матрицу в виде `LU`-разложения и если да, то единственное ли такое разложение.\n\n**Существование и единственность `LU`-разложения**\n\nКак мы выяснили выше, на данное разложение можно смотреть как на применение прямого хода алгоритма Гаусса, который, в свою очередь, приводит матрицу к ступенчатому виду.\n\nМожет так оказаться, что на каком-то этапе прямого хода ведущий элемент оказался равен нулю, тогда, выполнив перестановку строк, мы можем найти опорный элемент и продолжить выполнение алгоритма. Поэтому `PLU`-разложение существует всегда для любой квадратной матрицы.\n\nЕсли мы рассмотрим `LU`-разложение, то для любой обратимой диагональной матрицы $D$ можно записать:\n\n$$A=LU=(LD)(D^{-1}U)\n$$\n\nПри этом:\n\n- $LD$ — нижнетреугольная матрица;\n- $D^{-1}U$ — верхнетреугольная матрица\n\nТаким образом, мы получаем, что `LU`-разложение может быть не единственным. Однако в схеме Дулитла мы фиксируем диагональные элементы матрицы $L$ равными единице ($l_{ii} = 1$), что приводит к единственности разложения при условии, что матрица $A$ обратима.\n\n{% cut \"Доказательство\" %}\n\nПредположим, что существуют два разложения при условии, что $A$ — обратима:\n\n$$PA=L_1U_1=L_2U_2\n$$\n\nгде:\n\n- $L_1, L_2$ — нижнетреугольные матрицы с единицами на диагонали;\n- $U_1, U_2$ — верхнетреугольные матрицы.\n\nТогда умножим равенство $L_1U_1=L_2U_2$ слева на $L_1^{-1}$, а справа на $U_2^{-1}$:\n\n$$L_1^{-1}L_2=U_1U_2^{-1}\n$$\n\n- Левая часть $M=L_1^{-1}L_2$ — произведение двух нижнетреугольных матриц, причем обе имеют единицы на диагонали. Поэтому $M$ сама является нижнетреугольной матрицей с единицами на диагонали.\n- Правая часть $N=U_1U_2^{-1}$ — произведение двух верхнетреугольных матриц, причем у обеих на диагонали нет нулей, значит, и у $N$ диагональные элементы ненулевые, при этом все элементы ниже главной диагонали равны нулю.\n\nПолучается тождество\n\n$$M=N\n$$\n\nгде:\n\n- $M$ — нижнетреугольная с единицами на диагонали\n- $N$ — верхнетреугольная, у которой диагональные элементы не равны нулю\n\nЕдинственная матрица, одновременно нижнетреугольная и верхнетреугольная и с единицами на диагонали, — это единичная матрица $E$. Следовательно:\n\n$$L_1^{-1}L_2=E \\implies L_1=L_2,\n\\\\\nU_1U_2^{-1}=E \\implies U_1=U_2\n$$\n\nВ случае если матрица $A$ не обратима, то `LU`-разложение не единственно.\n\n{% endcut %}\n\n**Решение СЛУ**\n\nОчень хороший пример использования `LU`-разложения — это решение `СЛУ`. Основное преимущество данного разложения над методом Гаусса заключается в том, что, получив однажды представление исходной матрицы в виде произведения двух треугольных, мы можем очень быстро решать `СЛУ` при изменении правой части — вектора свободных членов $b$.\n\nКак мы помним, система в матричном виде задаётся как $Ax=b$. Тогда, воспользовавшись `PLU`-разложением, получаем:\n\n$$Ax=b,\\; PA=LU\n\\\\\nPAx=Pb\n\\\\\nLUx=Pb\n$$\n\nТеперь нам нужно решить систему $LUx=Pb$. Чтобы упростить задачу, введём промежуточный вектор $y=Ux$. Тогда нам нужно для решения исходной системы последовательно решить следующие две системы:\n\n$$Ly=Pb, \\; Ux=y\n$$\n\nСперва может показаться, что такой подход только усложняет решение задачи, ведь нам теперь нужно решить уже две системы вместо одной. Однако, вспомнив, как выглядят матрицы $L$ и $U$, можно заметить, что решения новых систем можно найти очень легко:\n\n- $L$ — нижнетреугольная матрица, поэтому для решения системы $Ly=Pb$ выполним прямую подстановку, то есть сперва найдём $y_1$, затем, зная $y_1$, найдём $y_2$ и так далее до $y_n$.\n- $U$ — верхнетреугольная матрица, поэтому для решения системы $Ux=y$ выполним обратную подстановку, то есть сперва найдём $x_n$, затем, зная $x_n$, найдём $x_{n-1}$ и так далее до $x_1$.\n\nРассмотрим конкретный пример, пусть дана следующая `СЛУ`:\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_2 + 2x_3 = 0\\\\\n\u002612x_1 + 4x_2 + 6x_3 = 14\\\\\n\u002612x_1 + 7x_2 + 9x_3 = 17\n\\end{aligned}\n\\end{cases}\n$$\n\nВыполним `PLU`-разложение и получим следующие матрицы:\n\n$$P=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix},\n\\;\nL=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n1\u0026 3\u0026 1  \n\\end{pmatrix},\n\\;\nU=\n\\begin{pmatrix}\n12\u0026 4\u0026 6\\\\\n0\u0026 1\u0026 2\\\\\n0\u0026 0\u0026 -3  \n\\end{pmatrix}\n$$\n\nРешим систему $Ly=Pb$:\n\n$$\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n1\u0026 3\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\ny_1\\\\\ny_2\\\\\ny_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n14\\\\\n0\\\\\n17\n\\end{pmatrix}\n$$\n\nВыполнив прямую подстановку, получим:\n\n- Из первого уравнения: $y_1=14$\n- Из второго уравнения: $y_2=0$\n- Из третьего уравнения: $y_1+3y_2+y_3=17 \\implies y_3=3$\n\nТаким образом:\n\n$$y\n=\n\\begin{pmatrix}\n14\\\\\n0\\\\\n3\n\\end{pmatrix}\n$$\n\nРешим систему $Ux=y$:\n\n$$\\begin{pmatrix}\n12\u0026 4\u0026 6\\\\\n0\u0026 1\u0026 2\\\\\n0\u0026 0\u0026 -3  \n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n14\\\\\n0\\\\\n3\n\\end{pmatrix}\n$$\n\nВыполнив обратную подстановку, получим:\n\n- Из третьего уравнения: $x_3=-1$\n- Из второго уравнения: $x_2=2$\n- Из первого уравнения: $12x_1+4x_2+6x_3=14 \\implies x_1=1$\n\nТаким образом, получаем решение:\n\n$$x\n=\n\\begin{pmatrix}\n1\\\\\n2\\\\\n-1\n\\end{pmatrix}\n$$\n\nИтак, мы рассмотрели, как с помощью `LU`-разложения можно решать `СЛУ`. Если требуется решить несколько систем с одной и той же матрицей коэффициентов AA, то вычисление `LU`-разложения (однократно) с последующей подстановкой для разных правых частей $b$ значительно экономит ресурсы.\n\n{% cut \"Применение в машинном обучении\" %}\n\nОдин из наиболее наглядных примеров — метод `гауссовских процессов` ([Gaussian Process Regression](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9_%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81), GPR). Это модель, которую часто используют для построения прогнозов вместе с оценкой неопределённости. Она считается довольно тяжёлой в вычислительном смысле, особенно на больших выборках.\n\nВот как выглядит её работа в самом простом случае:\n\n1. Вычисляется ковариационная матрица между объектами в обучающей выборке:\n\n$$K_\\theta + \\sigma^2 I \\in \\mathbb{R}^{n \\times n},\n$$\n\nгде $K_\\theta$ — матрица попарных похожестей объектов, а $\\sigma^2 I$ добавляет устойчивость к шуму.\n\n2. Затем на каждом шаге необходимо решить систему линейных уравнений:\n\n$$(K_\\theta + \\sigma^2 I)\\,\\alpha = y,\n$$\n\nчтобы получить вектор весов $\\alpha$, с помощью которого строится прогноз.\n\nИ вот здесь появляется знакомый приём: на практике используют [разложение Холецкого](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D0%A5%D0%BE%D0%BB%D0%B5%D1%86%D0%BA%D0%BE%D0%B3%D0%BE) или `LU`-разложение этой матрицы один раз, а затем просто дважды решают треугольную систему, ровно как мы только что делали с $Ly = Pb,\\; Ux = y$.\n\nЭто важно, так как такая процедура требует $O(n^3)$ времени и $O(n^2)$ памяти. Поэтому GPR плохо масштабируется на большие $n$ (например, больше 10 тысяч). Чтобы с этим справиться, используют приближения: *разреженные ядра*, *низкоранговые разложения*, *индуцированные точки* и так далее.\n\n{% endcut %}\n\nТеперь, когда мы увидели, как `LU`-разложение помогает в машинном обучении, перейдём к следующей важной задаче — `нахождению обратной матрицы`.\n\n**Нахождение обратной матрицы**\n\nПусть дана обратимая матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$ и известно разложение $PA=LU$. Для того чтобы найти $A^{-1}$, запишем её в виде набора столбцов и для каждого столбца решим систему уравнений:\n\n$$A^{-1}=\n\\begin{pmatrix}\nx_1\u0026x_2\u0026\\ldots\u0026 x_n\\\\\n\\end{pmatrix}\n\\\\\nE=\n\\begin{pmatrix}\ne_1\u0026e_2\u0026\\ldots\u0026 e_n\\\\\n\\end{pmatrix}\n$$\n\nгде $x_i$ — $i$ вектор-столбец матрицы $A^{-1}$, а $e_i$ — вектор-столбец матрицы $E$.\n\nДля каждого $i = 1, 2, \\dots, n$:\n\nЗаписывается система вида:\n\n$$Ax_i=e_i\n$$\n\nИспользуя PLU-разложение $PA=LU$, переписываем эту систему как:\n\n$$PAx_i=Pe_i,\n\\\\\nLUx_i=Pe_i\n$$\n\nДалее воспользуемся схемой применения `PLU`-разложения для решения `СЛУ`:\n\n1. Введем промежуточный вектор $y_i$ и решим систему:  $Ly_i=e_i$. Так как $L$ — нижнетреугольная матрица, то система решается прямой подстановкой.\n2. После нахождения $y_i$ решаем систему: $Ux_i=y_i$. Поскольку $U$ — верхнетреугольная матрица, то система решается обратной подстановкой.\n\nВ результате для каждого $i$ мы находим столбец $x_i$ обратной матрицы $A^{-1}$ .\n\nДавайте рассмотрим конкретный пример, пусть дана обратимая матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$, такая что:\n\n$$A=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 1\u0026 2\\\\\n0\u0026 3\u0026 1  \n\\end{pmatrix}\n$$\n\nВыполним `PLU`-разложение и получим следующие матрицы:\n\n$$P=\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 0\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix},\n\\;\nL=\n\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 3\u0026 1  \n\\end{pmatrix},\n\\;\nU=\n\\begin{pmatrix}\n1\u0026 1\u0026 2\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n$$\n\nНайдем векторы $Pe_i$:\n\n$$Pe_1=\n\\begin{pmatrix}\n0\\\\\n1\\\\\n0\n\\end{pmatrix}\n,\n\\;\nPe_2=\n\\begin{pmatrix}\n1\\\\\n0\\\\\n0\n\\end{pmatrix},\n\\;\nPe_3=\n\\begin{pmatrix}\n0\\\\\n0\\\\\n1\n\\end{pmatrix}\n$$\n\nНайдём $x_1$, для этого сперва решим систему $Ly=e_1$:\n\n$$\\begin{pmatrix}\n1\u0026 0\u0026 0\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 3\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\ny_1\\\\\ny_2\\\\\ny_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n1\\\\\n0\n\\end{pmatrix},\n\\\\~\\\\\ny=\n\\begin{pmatrix}\n0\\\\\n1\\\\\n-3\n\\end{pmatrix}\n$$\n\nТеперь решим систему $Ux_1=y$:\n\n$$\\begin{pmatrix}\n1\u0026 1\u0026 2\\\\\n0\u0026 1\u0026 0\\\\\n0\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\nx_{11}\\\\\nx_{12}\\\\\nx_{13}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n1\\\\\n-3\n\\end{pmatrix},\n\\\\~\\\\\nx_1=\n\\begin{pmatrix}\n5\\\\\n1\\\\\n-3\n\\end{pmatrix}\n$$\n\nПо аналогии найдём $x_2$ и $x_3$, получим:\n\n$$x_2=\n\\begin{pmatrix}\n1\\\\\n0\\\\\n0\n\\end{pmatrix},\n\\;\nx_3=\n\\begin{pmatrix}\n-2\\\\\n0\\\\\n1\n\\end{pmatrix}\n$$\n\nТеперь соберём обратную матрицу из найденных векторов и выполним проверку:\n\n$$A^{-1}=\n\\begin{pmatrix}\n5\u0026 1\u0026 -2\\\\\n1\u0026 0\u0026 0\\\\\n-3\u0026 0\u0026 1  \n\\end{pmatrix},\n\\\\~\\\\\nA^{-1}A=\n\\begin{pmatrix}\n5\u0026 1\u0026 -2\\\\\n1\u0026 0\u0026 0\\\\\n-3\u0026 0\u0026 1  \n\\end{pmatrix}\n\\begin{pmatrix}\n0\u0026 1\u0026 0\\\\\n1\u0026 1\u0026 2\\\\\n0\u0026 3\u0026 1  \n\\end{pmatrix}\n=\nE\n$$\n\nИтак, мы рассмотрели, как с помощью `PLU`-разложения можно не только решать системы линейных уравнений, но и находить обратные матрицы.\n\n{% cut \"Давайте теперь рассмотрим, как это можно делать, используя Python и библиотеки `numpy` и `scipy`.\" %}\n\n```python\n    import numpy as np\n    from scipy.linalg import lu, lu_factor, lu_solve, inv\n    \n    # Определяем матрицу A и правую часть b\n    A = np.array([[0, 1, 0],\n                  [1, 1, 2],\n                  [0, 3, 1]], dtype=float)\n    b = np.array([0, 14, 17], dtype=float)\n    \n    # 1. Вычисляем PLU-разложение с помощью lu_factor\n    lu_and_piv = lu_factor(A)\n    print(f\"Комбинированная матрица LU: \\n {lu_and_piv[0]}\")\n    print(f\"Индексы перестановок: \\n {lu_and_piv[1]}\")\n    \n    # 2. Решаем систему Ax = b, используя полученную факторизацию\n    x = lu_solve(lu_and_piv, b)\n    print(f\"Решение системы Ax = b: \\n {x}\")\n    \n    # 3. Вычисляем обратную матрицу A^{-1} с помощью scipy.linalg.inv\n    A_inv = inv(A)\n    print(f\"Найденная обратная матрица A^{-1}: \\n {A_inv}\")\n    \n    # Проверяем, что A * A_inv = E\n    res = np.allclose(np.eye(A.shape[0]), A @ A_inv)\n    print(f\"Проверка: 'A * A_inv = E' is {res}\")\n    \n    # 4. Матрицы P, L, U можно получить с помощью scipy.linalg.lu\n    P, L, U = lu(A)\n    print(f\"Перестановочная матрица P: \\n {P}\")\n    print(f\"Нижнетреугольная матрица L: \\n {L}\")\n    print(f\"Верхнетреугольная матрица U: \\n {U}\")\n    \n    # Проверяем, что A = PLU\n    res = np.allclose(A, P @ L @ U)\n    print(f\"Проверка: 'A = PLU' is {res}\")\n    \n    # Output:\n    # Комбинированная матрица LU: \n    #  [[ 1.          1.          2.        ]\n    #  [ 0.          3.          1.        ]\n    #  [ 0.          0.33333333 -0.33333333]]\n    # Индексы перестановок: \n    #  [1 2 2]\n    # Решение системы Ax = b: \n    #  [-20.   0.  17.]\n    # Найденная обратная матрица A^-1: \n    #  [[ 5.  1. -2.]\n    #  [ 1.  0.  0.]\n    #  [-3.  0.  1.]]\n    # Проверка: 'A * A_inv = E' is True\n    # Перестановочная матрица P: \n    #  [[0. 0. 1.]\n    #  [1. 0. 0.]\n    #  [0. 1. 0.]]\n    # Нижнетреугольная матрица L: \n    #  [[1.         0.         0.        ]\n    #  [0.         1.         0.        ]\n    #  [0.         0.33333333 1.        ]]\n    # Верхнетреугольная матрица U: \n    #  [[ 1.          1.          2.        ]\n    #  [ 0.          3.          1.        ]\n    #  [ 0.          0.         -0.33333333]]\n    # Проверка: 'A = PLU' is True\n```\n\nПояснение:\n\n- Функция `lu_factor` выполняет факторизацию $A$ с выбором главного элемента, возвращая матрицу, содержащую сразу $L$ (с единицами на диагонали) и $U$, а также массив индексов перестановок, что соответствует PLU‑разложению.\n- Функция `lu_solve` использует полученную факторизацию для решения системы $Ax=b$ с помощью последовательного решения систем $Ly=Pb$ и $Ux=y$.\n- Функция `inv` вычисляет обратную матрицу, решая $n$ систем $Ax=e_i$ (внутри используется PLU‑разложение).\n- Функция `lu` возвращает три матрицы $P$, $L$ и $U$, позволяющие увидеть явное представление PLU‑разложения $PA=LU$.\n\n{% endcut %}\n\nИтак, мы подробно рассмотрели `LU`-разложение и возможности его применения в различных задачах. На практике данный алгоритм достаточно популярен и используется в библиотеках `scipy` и `numpy` через [LAPACK‑рутины](https://www.netlib.org/lapack/). Например, [dgetrf](https://netlib.org/lapack/explore-html-3.6.1/dd/d9a/group__double_g_ecomputational_ga0019443faea08275ca60a734d0593e60.html) для факторизации и [dgetri](https://www.netlib.org/lapack/explore-html-3.6.1/dd/d9a/group__double_g_ecomputational_ga56d9c860ce4ce42ded7f914fdb0683ff.html) для вычисления обратной матрицы.\n\nОднако существуют случаи, когда структура матрицы позволяет упростить вычисления ещё больше. Если матрица симметрична и положительно определённая, для неё можно использовать разложение Холецкого — более быстрый и устойчивый метод.\n\nДавайте разберём, в чём его особенности и как его находить.\n\n### Разложение Холецкого\n\nВ анализе данных часто возникает задача эффективного и надёжного решения систем линейных уравнений, вычисления определителей и обратных матриц. Среди классических методов выделяются метод Гаусса и `LU`‑разложение, которые работают с произвольными обратимыми матрицами. Однако для специального класса матриц существует более экономный и устойчивый при численных расчётах метод — разложение Холецкого.\n\n**Применимость**\n\nДанный алгоритм заключается в разложении симметричной положительно определенной матрицы $A \\in \\operatorname{M}_{n} (\\mathbb R)$ на произведение двух треугольных матриц: нижнетреугольной матрицы $L \\in \\operatorname{M}_{n} (\\mathbb R)$ и верхнетреугольной матрицы $L^t \\in \\operatorname{M}_{n} (\\mathbb R)$, у которых диагональные элементы являются строго положительными.\n\n$$A=LL^t, \\; l_{ii}\u003e0\n$$\n\nДавайте рассмотрим, что такое симметричные положительно определённые матрицы. Для того чтобы дать определение положительной определённости, мы немного забежим вперёд и воспользуемся `определителем` (это числовая характеристика квад­рат­ной матрицы, обладающая рядом важных свойств и имеющая геометрический смысл), о котором мы поговорим в следующем параграфе.\n\n- **Симметричность.** Матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$ называется `симметричной`, если её элементы зеркально совпадают относительно главной диагонали:\n\n  $$A = A^t,\\; a_{ij} = a_{ji}\\;\\text{для всех }i,j\n  $$\n  \n  \n\n- **Положительная определённость.** Матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$ называется `положительно определённой`, если для каждого $k=1,2,\\dots,n$ определитель верхней левой $k \\times k$ подматрицы положителен:\n\n  $$\\Delta_k = \\det\\bigl(A_{1:k,1:k}\\bigr) \u003e 0.\n  $$\n  \n  Для $k=1$:\n\n  $$\\Delta_1 = a_{11} \u003e 0\n  $$\n  \n  Для $k=2$:\n\n  $$\\Delta_2 = \\det\n  \\begin{pmatrix}\n  a_{11} \u0026 a_{12}\\\\\n  a_{21} \u0026 a_{22}\n  \\end{pmatrix} \u003e 0\n  $$\n  \n  \n\n**Способ нахождения разложения Холецкого**\n\nДля нахождения элементов искомой матрицы $L$ определены рекуррентные формулы:\n\n$${\\ell}_{11}=\\sqrt{\\,a_{11}},\n\\\\~\\\\\n{\\ell}_{i1} = \\frac{a_{i1}}{\\ell_{11}},\\; i\\ge2,\n\\\\~\\\\\n{\\ell}_{ii} = \\sqrt{\\,a_{ii} - \\sum_{k=1}^{i-1} \\ell_{ik}^2}, \\; i\\ge2,\n\\\\~\\\\\n{\\ell}_{ij} = \\frac{1}{\\ell_{jj}}\\Bigl(a_{ij} - \\sum_{k=1}^{j-1} \\ell_{ik}\\,\\ell_{jk}\\Bigr),\\;\ni\\ge2, \\;\nj\\ge2, \\;\ni\u003ej.\n$$\n\nВычисление происходит сверху вниз, слева направо, то есть сперва $\\ell_{ij}$, а затем $\\ell_{ii}$. Давайте рассмотрим конкретный численный пример. Пусть дана матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$, такая что:\n\n$$A =\n\\begin{pmatrix}\n4 \u0026 12 \u0026 -16\\\\\n12 \u0026 37 \u0026 -43\\\\\n-16 \u0026 -43 \u0026 98\n\\end{pmatrix}\n$$\n\nТогда:\n\n1. ${\\ell}_{11} = \\sqrt{4} = 2$\n\n2. ${\\ell}_{21} = 12/2 = 6, \\;\n   {\\ell}_{22} = \\sqrt{37 - 6^2} = 1$\n\n3. $\\ell_{31} = -16/2 = -8, \\;\n   \\ell_{32} = (-43 - (-8)\\cdot6)/1 = 5, \\;\n   \\ell_{33} = \\sqrt{98 - (64+25)} = 3$\n\n$$L =\n\\begin{pmatrix}\n2 \u0026 0 \u0026 0\\\\\n6 \u0026 1 \u0026 0\\\\\n-8 \u0026 5 \u0026 3\n\\end{pmatrix},\n\\;\nLL^t = A\n$$\n\nВажно заметить, что разложение Холецкого может быть также использовано для решения `СЛУ` и нахождения обратной матрицы по аналогии с `LU`-разложением. Возникает логичный вопрос, а зачем тогда нужно ещё одно разложение?\n\nКак мы сказали ранее, разложение Холецкого может быть применено только для специального класса матриц, и оказывается, что для таких матриц оно работает быстрее, чем `LU`-разложение.\n\nТак, для получения разложения `LU`-разложения требуется порядка $\\frac{2}{3} n^3$ операций, а для  разложения Холецкого — $\\frac{1}{3} n^3$, то есть оно в два раза быстрее. Кроме того, меньшее количество операций ведёт к меньшим накопленным вычислительным погрешностям, что повышает точность и устойчивость полученного разложения.\n\n{% cut \"Вот как выглядит разложение Холецкого в Python с помощью библиотеки `numpy`.\" %}\n\n```python\n    import numpy as np\n    \n    # 1. Задаём A и b\n    A = np.array([[4,12,-16],[12,37,-43],[-16,-43,98]], dtype=float)\n    b = np.array([1,2,3], dtype=float)\n    \n    # 2. Разложение Холецкого\n    L = np.linalg.cholesky(A)  # A = L @ L.T\n    \n    # 3. Решение A x = b\n    y = np.linalg.solve(L, b)       # прямой ход\n    x = np.linalg.solve(L.T, y)     # обратный ход\n    \n    # 4. Обратная матрица и определитель\n    invA = np.linalg.inv(A)\n    \n    # 5. Оформление результатов\n    print('L (Cholesky):', L, sep='\\n')\n    print('y:', y, sep='\\n')\n    print('x:', x, sep='\\n')\n    print('A^{-1}:', invA, sep='\\n')\n    \n    # Output:\n    # L (Cholesky):\n    # [[ 2.  0.  0.]\n    #  [ 6.  1.  0.]\n    #  [-8.  5.  3.]]\n    # y:\n    # [ 0.5 -1.   4. ]\n    # x:\n    # [28.58333333 -7.66666667  1.33333333]\n    # A^{-1}:\n    # [[ 49.36111111 -13.55555556   2.11111111]\n    #  [-13.55555556   3.77777778  -0.55555556]\n    #  [  2.11111111  -0.55555556   0.11111111]]\n```\n\n{% endcut %}\n\nИтак, мы рассмотрели алгоритм разложения Холецкого, а также класс матриц, для которых его можно применять. На практике данный алгоритм относительно популярен и реализован в библиотеках `numpy` и `scipy`.\n\nСреди практических применений данного алгоритма можно выделить Гауссовские процессы. В библиотеке `scikit-learn` данное разложение [используется](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html) для вычисления $K^{-1}y$ и логарифма функции правдоподобия, а также [рассматривается](https://proceedings.mlr.press/v206/bartels23a/bartels23a.pdf?) его применение на больших данных.\n\nРазложение Холецкого, как и другие методы работы с системами линейных уравнений, важно не только с теоретической точки зрения, но и в практических задачах. Давайте теперь посмотрим, как системы линейных уравнений применяются в анализе данных.\n\n### **Применение СЛУ в анализе данных**\n\nОдним из классических примеров применения систем линейных уравнений является линейная регрессия. Модель записывается в виде:\n\n$$y=X \\beta+\\varepsilon\n$$\n\nгде:\n\n- $X$ — это матрица признаков размера $m \\times n$, где $m$ — это число наблюдений, а $n$ — число независимых переменных (включая, при необходимости, столбец единиц для учёта свободного члена);\n- $\\beta$ — вектор коэффициентов, который определяет вклад каждой переменной (признака) в модель;\n- $y$ — вектор зависимой переменной (предсказываемое значение);\n- $\\varepsilon$ — вектор случайных ошибок (шум); предполагается, что он имеет нулевое математическое ожидание и однородную дисперсию.\n\nЗадача состоит в нахождении такого вектора $\\beta$, при котором сумма квадратов отклонений предсказанных значений от наблюдаемых будет минимальна. Для этого формулируется следующий функционал ошибки:\n\n$$L(\\beta)= \\|y-X\\beta\\|^{2}=(y-X\\beta)^{t}(y-X\\beta)\n$$\n\nЧтобы минимизировать $L(\\beta)$, вычисляют его градиент по $\\beta$ и приравнивают к нулю:\n\n$$\\frac{\\partial{L}}{\\partial{\\beta}}=-2X^{t}(y-X\\beta)=0\n$$\n\nПолучаем следующее уравнение:\n\n$$X^{t}X\\beta=X^{t}y\n$$\n\nПри условии что матрица $X^{t}X$ обратима, решение находится аналитически:\n\n$$\\beta=(X^{t}X)^{-1}X^{t}y\n$$\n\nОднако на практике матрица $X^{t}X$ редко может быть обратима, поэтому используют альтернативные методы, такие как вычисление [псевдообратной матрицы](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pinv.html), [регуляризация](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression), [итеративные методы оптимизации](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) (например, стохастический градиентный спуск), которые не требуют вычисления обратной матрицы, или методы декомпозиции (например, [LU](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.inv.html)- и [SVD](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html)- разложения), что позволяет получить устойчивое и надёжное решение модели линейной регрессии.\n\nДавайте рассмотрим наглядный пример, как решение `СЛУ` может быть полезно в задаче анализа данных. Представьте, что вы работаете в компании по разработке новых сплавов. Вам поручено создать сплав, который должен иметь заданную плотность и прочность. Для этого необходимо комбинировать три базовых сплава A, B и C. Каждый из них характеризуется известными параметрами (плотностью и прочностью).\n\n#|\n||\n\nСплав\n\n|\n\nПлотность (г/см³)\n\n|\n\nПрочность (МПа)\n\n||\n||\n\nA\n\n|\n\n8\\.0\n\n|\n\n560\n\n||\n||\n\nB\n\n|\n\n7\\.8\n\n|\n\n580\n\n||\n||\n\nC\n\n|\n\n7\\.7\n\n|\n\n600\n\n||\n|#\n\nВаша цель — найти доли каждого базового сплава. Такие, чтоб итоговый сплав удовлетворял заданным требованиям:\n\n- плотность: 7.85 г/см³\n- прочность: 580 МПа\n\nЗапишем условие задачи в виде СЛУ. Для этого обозначим доли сплавов $A$, $B$, $C$ как $x_A$, $x_B$  и $x_C$ соответственно. По условию, сумма долей сплавов должна быть равна $1$, а итоговая плотность и прочность — $7.85$ и $580$ соответственно. Тогда получаем следующую СЛУ:\n\n$$\\begin{cases}\n\\begin{aligned}\n\u0026x_A + x_B + x_C = 1\\\\\n\u00268x_A + 7.8x_B + 7.7x_C = 7.85\\\\\n\u0026560x_A + 580x_B + 600x_C = 580\n\\end{aligned}\n\\end{cases}\n$$\n\nРанее мы уже разобрали, как можно решать СЛУ в Python:\n\n```python\nimport numpy as np\nimport scipy\n\n# Матрица коэффициентов\nA = np.array([\n    [1,     1,    1],\n    [8.0,  7.8,  7.7],\n    [560, 580, 600]\n])\n\n# Вектор свободных членов\nb = np.array([1, 7.85, 580])\n\n# Решаем систему уравнений Ax = b\nx = scipy.linalg.solve(A, b)\n\n# Вывод решения\nx_A, x_B, x_C = x\nprint(f\"Решение системы: x_A = {x_A:.4f}, x_B = {x_B:.4f}, x_C = {x_C:.4f}\")\n\n# Output:\n# Решение системы: x_A = 0.5000, x_B = 0.0000, x_C = 0.5000\n```\n\nПолучаем значения:\n\n$$x_A=0.5,\\; x_B=0,\\; x_C=0.5\n$$\n\nЭто означает, что для достижения требуемых параметров (целевой плотности 7.85 г/см³ и прочности 580 МПа) оптимально смешивать сплавы следующим образом:\n\n- 50 % сплава $A$\n- 0 % сплава $B$\n- 50 % сплава $C$\n\n---\n\nИтак, в этом параграфе мы познакомились с понятием обратимой матрицы, LU-разложения и увидели, как решения `СЛУ` могут быть применимы в практических задачах анализа данных. Но это только верхушка айсберга.\n\nВ машинном обучении линейная алгебра — это не просто формальность, а основа устойчивой и быстрой работы с моделями.\n\nВот что особенно важно помнить:\n\n- Умение быстро понять, сколько решений у `СЛУ`, — это способ диагностики болезней данных: мультиколлинеарности, недоопределённости, плохой обусловленности (condition number). Всё это может привести к нестабильным или бессмысленным результатам, если не распознать проблему вовремя.\n\n- Любая линейная модель в постановке наименьших квадратов в итоге приводит к решению уравнения:\n\n  $$X^t X\\beta = X^t y.\n  $$\n  \n  Золотым стандартом считаются решения через [Cholesky](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D0%A5%D0%BE%D0%BB%D0%B5%D1%86%D0%BA%D0%BE%D0%B3%D0%BE) или `LU`-разложение в связке с регуляризацией, особенно когда размерность данных растёт.\n\n- Даже в более сложных моделях — Gaussian Processes, GraphSAGE, PageRank — всё сводится к варианту одной задачи:\n\n$$\\text{Найти }x, \\text{ решив } Ax = b.\n$$\n\nЧем глубже вы понимаете структуру таких уравнений, тем проще вам будет работать как с ML-фреймворками, так и с задачами масштабирования на кластере.\n\nВ следующем параграфе мы рассмотрим определитель — числовой показатель, который раскрывает, как матрица искажает пространство и что она делает с объёмом данных.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13776316.feb70658ba7a36dc64a01e63f4e2e22cb2ac1d76?iframe=1\" frameborder=\"0\" name=\"ya-form-13776316.feb70658ba7a36dc64a01e63f4e2e22cb2ac1d76\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"68:T138b4,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы разберём понятие `определителя`. Обычно его рассматривают очень формально — как некоторую числовую характеристику матрицы.\n\nОднако у него есть куда более глубокое значение, которое можно понять, рассмотрев его геометрическую интерпретацию в векторном пространстве. Для этого мы построим наглядные интерактивные визуализации. Также разберём, как можно с помощью различных библиотек вычислять определитель.\n\n`Определитель` можно представить как числовой «коэффициент масштабирования», который показывает, во сколько раз линейное преобразование изменяет объём (или площадь) пространства, а также указывает на сохранение или изменение его ориентации.\n\n![Рисунок 4.5.1: Изменение пространства и площади до деформации и после](https://yastatic.net/s3/education-portal/media/4_6_1_dbe88a5498.webp \"Изменение пространства и площади до деформации и после\")\n\nЧтобы глубже понять это, давайте рассмотрим, как линейные преобразования действуют на отдельные векторы и на пространство в целом. Это поможет нам интуитивно увидеть связь между преобразованием и значением определителя.\n\n### Деформация пространства\n\nВ прошлом параграфе, когда говорили про матрицы, мы смотрели на них как на таблицу с числами. Однако в линейной алгебре у матриц есть куда более глубокий смысл — с их помощью можно задавать `линейные отображения`. В одном из следующих параграфов мы детально разберём это понятие, а пока сосредоточимся на эффекте, которое линейное отображение оказывает на геометрические объекты или пространство в целом, то есть на так называемой `линейной деформации`.\n\n**Линейная деформация векторов**\n\nДля наглядности будем работать в векторном пространстве $\\mathbb R^2$ со стандартным базисом. Прежде чем смотреть на деформацию всего пространства, давайте посмотрим, что происходит с вектором, когда мы действуем на него матрицей. В качестве примера возьмём вектор $v_1 \\in \\mathbb R^2$ и матрицу $A_1 \\in \\operatorname{M}_{2} (\\mathbb R)$, такие что:\n\n$$v_1 =\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2 \\\\\n2\n\\end{array}\n\\end{pmatrix}\n,\\quad\nA_1 =\n\\begin{pmatrix}\n\\begin{array}{r r}\n0\u0026 1 \\\\\n-1\u0026 0\n\\end{array}\n\\end{pmatrix}.\n\n$$\n\nУмножим вектор на матрицу слева и получим новый вектор $v_2 \\in \\mathbb R^2$ :\n\n$$v_2 =\nA_1 v_1 =\n\\begin{pmatrix}\n\\begin{array}{r r}\n0\u0026 1 \\\\\n-1\u0026 0\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2 \\\\\n2\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n2 \\\\\n2\n\\end{array}\n\\end{pmatrix}.\n\n$$\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://yastatic.net/s3/education-portal/media/4_6_2_53eb8a6b35.webp\" loading=\"lazy\" decoding=\"async\" alt=\"\"\u003e\n  \u003cfigcaption\u003e\n    Деформация вектора v\u003csub\u003e1\u003c/sub\u003e под действием умножения матрицы A\u003csub\u003e1\u003c/sub\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа рисунке выше мы видим, что исходный вектор $v_1$ повернулся на 270 градусов под действием умножения матрицы $A_1$. Данная матрица — это частный случай `матрицы поворота`, которая в двумерном пространстве имеет следующий вид:\n\n$$M(\\theta)=\n\\begin{pmatrix}\n\\cos{\\theta}\u0026 \\mp \\sin{\\theta} \\\\\n\\pm\\sin{\\theta}\u0026 \\cos{\\theta} \\\\\n\\end{pmatrix}\n,\\text{ где } \\theta \\text{ -- угол поворота}.\n$$\n\nРассмотрим теперь другой наглядный пример действия матричного умножения на исходный вектор. В качестве матрицы возьмем $A_2$:\n\n$$A_2 =\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2\u0026 0 \\\\\n0\u0026 -1\n\\end{array}\n\\end{pmatrix}\n$$\n\nУмножим вектор на матрицу слева и получим новый вектор $v_3 \\in \\mathbb R^2$ :\n\n$$v_3 =\nA_2 v_1 =\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2\u0026 0 \\\\\n0\u0026 -1\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2 \\\\\n2\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n4 \\\\\n-2\n\\end{array}\n\\end{pmatrix}.\n$$\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://yastatic.net/s3/education-portal/media/4_6_3_096cd9ae60.webp\" loading=\"lazy\" decoding=\"async\" alt=\"\"\u003e\n  \u003cfigcaption\u003e\n    Деформация вектора v\u003csub\u003e1\u003c/sub\u003e под действием умножения матрицы A\u003csub\u003e2\u003c/sub\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа рисунке выше мы видим, что исходный вектор $v_1$ трансформировался под действием умножения матрицы $A_2$: развернулся по обоим осям и растянулся в два раза по оси абсцисс. Данная матрица — это частный случай `матрицы растяжения`, которая в двумерном пространстве имеет следующий вид:\n\n$$C=\n\\begin{pmatrix}\n1/k_x\u0026 0 \\\\\n0\u0026 1/k_y \\\\\n\\end{pmatrix}\n,\\text{ где } k_x, k_y \\text{ - коэффициенты растяжения/сжатия}.\n$$\n\nМы рассмотрели как с помощью матрицы можно задавать деформацию вектора, а также увидели, какие есть наглядные примеры таких деформаций. Давайте пойдём дальше и попробуем понять, как матрица действует на всё множество векторов, то есть на векторное пространство в целом.\n\n**Линейная деформация пространства**\n\nЗная, как матрица действует на отдельные векторы, теперь мы можем проследить, как деформируется всё пространство. Для этого нам нужно построить все возможные векторы из данного пространства и к каждому из них применить деформацию, задаваемую некоторой матрицей.\n\nПрименяя матрицу ко всем векторам, мы получаем целостное представление о том, как изменяются форма, объём и ориентация всего пространства. Представить, что в таком случае произойдёт с пространством, может быть достаточно сложно, поэтому попробуем визуализировать это схематично. Для наглядности также будем работать в векторном пространстве $\\mathbb R^2$.\n\n- Шаг 1: представим каждый вектор пространства не как стрелку, а как точку, которая соответствует конечной точке вектора.\n- Шаг 2: множество всех таких точек можно представить как новую координатную сетку, наложенную на исходную.\n- Шаг 3: применим деформацию ко всем векторам пространства, тогда каждая точка будет двигаться в какую-то другую точку на плоскости, а новая координатная сетка — деформироваться. При этом также сохраним и исходную координатную сетку, чтобы понять, как подействовала деформация относительно начального положения.\n\n![Рисунок 4.5.4: Схематичная визуализация деформации векторного пространства](https://yastatic.net/s3/education-portal/media/4_6_4_d705d6a41e.webp \"Схематичная визуализация деформации векторного пространства\")\n\nТаким образом, применяя линейное преобразование ко всем векторам пространства, мы не просто изменяем отдельные точки, а получаем новое представление о том, как преобразуется всё пространство: его форма, ориентация и масштаб. Это позволяет нам перейти от локального понимания действия матрицы к глобальному — увидеть, как матрица изменяет само полотно пространства.\n\n\u003e 💡В линейной алгебре множество допустимых деформаций не является произвольным: все они должны удовлетворять определённым свойствам. Более строго эти свойства будут сформулированы при рассмотрении линейных отображений.\n\u003e\n\u003e Сейчас же нам важнее утверждения, которые следуют из этих свойств. При деформации пространства:\n\u003e\n\u003e 1. Все линии координатной сетки остаются параллельными и на равных расстояниях.\n\u003e 2. Начало координат зафиксировано и не меняется.\n\nЧтобы вы смогли более наглядно пронаблюдать, как изменяется заданный вектор и всё пространство в целом под действием деформации, мы подготовили код с интерактивной визуализацией с помощью библиотек `plotly` и `streamlit`.\n\n{% cut \"Код для визуализации линейной деформации двумерного пространства\" %}\n\nДля запуска:\n\n1. Создайте `.py` файл, например `app.py`.\n2. Вставьте туда написанный ниже код.\n3. Установите необходимые зависимости.\n4. Запустите приложение командой: `streamlit run app.py`\n\n```python\nfrom typing import Any\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport streamlit as st\n\n\ndef create_figure(a: float, b: float, c: float, d: float, \n                  vector: np.ndarray, grid_range: int = 5, grid_step: int = 1\n                 ) -\u003e go.Figure:\n    \"\"\"\n    Создаёт фигуру с двумя подграфиками (1x2):\n\n    1) Исходное пространство с заданным вектором v.\n    2) Деформированное пространство с образом вектора v'.\n\n    Параметры:\n        a, b, c, d (float): Элементы матрицы A = [[a, b], [c, d]].\n        vector (np.ndarray): Исходный вектор (длина 2).\n        grid_range (int): Диапазон значений координатной сетки.\n        grid_step (int): Шаг между линиями сетки.\n\n    Возвращает:\n        go.Figure: Фигура с двумя подграфиками.\n    \"\"\"\n    xs: np.ndarray = np.arange(-grid_range, grid_range + grid_step, grid_step)\n    ys: np.ndarray = np.arange(-grid_range, grid_range + grid_step, grid_step)\n    matrix: np.ndarray = np.array([[a, b], [c, d]])\n\n    fig: go.Figure = make_subplots(\n        rows=1,\n        cols=2,\n        subplot_titles=[\"Исходное пространство\", \"Деформированное пространство\"],\n        horizontal_spacing=0.02\n    )\n\n    def add_vector_annotation(x: float, y: float, xref: str, yref: str, \n                              color: str, label: str,\n                              row: int = 1, col: int = 1, \n                              text_xshift: int = 10, text_yshift: int = -10) -\u003e None:\n        \"\"\"Добавляет стрелку и подпись в указанном подграфике.\"\"\"\n        fig.add_annotation(\n            x=x, y=y,\n            ax=0, ay=0,\n            xref=xref, yref=yref,\n            axref=xref, ayref=yref,\n            showarrow=True,\n            arrowhead=3,\n            arrowcolor=color,\n            arrowsize=1,\n            arrowwidth=2,\n            text=\"\",\n            row=row,\n            col=col\n        )\n        fig.add_annotation(\n            x=x, y=y,\n            xref=xref, yref=yref,\n            text=label,\n            showarrow=False,\n            xshift=text_xshift,\n            yshift=text_yshift,\n            row=row,\n            col=col\n        )\n\n    # Исходное пространство\n    for x_val in xs:\n        fig.add_trace(\n            go.Scatter(\n                x=[x_val, x_val],\n                y=[-grid_range, grid_range],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n    for y_val in ys:\n        fig.add_trace(\n            go.Scatter(\n                x=[-grid_range, grid_range],\n                y=[y_val, y_val],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n    add_vector_annotation(1, 0, \"x\", \"y\", \"red\", \"i\", row=1, col=1)\n    add_vector_annotation(0, 1, \"x\", \"y\", \"green\", \"j\", row=1, col=1)\n    add_vector_annotation(vector[0], vector[1], \"x\", \"y\", \"blue\", \"v\", row=1, col=1)\n\n    # Деформированное пространство\n    for x_val in xs:\n        line_y = np.linspace(-grid_range, grid_range, 100)\n        original_points = np.array([[x_val, y_val] for y_val in line_y])\n        transformed = original_points @ matrix.T\n        fig.add_trace(\n            go.Scatter(\n                x=transformed[:, 0],\n                y=transformed[:, 1],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n    for y_val in ys:\n        line_x = np.linspace(-grid_range, grid_range, 100)\n        original_points = np.array([[x_val, y_val] for x_val in line_x])\n        transformed = original_points @ matrix.T\n        fig.add_trace(\n            go.Scatter(\n                x=transformed[:, 0],\n                y=transformed[:, 1],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n\n    v_prime: np.ndarray = matrix @ vector\n    add_vector_annotation(v_prime[0], v_prime[1], \"x2\", \"y2\", \"blue\", \"v'\", row=1, col=2)\n    i_prime: np.ndarray = matrix @ np.array([1, 0])\n    j_prime: np.ndarray = matrix @ np.array([0, 1])\n    add_vector_annotation(i_prime[0], i_prime[1], \"x2\", \"y2\", \"red\", \"i'\", row=1, col=2)\n    add_vector_annotation(j_prime[0], j_prime[1], \"x2\", \"y2\", \"green\", \"j'\", row=1, col=2)\n\n    fig.update_layout(\n        width=1000,\n        height=500,\n        margin=dict(l=0, r=0, t=50, b=50),\n        xaxis=dict(\n            domain=[0, 0.45],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"y\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        yaxis=dict(\n            domain=[0, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"x\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        xaxis2=dict(\n            domain=[0.55, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"y2\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        yaxis2=dict(\n            domain=[0, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"x2\",\n            scaleratio=1,\n            constrain=\"domain\"\n        )\n    )\n\n    return fig\n\ndef main() -\u003e None:\n    \"\"\"Главная функция приложения Streamlit.\"\"\"\n    st.title(\"Интерактивная визуализация линейной деформации\")\n\n    st.subheader(\"Коэффициенты матрицы линейного отображения\")\n    col1, col2 = st.columns(2)\n    a: float = col1.number_input(\"a:\", value=1.0, step=0.1)\n    b: float = col2.number_input(\"b:\", value=0.0, step=0.1)\n    col3, col4 = st.columns(2)\n    c: float = col3.number_input(\"c:\", value=0.0, step=0.1)\n    d: float = col4.number_input(\"d:\", value=1.0, step=0.1)\n\n    st.subheader(\"Координаты исходного вектора\")\n    v1: float = st.number_input(\"v₁:\", value=2.0, step=0.1)\n    v2: float = st.number_input(\"v₂:\", value=1.0, step=0.1)\n    vector: np.ndarray = np.array([v1, v2])\n\n    fig: go.Figure = create_figure(a, b, c, d, vector)\n    st.plotly_chart(fig, use_container_width=True)\n\nif __name__ == '__main__':\n    main()\n    \n```\n\n{% endcut %}\n\nИтак, теперь у нас есть визуальная интуиция, которая поможет лучше понять линейные деформации: как они изменяют и отдельные векторы, и всё пространство в целом. Таким образом, у нас есть все необходимые знания, чтобы понять, какой смысл скрывается за определителем.\n\n### Геометрический смысл определителя\n\nТеперь давайте попробуем разобраться, какой именно геометрический смысл скрывается за понятием определителя.\n\nМы уже видели, как матрица деформирует пространство, — теперь настало время количественно оценить результат этой деформации. Для этого мы рассмотрим, как меняется площадь при линейном преобразовании, и выясним, какую роль в этом играет определитель.\n\n**Изменение площади пространства при деформации**\n\nРассматривая примеры выше, вы могли заметить, что какие-то из деформаций сжимают исходное пространство, а другие —\u0026nbsp;наоборот, растягивают его. Поэтому, чтобы лучше понять, как действует та или иная деформация, хочется уметь оценивать коэффициент изменения исходной площади пространства под действием деформации.\n\nЭтот коэффициент по модулю как раз и будет равен `определителю`. Чтобы его найти, давайте рассмотрим единичный квадрат, натянутый на векторы нашего базиса $i$ и $j$:\n\n![Рисунок 4.5.5: Единичный квадрат, натянутый на стандартный базис](https://yastatic.net/s3/education-portal/media/4_6_5_832537a9be.webp \"Единичный квадрат, натянутый на стандартный базис\")\n\nИзменение площади данного квадрата будет отражать изменение площади любой области и всего пространства в целом. Пространство $\\mathbb{R}^2$ можно полностью разложить на непересекающиеся единичные квадраты (как клетки бесконечной клетчатой сетки). Так как линейная деформация изменяет площадь одного такого квадрата, то благодаря линейности преобразования площадь каждого квадрата изменится одинаково.\n\nТак происходит потому, что мы рассматриваем только линейные деформации пространства, а следовательно, как было сказано выше, все линии координатной сетки остаются параллельными и на равных расстояниях. Поэтому площадь любого измеримого множества, которое можно аппроксимировать такой сеткой, масштабируется тем же коэффициентом, что и площадь единичного квадрата.\n\nТеперь давайте рассмотрим несколько примеров деформаций и проанализируем, как меняется площадь.\n\n**Растяжение пространства**\n\nВ качестве примера возьмём следующую матрицу $C$:\n\n$$C=\n\\begin{pmatrix}\n2\u0026 0 \\\\\n0\u0026 3 \\\\\n\\end{pmatrix}\n$$\n\nМы ранее уже рассмотрели `матрицы растяжения` и увидели, что происходит с векторами при линейной деформации пространства. Поэтому найдём новые координаты наших базисных векторов $i$ и $j$:\n\n$$i' =\nC i =\n\\begin{pmatrix}\n\\begin{array}{r r}\n2\u0026 0 \\\\\n0\u0026 3\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n2 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n$$\n\n$$j' =\nC j =\n\\begin{pmatrix}\n\\begin{array}{r r}\n2\u0026 0 \\\\\n0\u0026 3\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n1\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n3\n\\end{array}\n\\end{pmatrix}\n.\n$$\n\nТеперь отобразим исходный единичный квадрат и полученный прямоугольник:\n\n![Рисунок 4.5.6: Растяжение пространства](https://yastatic.net/s3/education-portal/media/4_6_6_8073ca45fb.webp \"Растяжение пространства\")\n\nПри линейной деформации пространства исходный единичный квадрат может перейти в параллелограмм, отрезок или точку. Напомним формулу для подсчёта площади параллелограмма: $S=bh$, где $b$ — сторона параллелограмма, а $h$ — высота, проведённая к этой стороне. Тогда после деформации пространства матрицей $C$ коэффициент изменения площади равен: $2 \\times 3= 6$.\n\n**Отражение пространства**\n\nТеперь добавим к рассмотренному выше растяжению пространства его отражение. Возьмём следующую матрицу $M$:\n\n$$M=\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2\u0026 0 \\\\\n0\u0026 2 \\\\\n\\end{array}\n\\end{pmatrix}.\n$$\n\nНайдём новые координаты наших базисных векторов $i$ и $j$:\n\n$$i' =\nM i =\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2\u0026 0 \\\\\n0\u0026 2\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n$$\n\n$$j' =\nM j =\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2\u0026 0 \\\\\n0\u0026 2\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n1\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n2\n\\end{array}\n\\end{pmatrix}.\n$$\n\nТеперь отобразим исходный единичный квадрат, а также полученный прямоугольник:\n\n![Рисунок 4.5.7: Поворот пространства](https://yastatic.net/s3/education-portal/media/4_6_7_6dd2b4cddb.webp \"Поворот пространства\")\n\nПо аналогии с предыдущим примером рассчитаем коэффициент изменения площади: $2 \\times 2 = 4$. Однако в этом примере стоит обратить внимание на то, что наши базисные векторы тоже изменили свою ориентацию в пространстве.\n\nЕсли мы представим рассматриваемую плоскость как белый лист бумаги, на котором отложены наши базисные векторы, то, чтобы поменять направление базисных векторов так, как показано на рисунке выше, нам обязательно придётся перевернуть лист на другую сторону — можете проделать данное упражнение. Также можно смотреть на это и иначе: в исходном пространстве вектор $j$ находится слева от вектора $i$; если после деформации пространства вектор $j$ оказывается справа от вектора $i$, это означает, что ориентация пространства была обращена.\n\nТаким образом, при изучении деформаций пространства полезно понимать не только как изменяется площадь, но и происходит ли изменение ориентации. Выше мы сказали, что коэффициент изменения исходной площади пространства под действием деформации по модулю равен `определителю`. Если учесть смену ориентации как изменение знака коэффициента, то получим в точности значение `определителя`. То есть в примере выше `определитель` будет равен $-4$.\n\n**Скос пространства**\n\nВозьмём следующую матрицу $B$:\n\n$$B=\n\\begin{pmatrix}\n\\begin{array}{r r}\n1\u0026 1 \\\\\n0\u0026 1 \\\\\n\\end{array}\n\\end{pmatrix}.\n$$\n\nНайдём новые координаты наших базисных векторов $i$ и $j$:\n\n$$i' =\nB i =\n\\begin{pmatrix}\n\\begin{array}{r r}\n1\u0026 1 \\\\\n0\u0026 1\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n0\n\\end{array}\n\\end{pmatrix},\n$$\n\n$$j' =\nB j =\n\\begin{pmatrix}\n\\begin{array}{r r}\n1\u0026 1 \\\\\n0\u0026 1\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n1\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n1\n\\end{array}\n\\end{pmatrix}.\n$$\n\nТеперь отобразим исходный единичный квадрат, а также полученный параллелограмм:\n\n![Рисунок 4.5.8: Скос пространства](https://yastatic.net/s3/education-portal/media/4_6_8_8517ebbed9.webp \"Скос пространства\")\n\nВ данном случае легко заметить, что изменения площади не произошло, и после деформации она по-прежнему равна $1$.\n\n- Связь с нормализующими потоками в глубинном обучении\n\n  В задачах глубинного обучения можно отметить интересную связку с определителем. В современных генеративных моделях, таких как **[нормализующие потоки](https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki)** (например, RealNVP или Glow), используется формула изменения плотности:\n\n  $$\\log p(x) = \\log p(z) - \\sum \\log |\\det J_f|\n  $$\n  \n  где $J_f$ — якобиан одного слоя преобразования. Когда преобразование сохраняет объём (то есть $|\\det J_f| = 1$), оно становится проще и эффективнее в вычислениях. Такие преобразования называются volume-preserving.\n\n  Примеры: поворот, скос, ортогональные матрицы — всё это деформации, при которых определитель равен $±1$. Они активно используются в слоях типа *affine coupling* или $1 \\times 1$-свёртках, параметризуемых с помощью LU-разложений. Например, в PyTorch логарифм определителя можно быстро посчитать так:\n\n  ```python\n  P, L, U = torch.linalg.lu(W)  # W — матрица свёртки\n  logdet = torch.sum(torch.log(torch.abs(torch.diag(U))))\n  ```\n\n\u003e 👉🏽 А значит, снова срабатывает хорошо знакомое правило: если матрица треугольная — определитель равен произведению диагонали.\n\n**Сплющивание пространства**\n\nВозьмём следующую матрицу $F$:\n\n$$F=\n\\begin{pmatrix}\n\\begin{array}{r r}\n2\u0026 -2 \\\\\n2\u0026 -2 \\\\\n\\end{array}\n\\end{pmatrix}.\n$$\n\nНайдём новые координаты наших базисных векторов $i$ и $j$:\n\n$$i' =\nF i =\n\\begin{pmatrix}\n\\begin{array}{r r}\n2\u0026 -2 \\\\\n2\u0026 -2\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n1 \\\\\n0\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n2 \\\\\n2\n\\end{array}\n\\end{pmatrix},\n$$\n\n$$j' =\nF j =\n\\begin{pmatrix}\n\\begin{array}{r r}\n2\u0026 -2 \\\\\n2\u0026 -2\n\\end{array}\n\\end{pmatrix}\n\n\\begin{pmatrix}\n\\begin{array}{r r}\n0 \\\\\n1\n\\end{array}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{array}{r r}\n-2 \\\\\n-2\n\\end{array}\n\\end{pmatrix}.\n$$\n\nТеперь отобразим исходный единичный квадрат и полученный отрезок:\n\n![Рисунок 4.5.9: Сплющивание пространства](https://yastatic.net/s3/education-portal/media/4_6_9_cdb119484c.webp \"Сплющивание пространства\")\n\nМы видим, что в данном случае единичный квадрат, натянутый на базисные векторы, сплющило в отрезок. Это важный пример, который показывает, как сильно линейная деформация может изменить пространство. Тут полезно вспомнить про рассмотренную ранее линейную независимость и внимательно посмотреть на векторы, которые получаются после деформации. В данном примере `определитель` равен нулю.\n\n**Пояснение**\n\nВыше мы везде считали определитель как изменение площади единичного квадрата при деформации. Это было сделано для лучшего визуального понимания. В действительности мы можем считать определитель как ориентированную площадь (учитывая знак, который показывает, сохраняется или меняется ориентация) параллелограмма, натянутого на векторы, которые уложены как векторы-столбцы в матрицу $A$, задающую деформацию:\n\n$$A =\n(A_1\\mid \\ldots\\mid A_n)\\in\n\\operatorname{M}_{n} (\\mathbb R),\n\\quad\nA_{i} \\in \\mathbb{R}^n.\n\n$$\n\nЕсли мы подействуем матрицей $A$ на наши базисные векторы, то полученные образы этих векторов и будут в точности столбцы матрицы $A$. Вы могли убедиться в этом на примерах, рассмотренных выше.\n\nЧтобы вы смогли более наглядно увидеть, как под действием деформации изменяется пространство и базисные векторы, а также чему равен определитель, мы подготовили код с интерактивной визуализацией посредством библиотек `plotly` и `streamlit`.\n\n{% cut \"Код для визуализации линейной деформации двумерного пространства и изменения площади\" %}\n\nАлгоритм запуска кода [тот же](https://prestable.education.yandex.ru/handbook/math/article/math-%7C-4.6-opredelitel#:~:text=%D0%B8%20%D0%BD%D0%B5%20%D0%BC%D0%B5%D0%BD%D1%8F%D0%B5%D1%82%D1%81%D1%8F.-,%D0%A7%D1%82%D0%BE%D0%B1%D1%8B%20%D0%B2%D1%8B%20%D1%81%D0%BC%D0%BE%D0%B3%D0%BB%D0%B8,-%D0%B1%D0%BE%D0%BB%D0%B5%D0%B5%20%D0%BD%D0%B0%D0%B3%D0%BB%D1%8F%D0%B4%D0%BD%D0%BE%20%D0%BF%D1%80%D0%BE%D0%BD%D0%B0%D0%B1%D0%BB%D1%8E%D0%B4%D0%B0%D1%82%D1%8C).\n\n```python\nimport streamlit as st\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom typing import Any\n\ndef create_figure(a: float, b: float, c: float, d: float, grid_range: int = 5, grid_step: int = 1\n                 ) -\u003e go.Figure:\n    \"\"\"\n    Создаёт фигуру с двумя подграфиками (1x2):\n\n    1) Исходное пространство и единичный квадрат.\n    2) Деформированное пространство с образом единичного квадрата.\n\n    Параметры:\n        a, b, c, d (float): Элементы матрицы A = [[a, b], [c, d]].\n        grid_range (int): Диапазон значений координатной сетки.\n        grid_step (int): Шаг между линиями сетки.\n\n    Возвращает:\n        go.Figure: Фигура с двумя подграфиками.\n    \"\"\"\n    # Создаём сетку координат\n    xs: np.ndarray = np.arange(-grid_range, grid_range + grid_step, grid_step)\n    ys: np.ndarray = np.arange(-grid_range, grid_range + grid_step, grid_step)\n    matrix: np.ndarray = np.array([[a, b], [c, d]])\n\n    # Создаём подграфики\n    fig: go.Figure = make_subplots(\n        rows=1,\n        cols=2,\n        subplot_titles=[\"Исходное пространство\", \"Деформированное пространство\"],\n        horizontal_spacing=0.02\n    )\n\n    def add_vector_annotation(x: float, y: float, xref: str, yref: str,\n                              color: str, label: str,\n                              row: int = 1, col: int = 1,\n                              text_xshift: int = 10, text_yshift: int = -10) -\u003e None:\n        \"\"\"\n        Добавляет на фигуру стрелку (от (0,0) до (x,y)) и текстовую подпись рядом.\n        \"\"\"\n        # Стрелка\n        fig.add_annotation(\n            x=x, y=y,\n            ax=0, ay=0,\n            xref=xref, yref=yref,\n            axref=xref, ayref=yref,\n            showarrow=True,\n            arrowhead=3,\n            arrowcolor=color,\n            arrowsize=1.0,\n            arrowwidth=2,\n            text=\"\",\n            row=row,\n            col=col\n        )\n        # Подпись вектора\n        fig.add_annotation(\n            x=x, y=y,\n            xref=xref, yref=yref,\n            text=label,\n            showarrow=False,\n            xshift=text_xshift,\n            yshift=text_yshift,\n            row=row,\n            col=col\n        )\n\n    # -----------------------------\n    # 1) Исходное пространство\n    # -----------------------------\n    # Рисуем координатную сетку\n    for x_val in xs:\n        fig.add_trace(\n            go.Scatter(\n                x=[x_val, x_val],\n                y=[-grid_range, grid_range],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n    for y_val in ys:\n        fig.add_trace(\n            go.Scatter(\n                x=[-grid_range, grid_range],\n                y=[y_val, y_val],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n\n    # Рисуем базис: i, j\n    add_vector_annotation(1, 0, \"x\", \"y\", \"red\", \"i\", row=1, col=1)\n    add_vector_annotation(0, 1, \"x\", \"y\", \"green\", \"j\", row=1, col=1)\n\n    # Рисуем единичный квадрат (параллелограмм) в исходном пространстве\n    square: np.ndarray = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n    fig.add_trace(\n        go.Scatter(\n            x=square[:, 0],\n            y=square[:, 1],\n            mode='lines',\n            fill='toself',\n            fillcolor='rgba(255,165,0,0.15)',  # оранжевый, прозрачность 15%\n            line=dict(color='orange'),\n            showlegend=False\n        ),\n        row=1, col=1\n    )\n\n    fig.update_xaxes(range=[-grid_range, grid_range], scaleanchor=\"y\", scaleratio=1, row=1, col=1)\n    fig.update_yaxes(range=[-grid_range, grid_range], row=1, col=1)\n\n    # -----------------------------\n    # 2) Деформированное пространство\n    # -----------------------------\n    for x_val in xs:\n        line_y = np.linspace(-grid_range, grid_range, 100)\n        original_points = np.array([[x_val, y_val] for y_val in line_y])\n        transformed = original_points @ matrix.T\n        fig.add_trace(\n            go.Scatter(\n                x=transformed[:, 0],\n                y=transformed[:, 1],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n    for y_val in ys:\n        line_x = np.linspace(-grid_range, grid_range, 100)\n        original_points = np.array([[x_val, y_val] for x_val in line_x])\n        transformed = original_points @ matrix.T\n        fig.add_trace(\n            go.Scatter(\n                x=transformed[:, 0],\n                y=transformed[:, 1],\n                mode='lines',\n                line=dict(color='lightgray', dash='dash'),\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n\n    # Образы базиса: i -\u003e i' и j -\u003e j'\n    i_prime: np.ndarray = matrix @ np.array([1, 0])\n    j_prime: np.ndarray = matrix @ np.array([0, 1])\n    add_vector_annotation(i_prime[0], i_prime[1], \"x2\", \"y2\", \"red\", \"i'\", row=1, col=2)\n    add_vector_annotation(j_prime[0], j_prime[1], \"x2\", \"y2\", \"green\", \"j'\", row=1, col=2)\n\n    # Рисуем образ единичного квадрата в деформированном пространстве\n    square_transformed: np.ndarray = square @ matrix.T\n    fig.add_trace(\n        go.Scatter(\n            x=square_transformed[:, 0],\n            y=square_transformed[:, 1],\n            mode='lines',\n            fill='toself',\n            fillcolor='rgba(255,165,0,0.15)',\n            line=dict(color='orange'),\n            showlegend=False\n        ),\n        row=1, col=2\n    )\n\n    # Настройка осей и размеров подграфиков\n    fig.update_layout(\n        width=1000,\n        height=500,\n        margin=dict(l=0, r=0, t=50, b=50),\n        xaxis=dict(\n            domain=[0, 0.45],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"y\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        yaxis=dict(\n            domain=[0, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"x\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        xaxis2=dict(\n            domain=[0.55, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"y2\",\n            scaleratio=1,\n            constrain=\"domain\"\n        ),\n        yaxis2=dict(\n            domain=[0, 1],\n            range=[-grid_range, grid_range],\n            scaleanchor=\"x2\",\n            scaleratio=1,\n            constrain=\"domain\"\n        )\n    )\n\n    return fig\n\ndef main() -\u003e None:\n    \"\"\"Главная функция приложения Streamlit.\"\"\"\n    st.title(\"Интерактивная визуализация линейной деформации\")\n\n    st.subheader(\"Коэффициенты матрицы линейного отображения\")\n    col1, col2 = st.columns(2)\n    a: float = col1.number_input(\"a:\", value=1.0, step=0.1)\n    b: float = col2.number_input(\"b:\", value=0.0, step=0.1)\n    col3, col4 = st.columns(2)\n    c: float = col3.number_input(\"c:\", value=0.0, step=0.1)\n    d: float = col4.number_input(\"d:\", value=1.0, step=0.1)\n\n    # Вычисляем площадь (|det A|) и выводим в отдельном блоке\n    matrix: np.ndarray = np.array([[a, b], [c, d]])\n    area_scale: float = np.linalg.det(matrix)\n    st.markdown(f\"**det A:** {area_scale:.2f}\")\n    st.markdown(f\"**Изменение площади (|det A|):** {abs(area_scale):.2f}\")\n    \n    fig: go.Figure = create_figure(a, b, c, d)\n    st.plotly_chart(fig, use_container_width=True)\n\nif __name__ == '__main__':\n    main()\n    \n```\n\n{% endcut %}\n\nТеперь давайте посмотрим на результат кода выше.\n\nПусть дана матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$, такая что:\n\n$$A=\n\\begin{pmatrix}\n\\begin{array}{r r}\n0\u0026 2 \\\\\n2\u0026 0 \\\\\n\\end{array}\n\\end{pmatrix}\n$$\n\nТогда деформация пространства под действием линейного отображения, заданного матрицей $A$, будет выглядеть следующим образом:\n\n![Рисунок 4.5.10: Анимация деформации пространства](https://yastatic.net/s3/education-portal/media/4_6_10_5bd09db575.gif \"Анимация деформации пространства\")\n\nИтак, мы рассмотрели, какая визуальная интуиция стоит за линейными деформациями, на примере пространства $\\mathbb{R}^2$. В пространстве $\\mathbb{R}^3$ можно привести аналогичные рассуждения, только вместо единичного квадрата мы будем оперировать единичным кубом, а при деформациях получать уже не параллелограмм, а параллелепипед, и оценивать не площадь, а объём. То есть эти рассуждения справедливы и для пространства $\\mathbb{R}^n$. Таким образом, можно кратко заключить:\n\n\u003e 💡 Определитель — это $n$-мерный ориентированный объём, натянутый на векторы матрицы, задающей линейное отображение (деформацию пространства).\n\n{% cut \"Связь с анализом данных: важность определителя в PCA\" %}\n\nОдна из самых известных задач в анализе данных — это задача понижения размерности. Она возникает, когда у нас есть высокоразмерные данные (например, каждый объект описан сотней признаков), а мы хотим свести их к меньшему числу характеристик, сохранив как можно больше информации.\n\nМетод главных компонент (Principal Component Analysis, PCA) решает эту задачу. Он находит новое ортонормированное базисное пространство (новые оси), в котором можно выразить данные, и при этом:\n\n- эти новые оси упорядочены по убыванию дисперсии — то есть первая главная компонента направлена туда, где у данных наибольший разброс;\n- если оставить только первые $k$ компонент (осей), то в проекции на это $k$‑мерное подпространство данные сохраняют максимум своей изменчивости.\n\nНо есть и геометрический взгляд на PCA: если $S$ — ковариационная матрица наших данных (то есть матрица, которая описывает, как признаки *расползлись* в пространстве), а $V_k$ — матрица из $k$ ортонормированных векторов (направлений, вдоль которых мы хотим проектировать данные), то PCA ищет такие векторы $V_k$, чтобы максимизировать объём проекции точек данных. Этот объём можно выразить через определитель:\n\n$\\max_{V_k^\\top V_k = I} \\det(V_k^\\top S V_k)$\n\nЗдесь $V_k^\\top S V_k$ — это ковариационная матрица уже после проекции, а $\\det(V_k^\\top S V_k)$ — ориентированный объём, занимаемый данными в новом $k$‑мерном пространстве. Таким образом, PCA стремится найти те направления, вдоль которых данные сохраняют наибольший объём, а не «сплющиваются».\n\n{% endcut %}\n\nТеперь, когда у нас есть интуитивное представление о геометрическом смысле определителя как коэффициента масштабирования объёма, перейдём к его строгому математическому определению.\n\nЭто поможет нам не только закрепить интуицию, но и понять, как определитель вычисляется формально, особенно в случае размерностей выше трёх, где визуализация уже не столь очевидна.\n\n### Формальное определение\n\nДля квадратной матрицы $A \\in\n\\operatorname{M}_{n} (\\mathbb R)$ её определитель обозначается как $\\det{A}$ (от англ. determinant) и вычисляется по формуле:\n\n$$\\det A =\n\\sum_{\\sigma\\in S {n}} \\operatorname{sgn}(\\sigma) a_{1\\sigma(1)}\\cdot \\ldots \\cdot a_{n\\sigma(n)}\n$$\n\nгде:\n\n- $\\sigma$ — перестановка;\n- $S_{n}$ — множество всех перестановок на $n$-элементном множестве;\n- $\\operatorname{sgn}(\\sigma)$ — знак перестановки $\\sigma$;\n- $a_{i\\sigma(j)}$ — элемент матрицы $A$, выбираемый из $i$-й строки и $\\sigma(j)$-го столбца.\n\nКак вы видите, в формуле используются `перестановки`. Разбирать подробно, что это такое, сейчас мы не будем. Но для контекста дадим формальное определение, покажем, как их можно задавать и как задаётся знак перестановки.\n\n{% cut \"Перестановки\" %}\n\nПусть $X_n = \\{1,\\ldots,n\\}$ — конечное множество из $n$ занумерованных элементов. `Перестановкой` называется биективное отображение $\\sigma\\colon X_n\\to X_n$.\n\n**Как задавать перестановки**\n\nС помощью таблицы значений, где под каждым $i$ элементом пишется его образ $\\sigma(i)$:\n\n$$\\begin{pmatrix}{1}\u0026{2}\u0026{\\ldots}\u0026{n}\\\\{\\sigma(1)}\u0026{\\sigma(2)}\u0026{\\ldots}\u0026{\\sigma(n)}\\\\\\end{pmatrix}\n$$\n\n**Знак перестановки**\n\nОбычно знак перестановки $\\sigma$ определяют в виде $\\operatorname{sgn}(\\sigma) =(-1)^{d(\\sigma)}$, где $d(\\sigma)$ — `число инверсий`. Пусть $\\sigma \\in S_n$ и $i,j \\in X_n$ — пара различных элементов, тогда эта пара называется `инверсией`, если $i \u003c j$ влечёт $\\sigma(i) \u003e  \\sigma(j)$, а $i \u003e j$ влечёт $\\sigma(i) \u003c \\sigma(j)$.\n\n{% endcut %}\n\nТеперь давайте снова вернёмся к формальному определению определителя и попробуем его разобрать, уже зная, что такое перестановки. Внутри суммы стоят произведения вида $a_{1\\sigma(1)}\\cdot \\ldots \\cdot a_{n\\sigma(n)}$, где первый индекс — это индекс строки в матрице $A$ и эти индексы идут по порядку от $1$ до $n$.\n\nВторой индекс записан так, что мы получаем его из перестановки $\\sigma$. То есть мы каждый раз выбираем элементы $a_{ij}$ так, чтобы из каждой строки и каждого столбца взять строго один элемент. Все эти элементы перемножаем, добавляем знак $\\pm 1$ и получаем одно слагаемое. Затем мы берём другую перестановку из $S_n$ и повторяем процедуру, получая новое слагаемое. В итоге всего получится $n!$ слагаемых.\n\nВозможно, теперь стало наглядно понятно, что без визуальной интуиции довольно сложно понять смысл, который стоит за определителем, именно поэтому мы так много внимания уделили его геометрической интерпретации. Кроме этого, определитель очень тесно связан с `невырожденными матрицами`, давайте рассмотрим этот момент подробнее.\n\n**Невырожденные матрицы**\n\nРанее, когда мы рассматривали обратные матрицы, то сделали оговорку, что обратимые матрицы также называют невырожденными, так как, по существу, эти два термина отражают одно и то же свойство квадратной матрицы — существование обратной матрицы. Теперь, познакомившись с определителем, можно дать определение невырожденной матрицы.\n\n`Невырожденная матрица` — это квадратная матрица, определитель которой отличен от нуля. В противном случае матрица называется `вырожденной`.\n\nПроизвольная матрица $A \\in \\operatorname{M}_{n} (\\mathbb R)$ является `невырожденной`, если удовлетворяет любому из следующих эквивалентных условий и наоборот.\n\n1. Система линейных уравнений $Ax=0$ имеет только нулевое решение $x \\in \\mathbb R^n$ .\n2. Матрица $A$ представляется в виде произведения матриц элементарных преобразований.\n3. Матрица $A$ обратима.\n4. Определитель матрицы $A$ отличен от нуля.\n\nНа практике, чтобы понять, является ли произвольная матрица $A$ обратимой, можно, например, проверить, выполняется ли самое простое условие: $\\det A \\neq 0$.\n\n### **Вычисление в малых размерностях**\n\nИз формулы определителя видно, что вычислять его достаточно сложно — нужно получить $n!$ слагаемых. Поэтому в реальной жизни данной формулой пользуются крайне редко. Обычно [используют](https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html#:~:text=The%20determinant%20is%20computed%20via%20LU%20factorization) уже знакомое нам `LU`-разложение, так как для треугольных матриц можно очень легко посчитать определитель, просто перемножив диагональные элементы.\n\nДля пространств малой размерности существуют явные формулы (которые выводятся из формулы, приведённой в определении определителя), и по ним легко вычислять определитель. Давайте их рассмотрим.\n\n1. Если $A \\in\n   \\operatorname{M}_{1} (\\mathbb R) = \\mathbb R$:\n\n$$\\det A = A.\n$$\n\n2. Если $A \\in\n   \\operatorname{M}_{2} (\\mathbb R)$:\n\n$$A=\\begin{pmatrix}\n\\begin{array}{r r}\na\u0026 b \\\\\nc\u0026 d \\\\\n\\end{array}\n\\end{pmatrix}\n, \\quad\n\\det A =\nad - bc.\n$$\n\nСхематично данную формулу можно изобразить так:\n\n![Рисунок 4.5.11: Схематичная формула для определителя в двумерном пространстве](https://yastatic.net/s3/education-portal/media/4_6_11_a3f36a6163.webp \"Схематичная формула для определителя в двумерном пространстве\")\n\nПример:\n\n$$    A=\\begin{pmatrix}\n    \\begin{array}{r r}\n    1\u0026 3 \\\\\n    0\u0026 -2 \\\\\n    \\end{array}\n    \\end{pmatrix}\n    , \\quad\n    \\det A =\n    1 \\cdot (-2)-3 \\cdot 0=-2\n$$\n\n3. Если $A \\in\n   \\operatorname{M}_{3} (\\mathbb R)$:\n\n$$    A=\n    \\begin{pmatrix}\n    \\begin{array}{r r}\n    {a_{11}}\u0026{a_{12}}\u0026{a_{13}}\\\\{a_{21}}\u0026{a_{22}}\u0026{a_{23}}\\\\{a_{31}}\u0026{a_{32}}\u0026{a_{33}}\n    \\end{array}\n    \\end{pmatrix},\n    \\\\~\\\\\n    \\det A =\n    a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32}.\n$$\n\nСхематично данную формулу можно изобразить так:\n\n![Рисунок 4.5.12: Схематичная формула для определителя в трёхмерном пространстве](https://yastatic.net/s3/education-portal/media/4_6_12_93753819dc.webp \"Схематичная формула для определителя в трёхмерном пространстве\")\n\nПример:\n\n$$    A=\n    \\begin{pmatrix}\n    \\begin{array}{r r}\n    {1}\u0026{2}\u0026{1}\\\\{0}\u0026{1}\u0026{0}\\\\{-1}\u0026{-1}\u0026{0}\n    \\end{array}\n    \\end{pmatrix},\n    \\\\~\\\\\n    \\det A =\n    (1 \\cdot 1 \\cdot 0) +\n    (2 \\cdot 0 \\cdot (-1)) +\n    (1 \\cdot 0 \\cdot (-1)) -\n    (1 \\cdot 1 \\cdot (-1)) -\n    (2 \\cdot 0 \\cdot 0) -\n    (1 \\cdot 0 \\cdot (-1))=1\n$$\n\nЗдесь полезно, глядя на формулы вычисления определителя, задуматься о виде матриц, для которых его вычисление будет намного проще. Рассмотрим верхне- и нижнетреугольные матрицы.\n\nДля них определитель будет вычисляться следующим образом:\n\n$$\\det\n\\begin{pmatrix}\n{\\lambda_1}\u0026{\\ldots}\u0026{*}\\\\\n{}\u0026{\\ddots}\u0026{\\vdots}\\\\\n{}\u0026{}\u0026{\\lambda_n}\\\\\n\\end{pmatrix}\n=\n\n\\det\n\\begin{pmatrix}\n{\\lambda_1}\u0026{}\u0026{}\\\\\n{\\vdots}\u0026{\\ddots}\u0026{}\\\\\n{*}\u0026{\\ldots}\u0026{\\lambda_n}\\\\\n\\end{pmatrix}\n=  \n\n\\lambda_1 \\cdot \\ldots \\cdot \\lambda_n.\n$$\n\nДля доказательства нужно посчитать определитель по определению через перестановки и увидеть, что все слагаемые, кроме одного, где берутся только диагональные элементы, будут равны нулю. Таким образом, для подобного рода матриц можно очень быстро найти определитель, следовательно, полезно попытаться привести исходную матрицу именно к такому виду.\n\nКак вы думаете, чему равен определитель для каждой из матриц, задающих элементарные преобразования?\n\n{% cut \"Ответ (не открывайте сразу, сначала подумайте сами!)\" %}\n\n1. Прибавление одной строки к другой с множителем ($i \\neq j$): $\\det(S) = 1$\n2. Перестановка двух строк: $\\det (P) = -1$\n3. Умножение строки на ненулевую константу $\\lambda$: $\\det(D) = \\lambda$\n\n{% endcut %}\n\nДавайте рассмотрим свойства определителя, чтобы понять, какие операции можно совершать с исходной матрицей, не повлияв на значение определителя.\n\n### **Свойства определителя**\n\nОпределитель напрямую по формуле вычислять сложно — слагаемых слишком много. Но, если использовать его свойства, можно упростить задачу: привести матрицу к такому виду, при котором это будет сделать проще.\n\nПусть $A \\in\n\\operatorname{M}_{n} (\\mathbb R)$, тогда на неё можно смотреть как на набор из $n$ векторов, уложенных как вектор-столбцы, то есть:\n\n$$A =\n(A_1\\mid \\ldots\\mid A_n)\\in\n\\operatorname{M}*{n} (\\mathbb R),\n\\quad\nA*{i} \\in \\mathbb{R}^n.\n$$\n\nТогда определитель можно рассматривать как функцию от столбцов матрицы $A$, то есть:\n\n$$\\det A =\n\\det (A_1\\mid \\ldots\\mid A_n).\n$$\n\nИ можно сформулировать следующие свойства:\n\n1. Линейность определителя по каждому столбцу:\n\n   $$\\det(A_1|\\ldots|A_i + A_i'|\\ldots|A_n) =\n   \\\\\n   \\det(A_1|\\ldots|A_i|\\ldots|A_n) + \\det(A_1|\\ldots|A_i'|\\ldots|A_n)\n   $$\n   \n   \n\n2. При изменении позиции столбца меняется знак определителя:\n\n   $$\\det(A_1|\\ldots|A_i|\\ldots|A_j|\\ldots|A_n) = -\\det(A_1|\\ldots|A_j|\\ldots|A_i|\\ldots|A_n)\n   $$\n   \n   \n\n3. Если у матрицы есть два одинаковых столбца, то определитель равен нулю:\n\n   $$\\det(A_1|\\ldots|A'|\\ldots|A'|\\ldots|A_n) = 0\n   $$\n   \n   \n\n4. Если один из столбцов умножить на одно и то же число, то и весь определитель умножится на это же число:\n\n   $$\\det(A_1|\\ldots|\\lambda A_i|\\ldots|A_n)  = \\lambda \\det(A_1|\\ldots|A_i|\\ldots|A_n)\n   $$\n   \n   \n\n5. Если к одному столбцу матрицы прибавить другой, умноженный на коэффициент, то определитель не изменится:\n\n   $$\\det(A_1|\\ldots|A_i|\\ldots|A_j|\\ldots|A_n) = \\det(A_1|\\ldots|A_i|\\ldots|A_j + \\lambda A_i|\\ldots|A_n)\n   $$\n   \n   \n\n6. Все сформулированные выше свойства для столбцов также верны и для строк, так как:\n\n   $$\\det A = \\det A^t\n   $$\n   \n   \n\n7. Мультипликативность определителя:\n\n   $$\\det(AB) = \\det(A) \\det(B)\n   $$\n   \n   \n\n8. Определитель от обратной матрицы:\n\n   $$\\det (A^{-1})=\\det (A)^{-1}\n   $$\n   \n   \n\nКак видите, на практике может быть эффективно приводить исходную матрицу $A$ элементарными преобразованиями к треугольному виду, попутно запоминая некоторые коэффициенты, которые можно вынести за определитель. Давайте рассмотрим ещё один способ вычисления определителя через разложение по строке (столбцу).\n\n### Миноры и алгебраические дополнения\n\nМы рассмотрели, как можно вычислить определитель через явную формулу, а также раскрыли её для случаев малой размерности. Давайте рассмотрим ещё один способ подсчёта определителя.\n\nДля этого введём ряд дополнительных понятий. Пусть $B \\in \\operatorname{M}_{n} (\\mathbb R)$ — некоторая матрица с элементом $b_{ij}$. Рассмотрим матрицу $D_{ij} \\in \\operatorname{M}_{n-1} (\\mathbb R)$, полученную путем вычёркивания $i$ строки и $j$ столбца из матрицы $B$.\n\n$$B =\n\\left(\n  \\begin{array}{c|c|c}\n    X_{ij}\n      \u0026 \\begin{smallmatrix}*\\\\\\vdots\\end{smallmatrix}\n      \u0026 Y_{ij} \\\\\n    \\hline\n    \\begin{smallmatrix}*\u0026\\dots\\end{smallmatrix}\n      \u0026 b_{ij}\n      \u0026 \\begin{smallmatrix}\\dots\u0026*\\end{smallmatrix} \\\\\n    \\hline\n    Z_{ij}\n      \u0026 \\begin{smallmatrix}\\vdots\\\\*\\end{smallmatrix}\n      \u0026 W_{ij}\n  \\end{array}\n\\right),\n\\;\nD_{ij}=\n\\begin{pmatrix}\n\\begin{array}{r r}\nX_{ij}\u0026 Y_{ij} \\\\\nZ_{ij}\u0026 W_{ij} \\\\\n\\end{array}\n\\end{pmatrix}\n\n$$\n\nгде $X_{ij}, Y_{ij}, Z_{ij}, W_{ij}$  — это в общем случае матрицы, оставшиеся в исходной матрице после вычёркивания $i$-строки и $j$-столбца. Такой вид записи матрицы $D_{ij}$ называют блочным, так как мы записываем матрицу не отдельными элементами, а сразу блоками из других матриц.\n\nОпределитель матрицы $D_{ij}$ обозначается $M_{ij}$ и называется `минором` матрицы $B$ или $ij$-минором. Число $A_{ij} = (-1)^{i+j}M_{ij}$ называется `алгебраическим дополнением` элемента $b_{ij}$ или $ij$-алгебраическим дополнением матрицы $B$.\n\n$$M_{ij}=\n\\det\n\\begin{pmatrix}\n\\begin{array}{r r}\nX_{ij}\u0026 Y_{ij} \\\\\nZ_{ij}\u0026 W_{ij} \\\\\n\\end{array}\n\\end{pmatrix},\n\\;\nA_{ij}=\n(-1)^{i+j}\nM_{ij}\n$$\n\nТакже выделяют `присоединённую матрицу` $\\hat {B}$ для $B$, которая определяется так:\n\n$$\\hat{B}=\n\\left(\\left.\n\\begin{matrix}\nA_{11}\u0026A_{21}\u0026\\ldots\u0026A_{n1}\\\\\nA_{12}\u0026A_{22}\u0026\\ldots\u0026A_{n2}\\\\\n\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\nA_{1n}\u0026A_{2n}\u0026\\ldots\u0026A_{nn}\n\\end{matrix}\n\\right)\\right.\n$$\n\nТо есть нужно сперва рассчитать для каждого элемента $b_{ij}$ его алгебраическое дополнение $A_{ij}$ и уложить их в матрицу согласно $ij$-индексу, а затем полученную матрицу транспонировать. Также часто `присоединённую матрицу` для $B$ обозначают как $\\operatorname{adj}(B)$, то есть $\\hat{B}=\\operatorname{adj}(B)$. С помощью присоединённой матрицы и определителя можно найти обратную матрицу:\n\n$$B^{-1} = \\frac{\\operatorname{adj}(B)}{\\det B}\n$$\n\n{% cut \"Присоединённая матрица в ML\" %}\n\nЕсли размер матрицы небольшой ($n \\leq 4$) и требуется вычислить производную по обратной матрице, можно использовать следующую формулу:\n\n$$\\frac{\\partial B^{-1}}{\\partial B} = -B^{-1} \\cdot (\\partial B) \\cdot B^{-1}\n$$\n\nТакой подход используется в DL-библиотеках (например, JAX, PyTorch) при написании кастомных градиентов для маленьких матриц $3 \\times 3$, особенно в задачах обучения свёрточных фильтров с фиксированным размером ядра (например, в 2D SPP‑модулях). Это быстрее и точнее, чем использовать универсальный `torch.inverse()`.\n\n{% endcut %}\n\nТеперь, когда у нас есть представление о минорах и алгебраических дополнениях, мы можем использовать их для альтернативного способа вычисления определителя.\n\nЭтот способ называется разложением по строке или столбцу, и он особенно удобен, когда в выбранной строке или столбце много нулей — тогда часть слагаемых в формуле зануляется и вычисления сильно упрощаются.\n\n**Разложение по строке (столбцу)**\n\nПусть дана некоторая матрица $B \\in \\operatorname{M}_{n} (\\mathbb R)$, тогда:\n\n- Для любой $i$-строки верно разложение:\n\n  $$\\det B = \\sum_{j=1}^n b_{ij}A_{ij}\n  $$\n  \n  \n\n- Для любого $j$-столбца верно разложение:\n\n  $$\\det B = \\sum_{i=1}^n b_{ij}A_{ij}\n  $$\n  \n  \n\nДавайте рассмотрим конкретный пример, разложим по второму столбцу:\n\n$$B=\n\\begin{pmatrix}\n\\begin{array}{r r}\n{1}\u0026{2}\u0026{1}\\\\\n{0}\u0026{1}\u0026{0}\\\\\n{-1}\u0026{-1}\u0026{0}\n\\end{array}\n\\end{pmatrix},\n\\\\~\\\\\n\\det B =\n2(-1)^{1+2}\\det\n\\begin{pmatrix}\n\\begin{array}{r r}\n{0}\u0026{0}\\\\\n{-1}\u0026{0}\n\\end{array}\n\\end{pmatrix}\n+\n1(-1)^{2+2}\\det\n\\begin{pmatrix}\n\\begin{array}{r r}\n{1}\u0026{1}\\\\\n{-1}\u0026{0}\n\\end{array}\n\\end{pmatrix}\n-\n\n1(-1)^{3+2}\\det\n\\begin{pmatrix}\n\\begin{array}{r r}\n{1}\u0026{1}\\\\\n{0}\u0026{0}\n\\end{array}\n\\end{pmatrix}=1\n$$\n\nОднако на практике гораздо удобнее и надёжнее воспользоваться готовыми библиотеками, которые автоматически рассчитывают определитель. Давайте посмотрим, как это можно сделать в Python.\n\n### **Вычисление в Python**\n\nСуществует несколько популярных библиотек, с помощью которых можно найти определитель матрицы. Ниже кратко сравним их и приведём примеры использования.\n\n#|\n||\n\nНазвание\n\n|\n\nПрименение\n\n||\n||\n\nNumPy\n\n|\n\nЭто наиболее универсальный и часто используемый инструмент для работы с числами на CPU. Функция \\`numpy.linalg.det()\\` оптимизирована за счёт использования библиотеки \\[LAPACK\\](https://github.com/Reference-LAPACK), что обеспечивает высокую скорость и низкий расход памяти при работе с матрицами, особенно если их размер невелик. Если вы не используете специализированные функции глубокого обучения или символьные вычисления, то \\`numpy\\` — оптимальный выбор.\n\n||\n||\n\nSciPy\n\n|\n\nМодуль \\`scipy.linalg\\` предоставляет схожие возможности с \\`numpy\\`, но может быть удобен, если вы работаете с более специализированными алгоритмами линейной алгебры или вам требуется поддержка некоторых редких матричных форматов. Скорость и расход памяти будут примерно на том же уровне, что и в \\`numpy\\`.\n\n||\n||\n\nPyTorch и TensorFlow\n\n|\n\nЭти библиотеки разработаны в первую очередь для задач глубокого обучения и автоматического дифференцирования. Они поддерживают вычисления на GPU, что может быть преимуществом при обработке больших матриц или батчей данных. Но для вычисления определителя небольших матриц на CPU они могут накладывать дополнительные издержки по сравнению с \\`numpy\\`. Если вы уже работаете с моделями нейронных сетей, то использовать встроенные функции (\\`torch.det()\\` или \\`tf.linalg.det()\\`) будет удобнее, так как это позволяет избежать лишних преобразований данных.\n\n||\n|#\n\nТеперь давайте на практике посмотрим, как с помощью этих библиотек можно вычислить определитель. Начнём с самой универсальной — `numpy`.\n\n**NumPy**\n\nСамая часто используемая библиотека для численных вычислений. Функция `numpy.linalg.det()` вычисляет определитель массива, используя оптимизированные алгоритмы из LAPACK.\n\n```python\nimport numpy as np\n\nA = np.array([[1, 2],\n              [3, 4]])\ndet_A = np.linalg.det(A)\nprint(\"Определитель матрицы A (numpy): \", det_A)\n\n# Output:\n# Определитель матрицы A (NumPy): -2.00\n```\n\n**SciPy**\n\nВ библиотеке `scipy` есть модуль `scipy.linalg`, где функция `det()` также позволяет вычислять определитель. Эта функция часто используется, если требуется использовать дополнительные возможности или типы матриц.\n\n```python\nimport numpy as np\nfrom scipy.linalg import det\n\nA = np.array([[1, 2],\n              [3, 4]])\ndet_A = det(A)\nprint(f\"Определитель матрицы A (SciPy): {det_A:.2f}\")\n\n# Output:\n# Определитель матрицы A (SciPy): -2.00\n```\n\n**PyTorch**\n\nВ контексте машинного обучения при работе с тензорами можно использовать `pytorch`. Функция `torch.det()` вычисляет определитель для тензоров.\n\n```python\nimport torch\n\nA = torch.tensor([[1.0, 2.0],\n                  [3.0, 4.0]])\ndet_A = torch.det(A)\nprint(f\"Определитель матрицы A (PyTorch): {det_A.item():.2f}\")\n\n# Output:\n# Определитель матрицы A (PyTorch): -2.00\n```\n\n**TensorFlow**\n\nДля тех кто использует `tensorflow`, также доступна функция `tf.linalg.det()`, которая вычисляет определитель тензоров.\n\n```python\nimport tensorflow as tf\n\nA = tf.constant([[1.0, 2.0],\n                 [3.0, 4.0]])\ndet_A = tf.linalg.det(A)\nprint(f\"Определитель матрицы A (TensorFlow): {det_A.numpy():.2f}\")\n\n# Output:\n# Определитель матрицы A (TensorFlow): -2.00\n```\n\nЕсли вы занимаетесь стандартными численными вычислениями или анализом данных на CPU, то `numpy` (или `scipy`) будет лучшим выбором благодаря своей скорости и низким затратам памяти. Если же вы работаете в контексте глубокого обучения и уже используете `pytorch` или `tensorflow`, имеет смысл применять их функции для согласованности вычислительного графа и возможного ускорения на GPU.\n\n**Применение определителя в задачах анализа данных**\n\nОбычно определитель используется как некоторый вспомогательный инструмент, который позволяет узнать полезную информацию про матрицу. В машинном обучении, когда требуется оценить вероятность наблюдения в многомерном пространстве, используется формула:\n\n$$p(x) = \\frac{1}{\\sqrt{(2\\pi)^d\\,|\\Sigma|}} \\exp\\!\\left(-\\frac{1}{2} (x-\\mu)^t \\Sigma^{-1} (x-\\mu)\\right)\n$$\n\nгде:\n\n- $p(x)$ — функция плотности вероятности для случайного вектора  $x \\in \\mathbb{R}^d$ с математическим ожиданием $\\mu$ и ковариационной матрицей $\\Sigma$;\n- $|\\Sigma|$ — определитель ковариационной матрицы, который играет роль нормализующей константы.\n\nРоль определителя:\n\n1. Нормализация распределения.\n\n   Определитель $|\\Sigma|$, входя в знаменатель нормирующего множителя $\\sqrt{(2\\pi)^d\\,|\\Sigma|}$, обеспечивает, что интеграл плотности по всему пространству равен единице. Математически это гарантирует корректное масштабирование распределения.\n\n2. Связь с объёмом.\n\n   Геометрически определитель ковариационной матрицы отражает объём эллипсоида, описывающего область, в которой находится большая часть вероятности (так называемый контур доверительного интервала). Чем меньше $|\\Sigma|$, тем более «сжатое» распределение, что означает, что данные сконцентрированы в меньшем объёме пространства.\n\n3. Чувствительность к вырожденности.\n\n   Если $|\\Sigma| \\to 0$, это указывает на то, что ковариационная матрица близка к вырожденной — например, когда признаки почти линейно зависимы. В таких случаях модель может испытывать проблемы с числовой стабильностью: обратная матрица $\\Sigma^{-1}$ становится нестабильной, а оценка правдоподобия — некорректной.\n\n4. Применение в алгоритмах.\n\n   В моделях типа Gaussian Mixture Models (GMM) для каждой компоненты рассчитывается своя ковариационная матрица, и её определитель влияет на вклад компоненты в общую вероятность наблюдения.\n\nВ заключении отметим, что определитель — это очень полезный инвариант, который выступает как лаконичный индикатор важных свойств и характеристик линейного отображения или матрицы. Так с помощью него можно проверить вырожденность матрицы или дать геометрическую интерпретацию линейным отображениям, о которых мы как раз и поговорим в следующем параграфе.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13776909.a2f734eb4cbbf18e01e2c999c15cf68e732fa4d0?iframe=1\" frameborder=\"0\" name=\"ya-form-13776909.a2f734eb4cbbf18e01e2c999c15cf68e732fa4d0\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"69:T8137,"])</script><script nonce="">self.__next_f.push([1,"Признаковое пространство\u0026nbsp;— это многомерный мир, где живут наши данные, представленные точками (векторами) с координатами-признаками. В этом параграфе мы исследуем фундаментальные измерительные инструменты этого пространства: `нормы` и `расстояния`.\n\nС помощью этих знаний вы поймёте, как работают многие методы машинного обучения, включая k-ближайших соседей (KNN), линейную регрессию и даже нейронные сети. От измерения схожести объектов до регуляризации и визуализации: всё в итоге упирается в то, как мы измеряем `длину` вектора и `расстояние` между точками. Для этого нам и нужны нормы.\n\nВ предыдущем параграфе мы рассматривали, как линейные преобразования изменяют объём. Теперь сосредоточимся на том, как сам объём и векторы измеряются — с помощью норм. Норма начинается с векторов, но, естественно, переносится и на матрицы. Это важно, поскольку в будущем вы будете оценивать размеры матрицы весов в нейросетях и использовать матричные нормы — например, в QR-разложении.\n\nДавайте сначала разберёмся:\n\n- Какими бывают нормы векторов и матриц.\n- Чем они отличаются от привычной евклидовой длины.\n- Как интерпретировать расстояния в разных метриках.\n- Как это связано с обучением и работой моделей машинного обучения.\n\nПараллельно взглянем на то, как выбор метрик помогает контролировать сложность моделей и их устойчивость к переобучению.\n\n## Нормы и расстояния\n\nВ математике норма — это обобщённое понятие длины. Когда мы говорим о норме вектора или норме матрицы, мы имеем в виду способ измерить размер или величину объекта.\n\nПоэтому в линейной алгебре `нормой` называют функцию, которая измеряет размер объекта (например, «размер» матрицы или длину вектора). От выбора нормы зависит, как мы интерпретируем расстояние и угол между объектами. Это критически важно, например, в метрических алгоритмах (KNN, кластеризация k-средних и метод визуализации tSNE).\n\nВведём строгое определение нормы.\n\n\u003e 💡 `Норма` в векторном пространстве $V$ над полем вещественных или комплексных чисел — это функционал $\\|\\cdot\\|:$ $V\\to\\mathbb{R}$, обладающий следующими свойствами:\n\u003e\n\u003e 1. Неотрицательность: $\\lVert x\\rVert \\ge 0$.\n\u003e 2. Обращение в нуль: $\\|x\\|=0\\iff x=0$.\n\u003e 3. Неравенство треугольника: $\\|x+y\\|\\leq\\|x\\|+\\|y\\|, \\quad \\forall x, y\\in V$.\n\u003e 4. Однородность: $\\|\\alpha x\\| = |\\alpha|\\,\\|x\\|, \\quad \\forall\\,\\alpha\\in\\mathbb{R}\\;(\\text{или }\\mathbb{C}),\\;\\forall\\,x\\in V$.\n\nПростыми словами, норма — некий функционал над элементом пространства $V$, который обозначается двумя вертикальными линиями справа и слева от объекта, норму которого мы берём. Этот функционал переводит все элементы пространства $V$ в вещественные числа. Перечисленные свойства нормы называются `аксиомами нормы`, и словами их можно сформулировать следующим образом:\n\n1. Если норма $x$ (например, вектор) равна нулю, то и сам объект $x$ должен быть нулём. Справедливо и обратное: если вектор нулевой, то его норма также равна нулю. В иных случаях норма больше нуля.\n2. Норма суммы объектов всегда меньше или равна сумме норм объектов. При этом каждый объект принадлежит пространству $V$. Это свойство называют *неравенством треугольников*.\n3. Скалярный множитель выносится из-под знака нормы по модулю.\n\nОбычно разные нормы используются для разных способов расчёта расстояний между объектами линейного пространства. Рассмотрим наиболее часто используемые нормы в пространстве $\\mathbb{R}^n$. И разберёмся, как считать нормы на примерах и в чём заключается их геометрический смысл.\n\n### L₁-норма (манхэттенская норма)\n\nНачнём с одной из самых наглядных норм: $L_1$-нормы, которую также называют `манхэттенской` или `нормой такси` — в дальнейшем мы проясним смысл этих названий. Она измеряет длину вектора как сумму модулей его координат:\n\n$$\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|\n$$\n\nТеперь на примере поговорим о расчёте расстояний на основе норм. Возьмём для примера $L_1$-норму для некоего вектора $a$:\n\n$$a= \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad \\|a\\|_1=|{1}|+|{-3}|=1+3=4\n$$\n\nТо есть норма $a$ равна четырём. Это можно интерпретировать как длину вектора $a$, рассчитанную на основе нормы $L_1$.\n\nНетрудно увидеть, что это не геометрическая длина вектора. Если мы применим теорему Пифагора, то без труда рассчитаем, что длина вектора $a$ будет равна $\\sqrt{10}$. Важно понимать, что нормы могут отличаться от геометрических расстояний.\n\nВ случае нормы $L_1$ мы измеряем расстояние, выходя из начала координат к точке $(1,0)$, а потом к точке $(1,-3)$ — то есть к самому концу вектора $a$. Так мы пройдём четыре клеточки, потому и расстояние по этой норме от начала координат (как и длина вектора $a$ по $L_1$-норме) будет равно четырём.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_1_8c152f8a4c.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация $L_1$-нормы для вектора $a=(1, -3)$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЗачем нам такое странное измерение расстояний? Разные способы измерения расстояний важны для разных задач.\n\nЕсли приводить пример из повседневной жизни, то расстояние по норме $L_1$ схоже с тем, как мы бы посчитали путь, двигаясь по сетке параллельных и перпендикулярных улиц города. Например, Нью-Йорка. Взгляните на один из его районов —\u0026nbsp;Манхэттен:\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://yastatic.net/s3/education-portal/media/4_7_3_1_5f6a45181c.webp\" loading=\"lazy\" decoding=\"async\" alt=\"\"\u003e\n  \u003cfigcaption\u003e\n    Карта Манхэттена \u003ca href=\"https://pt.maps-manhattan.com/manhattan-mapa-de-grade\"\u003eИсточник\u003c/a\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПо такой сетке нельзя двигаться по диагонали —\u0026nbsp;только вдоль улиц и проспектов. Если бы вы сели в такси на Манхэттене, то пройденное расстояние было бы суммой отрезков по горизонтали и вертикали. Вот почему альтернативные названия этой нормы —\u0026nbsp;`манхэттенская норма` (англ. Manhattan norm) и `норма такси` (англ. Taxicab norm).\n\nЗная, как измерить расстояние по $L_1$-норме между началом координат (то есть нулевым вектором) и некоторым вектором $a$, мы можем логично прийти к формуле для расчёта расстояний между точками:\n\n$$L_1(a, b) = \\|a - b\\|_1 = \\sum_{i=1}^{n} |a_i - b_i|\n$$\n\nНапример:\n\n$$a=\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad b= \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}, \n$$\n\n$$\\\\ L_1(a,b)=|{1-3}|+|{-3-2}|=|{-2}|+|{-5}|=2+5=7 \n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_3_efa528cb34.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Норма L_1\"\n  \u003e\n  \u003cfigcaption\u003e\n\nНорма $L_1$ как расстояние между двумя векторами $a=(1, -3)$ и $b=(3,2)$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ дальнейшем для визуализации норм мы будем приводить «единичные сферы» в двумерном пространстве. Это всё множество точек на единичном расстоянии от начала координат, рассчитанном по рассматриваемой норме вектора.\n\nДанные графики хорошо отражают принцип измерения расстояния в случаях разных норм. Например, для $L_1$-нормы единичная сфера представляет собой ромб.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_4_2fceeb8c9a.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Единичная сфера для нормы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nЕдиничная сфера для нормы $L_1$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n$L_1$-норма широко используется в машинном обучении, особенно в задачах [регуляризации](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_\\(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0\\)) — когда мы хотим не просто подобрать модель, хорошо объясняющую данные, но и избежать переобучения.\n\nТакие задачи требуют, чтобы модель была не слишком сложной, с небольшими коэффициентами, и желательно использовала только важные признаки. Именно для этого к функции потерь добавляют штраф, связанный с нормой коэффициентов.\n\nОдно из ключевых применений $L_1$-нормы — [Lasso-регрессия](https://en.wikipedia.org/wiki/Lasso_\\(statistics\\)), где к функции потерь добавляется штраф, пропорциональный сумме модулей коэффициентов:\n\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^{n} |\\beta_i|\n$$\n\nЗдесь $\\lambda$ — гиперпараметр регуляризации, а $\\sum_{i=1}^{p} |\\beta_i| = \\|\\beta\\|_1\n$ — $L_1$-норма вектора параметров. Такой штраф поощряет разрежённость, то есть он наказывает большие коэффициенты и приводит к тому, что многие из них обнуляются.\n\nЭто помогает выделять наиболее значимые признаки, снижать переобучение и улучшать интерпретируемость модели. $L_1$-регуляризация применяется не только в линейной регрессии, но и в логистической регрессии, и в [методе опорных векторов (SVM, Support Vector Machine)](https://en.wikipedia.org/wiki/Support_vector_machine).\n\n### L₂-норма (Евклидова норма)\n\nНаиболее интуитивно понятная, соответствует «обычному» расстоянию по Пифагору:\n\n$$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n$$\n\nТо есть она измеряет геометрическую длину вектора в евклидовом пространстве. Например:\n\n$$a=\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad \\|a\\|_2 = \\sqrt{1^2+(-3)^2}=\\sqrt{10}\n$$\n\nВ случае расчёта расстояния между двумя векторами:\n\n$$L_2(a,b) = \\sqrt{\\sum^{n}_{i=1}{\\left(a_i-b_i\\right)^2}},\n$$\n\n$$\\\\ a=\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad b= \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix},\n$$\n\n$$\\\\ L_2(a,b)=\\sqrt{(1-3)^2+(-3-2)^2}=\\sqrt{(-2)^2+(-5)^2}=\\sqrt{4+25}=\\sqrt{29}\n$$\n\nНорма $L_2$ как раз совпадает с геометрической длиной. И потому единичная сфера для этой нормы в двумерном линейном пространстве представляет собой окружность.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_5_8c70959d1a.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Единичная сфера для нормы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nЕдиничная сфера для нормы $L_2$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n$L_2$-норма обычно используется в $L_2$-регуляризации, также известной как [Ridge-регрессия](https://en.wikipedia.org/wiki/Ridge_regression). $L_2$-регуляризация добавляет штрафной член к функции потерь модели, приводя её коэффициенты к более или менее равномерному распределению.\n\nТакая регуляризация помогает предотвратить переобучение и улучшает способность модели к обобщению. Норма $L_2$ обеспечивает более сбалансированное решение, чем $L_1$-норма, распределяя штраф по всем коэффициентам. В отличие от $L_1$-регуляризации, она не зануляет веса признаков, а лишь сглаживает их, что делает модель более устойчивой и предотвращает вырождение матрицы признаков. $L_2$-регуляризация широко используется в линейной регрессии, логистической регрессии, нейронных сетях и SVM.\n\n### L∞-норма (максимальная)\n\nЭту норму также называют бесконечной нормой или нормой Чебышева. Она равна максимальному по модулю элементу вектора:\n\n$$\\|x\\|_\\infty = \\max_i |x_i|\n$$\n\nПример этой нормы для вектора:\n\n$$a=\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad \\|a\\|_{\\infty} = \\max{\\left(|{1}|, |-3|\\right)}=3\n$$\n\nРасстояние по бесконечной норме между двумя векторами:\n\n$$L_{\\infty}(a,b)=\\max_i{\\left(|a_i-b_i|\\right)}.\n$$\n\n$$\\\\ a=\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}, \\quad b= \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}, \\quad L_{\\infty}(a,b)=\\max{\\left(|1-3|,|-3-2|\\right)}=\n$$\n\n$$\\max{\\left(|-2|,|-5|\\right)}=5\n$$\n\nБесконечная норма отражает наибольший (или максимальный) вклад среди всех координат. Для одного вектора она равна наибольшему по модулю компоненту. А расстояние между двумя векторами в этой норме определяется как максимум модулей разностей соответствующих координат.\n\nБесконечная норма показывает максимальное расхождение по координатам и отражает поведение в крайнем направлении. Это полезно, например, в задачах, где важна наибольшая ошибка по одной из координат. В задачах отбора признаков или регуляризации можно использовать ограничение по $L_\\infty$-норме весов:\n\n$$\\min_w\\mathcal{L}(w),\\quad \\|w\\|_{\\infty}\\leq\\lambda\n$$\n\nЭто приводит к `clipping`-эффекту — все веса ограничены по модулю. То есть ни один признак не может оказать чрезмерно сильного влияния.\n\nВ $L_\\infty$-норме сфера радиуса 1 в двумерном линейном пространстве — это квадрат, выровненный по осям.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_6_36741e05b8.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Единичная сфера для нормы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nЕдиничная сфера для бесконечной нормы\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n### Норма Фробениуса\n\nДо этого момента мы рассматривали нормы векторов, которые измеряют размер или длину одного объекта — одномерного массива чисел. Однако во многих задачах машинного обучения и линейной алгебры основными объектами становятся матрицы: они могут представлять датасеты, линейные отображения, веса нейросетей или градиенты.\n\nЕстественный вопрос: как измерить такой размер или величину всей матрицы? Мы хотим обобщить понятие нормы с вектора на матрицу так, чтобы оно сохраняло полезные свойства — такие как интуитивность, согласованность с геометрией и возможность использовать в оптимизации.\nОдним из самых распространённых и понятных обобщений нормы на матрицы является норма Фробениуса. Она тесно связана с евклидовой нормой векторов и позволяет измерять «размер» всей матрицы.\n\n\u003e 💡Норма Фробениуса — это аналог евклидовой нормы ($L_2$) для матриц. Она равна квадратному корню из суммы квадратов всех элементов матрицы.\n\nЕё можно интерпретировать как $L_2$-норму вектора, если вытянуть все элементы матрицы в один длинный вектор:\n\n$$\\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2}\n\n$$\n\nПример расчёта:\n\n$$A= \\begin{pmatrix} 3 \u0026 1\\\\ 2 \u0026 3 \\end{pmatrix},\\quad \\|A\\|_F=\\sqrt{3^2+1^2+2^2+3^2}=\\sqrt{9+1+4+9}=\\sqrt{22}\n$$\n\nС геометрической точки зрения норма Фробениуса измеряет евклидову длину матрицы, рассматриваемой как вектор, полученный из всех её элементов.\n\nЧтобы визуализировать геометрический смысл, можно представить каждый столбец матрицы как вектор в пространстве, затем взять их длины и отложить их вдоль координатных осей. Получившиеся векторы будут ортогональны — то есть будут находиться друг к другу под прямым углом, это понятие мы ещё рассмотрим далее. Тогда длина диагонали (вектора $c$), соединяющей начало координат с концом суммы этих ортогональных векторов, и будет равна норме Фробениуса:\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_7_7_9ebd598fec.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация нормы Фробениуса\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация нормы Фробениуса\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПосчитаем длины векторов-столбцов:\n\n$$\\|a_1\\|_2=\\sqrt{3^2+2^2}=\\sqrt{9+3}=\\sqrt{12}\n$$\n\n$$\\|a_2\\|_2=\\sqrt{1^2+3^2}=\\sqrt{1+9}=\\sqrt{10}\n$$\n\nНовые вектора, отложенные по осям координат. будут:\n\n$$a'_1=\\begin{pmatrix} \\sqrt{12} \\\\ 0 \\end{pmatrix}, \\quad a'_2= \\begin{pmatrix} 0 \\\\ \\sqrt{10} \\end{pmatrix} \n$$\n\nТеперь найдём сумму этих векторов и обозначим её как вектор $c$, найдём его длину:\n\n$$c=a'_1+a'_2=\\begin{pmatrix} \\sqrt{12} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\sqrt{10} \\end{pmatrix}=\\begin{pmatrix} \\sqrt{12} \\\\ \\sqrt{10} \\end{pmatrix}\n$$\n\n$$\\\\ \\|c\\|_2=\\sqrt{\\left(\\sqrt{12}\\right)^2+\\left(\\sqrt{10}\\right)^2}=\\sqrt{12+10}=\\sqrt{22} \n$$\n\nИ эта длина равна норме Фробениуса матрицы $A$.\n\nПоэтому можно сказать, что норма Фробениуса геометрически характеризуется длиной вектора, представляющего собой сумму векторов базиса той же длины, что и каждый вектор, входящий в матрицу.\n\nВ регуляризации нейросетей (например, `weight decay`) часто штрафуют за большие веса. Именно норму Фробениуса от весов $\\|w\\|_F^2$ добавляют к функции потерь:\n\n$$L(w)=\\mathcal{L}(w)+\\lambda\\cdot\\|w\\|_F^2, \n$$\n\nгде:\n\n- $L(w)$ — функция потерь от весов нейросети до регуляризации;\n- $\\mathcal{L}\\mathcal(w)$ — первоначальная функция потерь от весов модели (например, MSE, CrossEntropy и др.);\n- $\\lambda$ — коэффициент регуляризации;\n- $\\|w\\|^2_F$ — квадрат нормы Фробениуса от весов модели.\n\nПомимо этого, норма Фробениуса удобна для сравнения матриц и довольно легко считается.\n\nОднако в некоторых задачах важно не просто измерить общую величину матрицы, а понять, насколько сильно она способна растягивать пространство, действуя как линейный оператор. Для этого вводится другая важная мера — `операторная норма`.\n\n### Операторная норма\n\nВ машинном обучении она важна при анализе устойчивости моделей, регуляризации, нормализации весов, а также для оценки чувствительности модели к входным данным.\n\n\u003e 💡`Операторная норма` — это максимальное растяжение, которое матрица $A$ может дать единичному вектору.\n\nПусть дана матрица $A \\in \\mathbb{R}^{m \\times n}$. Её операторной нормой (по $L_2$-норме) называется:\n\n$$\\|A\\|_2 = \\sup_{\\|x\\|_2=1} \\|Ax\\|_2\n$$\n\nгде $x$ — это вектор, на который действует матрица $A$, sup — это сокращение от слова `супремум` (англ. supremum), или наименьшая верхняя грань.\n\n{% cut \"Что такое супремум\" %}\n\nДля конечного набора значений супремум — это просто максимум. В случае бесконечного набора — верхняя его граница вне зависимости от того, включена эта граница в сам набор или нет.\n\nНапример:\n\n* Множество $\\{x\\in\\mathbb{R}|0\u003cx\u003c1\\}$. В этом множестве нет числа 1, но все числа в нём меньше 1. Супремум этого множества $\\sup=1$.\n* Множество: $\\{1,2,3\\}$. Здесь супремум совпадает с максимумом: $\\max=\\sup=3$.\n\n{% endcut %}\n\nМожно думать о матрице линейного оператора $A$ как о линейном преобразовании, которое поворачивает, растягивает и сжимает пространство. Операторная норма — это наибольшая длина, в которую может быть превращён любой вектор единичной длины при этом преобразовании.\n\nПусть у нас есть матрица $A$ и единичный вектор $x$:\n\n$$A= \\begin{pmatrix} 3 \u0026 0\\\\ 0 \u0026 4 \\end{pmatrix}, \\quad x= \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix};\n$$\n\n$$\\\\ \\|x\\|_2=\\sqrt{x_1^2+x_2^2}=1\\implies x_1^2+x_2^2=1 \n$$\n\nНайдем её операторную норму в евклидовой норме ($L_2$). Для этого вычислим произведение $Ax$ и $L_2$-норму получаемого вектора.\n\n$$Ax =\n\\begin{pmatrix}\n3 \u0026 0 \\\\\n0 \u0026 4\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3x_1 \\\\\n4x_2\n\\end{pmatrix}\n$$\n\n$$\\|Ax\\|_2 = \\sqrt{(3x_1)^2 + (4x_2)^2} = \\sqrt{9x_1^2 + 16x_2^2}\n\n$$\n\nЧтобы найти операторную норму, мы хотим максимизировать выражение $\\|Ax\\|_2$ При условии, что $x$ лежит на единичной окружности: $x_1^2+x_2^2=1$. Это задача на максимум функции на окружности, и он достигается в точке $(0,1)$:\n\n$$f(x_1,x_2): x_1^2+x_2^2=1\\implies \\max\\left(f(x_1,x_2)\\right)=f(0,1)\n$$\n\nПодставим этот максимум:\n\n$$\\|Ax\\|_2=\\sqrt{9x^2_1+16x_2^2}=\\sqrt{0+16}=\\sqrt{16}=4 \n$$\n\nТаким образом, операторная норма матрицы $A$ равна 4. Это наибольшее растяжение, которое она может вызвать, действуя на векторы единичной длины.\n\nСведем воедино всё, что мы узнали о нормах. В таблице ниже собраны основные характеристики каждой из рассмотренных метрик: формулы, геометрическая форма единичной сферы и типичные применения в машинном обучении.\n\n#|\n||\n\n**Норма**\n\n|\n\n**Обозначение**\n\n|\n\n**Формула**\n\n|\n\n**Геометрия**\n\n|\n\n**Применение в ML**\n\n||\n||\n\n$L_1$-**норма, Манхэттенская норма, норма такси (векторы)**\n\n|\n\n$\\|x\\|_1$\n\n|\n\n$\\sum_{i=1}^{n} \\|x_i\\|$\n\n|\n\nСумма расстояний по осям; расстояние по «сетке улиц»; сфера — ромб\n\n|\n\n$L_1$-регуляризация (Lasso-регрессия, SVM), отбор признаков\n\n||\n||\n\n$L_2$-**норма, Евклидова норма (векторы)**\n\n|\n\n$\\|x\\|_2$\n\n|\n\n$\\sqrt{\\sum_{i=1}^{n} x_i^2}$\n\n|\n\nОбычная длина вектора; расстояние по Пифагору; сфера — окружность\n\n|\n\n$L_2$-регуляризация (Ridge-регрессия, логистическая регрессия, SVM)\n\n||\n||\n\n**Бесконечная норма, норма Чебышева (векторы)**\n\n|\n\n$\\|x\\|_\\infty$\n\n|\n\n$\\max\\limits_i \\|x_i\\|$\n\n|\n\nМаксимальное отклонение вдоль осей; сфера — квадрат, выровненный по осям\n\n|\n\nОтбор признаков, регуляризация, ограничение весов\n\n||\n||\n\n**Норма Фробениуса (матрицы)**\n\n|\n\n$\\|A\\|_F$\n\n|\n\n$\\sqrt{\\sum\\limits_{i=1}^{m} \\sum\\limits_{j=1}^{n} a_{ij}^2}$\n\n|\n\nЕвклидова длина вектора, составленного из всех элементов матрицы\n\n|\n\nАналог $L_2$ для матриц: регуляризация весов, сравнение матриц, устойчивость, регуляризация нейросетей (`weight decay`)\n\n||\n||\n\n**Операторная норма (матрицы)**\n\n|\n\n$\\|A\\|_2$\n\n|\n\n$\\sup\\limits_{\\|x\\|_2 = 1} \\|Ax\\|_2$\n\n|\n\nМаксимальное растяжение единичного вектора при линейном преобразовании\n\n|\n\nКонтроль растяжения, сжимающие отображения, оценка чувствительности\n\n||\n|#\n\n***\n\nВот и всё!\n\nВ этом параграфе мы познакомились с пятью ключевыми нормами и увидели, что каждая из них задает собственную картинку расстояний в признаковом пространстве.\n\nСоветуем вам пройти квиз и двигаться дальше. В следующем параграфе мы перейдём от понятия «насколько далеко» к вопросу «в каком направлении» — другими словами, повторим скалярное произведение, косинусную меру, а также разберёмся c проекциями и ортогональностью — теми инструментами, что позволяют моделям опираться не только на длины, но и на углы между векторами.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801940.ecc0bb870ee59f9bdd10cd6c2b05cde9010ca55d?iframe=1\" frameborder=\"0\" name=\"ya-form-13801940.ecc0bb870ee59f9bdd10cd6c2b05cde9010ca55d\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6a:Tb14b,"])</script><script nonce="">self.__next_f.push([1,"В предыдущем параграфе мы говорили о том, насколько далеко расположены точки. Теперь нам нужно понять, под каким углом они смотрят друг на друга. Перейти от длин к ориентации позволяет скалярное произведение: оно связывает длины и направления — даёт угол между векторами и закладывает фундамент для `ортогональности` и `проекций`, которыми мы будем описывать растяжение линейных преобразований.\n\n## Вспомним скалярное произведение\n\n`Скалярное произведение` — один из ключевых способов описания взаимосвязи между векторами. Оно позволяет формализовать понятия длины и угла, вводить меры близости между объектами и понимать действие линейных моделей через геометрию проекций и разделяющих гиперплоскостей.\n\n\u003e 💡`Скалярным произведением` двух векторов $a$ и $b$ называется число, равное произведению длин этих векторов на косинус угла между ними:\n\u003e\n\u003e $$\\langle a, b \\rangle = \\|a\\| \\cdot \\|b\\| \\cdot \\cos \\alpha \n\u003e $$\n\u003e \n\u003e \n\nЗаметьте, что длина векторов выражена именно через $L_2$-норму, которую мы разбирали ранее. Данная формула справедлива только для этой нормы, так как именно она равна евклидовой длине вектора. И её можно также выразить через эту формулу как корень скалярного произведения вектора самого на себя.\n\nВ таком случае:\n\n$$\\cos{\\alpha}=\\cos 0=1\\implies \\langle a, a \\rangle = \\|a\\| \\cdot \\|a\\| \\cdot 1 \\implies \\|a\\|=\\sqrt{\\langle a, a \\rangle} \n$$\n\nВ пространстве $\\mathbb{R}^n$ скалярное произведение также можно вычислить как сумму попарных произведений координат:\n\n$$\\langle a, b \\rangle = \\sum_{i=1}^n a_i b_i \n$$\n\nКроме того, скалярное произведение можно трактовать как произведение длины одного вектора на длину `проекции` другого вектора на его направление.\n\n## Проекция и связь со скалярным произведением\n\nПриведём еще одно, «проекционное», определение скалярного произведения:\n\n\u003e 💡`Скалярное произведение двух векторов` — это произведение длины проекции одного вектора на направление другого и длины этого второго вектора:\n\u003e\n\u003e $$\\langle a, b \\rangle = \\|\\mathrm{proj}_b a\\| \\cdot \\|b\\| = \\|\\mathrm{proj}_a b\\|\\cdot \\|a\\| \n\u003e $$\n\u003e \n\u003e \n\n`Проекция` вектора $a$ на вектор $b$ — это вектор, лежащий на прямой, проходящей через $b$, который получается, если из конца вектора $a$ опустить перпендикуляр на эту прямую.\n\nВекторное выражение проекции:\n\n$$\\mathrm{proj}_b a = \\frac{\\langle a, b \\rangle}{\\|b\\|^2} \\cdot b \n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_1_a4fe276edb.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Проекция вектора $a$ на вектор $b$\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПроекция вектора $a$ на вектор $b$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНапример, если у нас есть два вектора:\n\n$$a = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} \n$$\n\nТо проекция вектора $a$ на вектор $b$ будет рассчитываться следующим образом с использованием $L_2$-нормы:\n\n$$\\mathrm{proj}_b a = \\frac{\\langle a, b \\rangle}{\\|b\\|_2^2} \\cdot b = \\frac{3\\cdot4+5\\cdot2}{4^2+2^2}\\cdot\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}= \\frac{22}{20}\\cdot\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}= \\frac{11}{10}\\cdot\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}= \n$$\n\n$$\\begin{pmatrix} \\frac{44}{10} \\\\ \\frac{22}{10} \\end{pmatrix}=\\begin{pmatrix} 4.4 \\\\ 2.2 \\end{pmatrix}\n$$\n\n## Косинусная мера\n\nИз формулы скалярного произведения как $\\langle a, b \\rangle = \\|a\\| \\cdot \\|b\\| \\cdot \\cos \\alpha$ сразу следует способ выразить угол между векторами:\n\n$$\\cos \\alpha = \\frac{\\langle a, b \\rangle}{\\|a\\|_2 \\cdot \\|b\\|_2} \n$$\n\nЭто лежит в основе `косинусной меры` — важной метрики в анализе текстов, изображений, эмбеддингов и рекомендательных систем:\n\n* Если $\\cos \\alpha \\approx 1$, то объекты близки по направлению, семантически похожи.\n* Если $\\cos \\alpha \\approx 0$, то объекты ортогональны, независимы.\n* Если $\\cos \\alpha\\approx -1$, то объекты противоположны по смыслу.\n\nВ обработке текстов (англ. NLP, Natural Language Processing) косинусная мера применяется при сравнении [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)-векторов документов: она позволяет находить документы с похожим содержанием, даже если их объёмы сильно различаются.\n\n{% cut \"Подробнее про TF-IDF\" %}\n\nTF-IDF (Term frequency-inverse document frequency — дословно, «частота термина — обратная документная частота») — это базовый метод представления текстов в виде числовых векторов.\n\nОн оценивает важность слова для конкретного документа в контексте всей коллекции документов. Значение TF-IDF для термина $t$ в документе $d$ определяется как произведение:\n\n$$\\mathrm{tfidf}(t,d)=\\mathrm{tf}(t,d)\\cdot\\mathrm{idf}(t), \n$$\n\nгде:\n\n* $\\mathrm{tf}(t,d)$ — частотность термина в документе: $\\mathrm{tf}(t,d) = \\frac{f_{t,d}}{\\text{длина }d}$;\n* $\\mathrm{idf}(t)$ — обратная частота в корпусе: $\\mathrm{idf}(t) = \\log\\left(\\frac{N}{n_t}\\right)$, где $N$ — общее число документов, $n_t$ — число документов, содержащих $t$.\n\nТаким образом, высокое значение TF-IDF означает, что слово часто используется в текущем документе, но редко встречается в других. Это снижает вес «пустых» слов вроде «и», «в», «на», которые называются *стоп-словами* (stopwords): они встречаются во всех документах и потому имеют низкий $\\mathrm{idf}$.\n\nПусть слово «алгоритм» встречается 5 раз в документе $d$, содержащем 100 слов, и встречается в 10 из 1000 документов. Тогда:\n\n* $\\mathrm{tf} = \\frac{5}{100} = 0.05$\n* $\\mathrm{idf} = \\log\\left(\\frac{1000}{10}\\right) = \\log(100) = 2$\n* $\\mathrm{tfidf} = 0.05 \\cdot 2 = 0.1$\n\nTF-IDF часто используется как:\n\n* Вход для тематических моделей (LSA, NMF).\n* Признаковое описание для классификации и кластеризации текстов.\n* Основа для построения векторного пространства в поисковых системах.\n\n{% endcut %}\n\nТаким образом, косинусная мера превращает угол между векторами в численную близость. Отдельный граничный случай — это когда $\\cos\\alpha = 0$. В нем схожесть обнуляется, а сами векторы образуют угол $90^{\\circ}$. Это состояние называется `ортогональностью` и играет особую роль в линейной алгебре: от отбора признаков до устойчивых разложений матриц. Поговорим о нём подробнее.\n\n## Ортогональность и ортогонализация\n\nОртогональность — это геометрический способ сказать «совершенно независимы»: скалярное произведение равно нулю, а угол между векторами составляет $90^{\\circ}$. Векторы-столбцы ортогональной матрицы образуют идеальный базис: единичной длины, взаимно перпендикулярны и, как покажем дальше, сильно упрощают вычисления (проекции, QR-разложение, метод главных компонент).\n\nСначала уточним формальное определение.\n\n### Ортогональность и линейная независимость\n\n`Ортогональность` — это фундаментальное понятие в линейной алгебре и геометрии, лежащее в основе проекций, разложений и [оптимизации](https://habr.com/ru/articles/813221/).\n\n\u003e 💡Чтобы система векторов $\\{z_1, …, z_j\\}$\u0026nbsp;считалась ортогональной, каждый её вектор должен быть ортогонален остальным.\u0026nbsp;Это означает, что они расположены под углом ${90}^{\\circ}$ друг к другу, поэтому их попарное скалярное произведение должно быть равно нулю:\n\u003e\n\u003e $$\\langle z_i, z_k \\rangle = 0 \\quad \\text{для всех } i \\ne k. \n\u003e $$\n\u003e \n\u003e \n\nЭто кажется вполне очевидным, учитывая формулу, которую мы рассмотрели в разделе о скалярном произведении векторов:\n\n$$\\langle a, b \\rangle = \\|a\\|_2 \\cdot \\|b\\|_2 \\cdot \\cos \\alpha \n$$\n\nСкалярное произведение равно нулю только в случае, когда длина одного из векторов нулевая либо когда $\\cos\\alpha=0$, то есть когда угол $\\alpha$ равен ${90}^{\\circ}$. По этой причине формально по определению любой вектор ортогонален нулевому. Хотя угол между любым вектором и нулевым не определён, поскольку у нулевого вектора нет направления.\n\nПри этом если векторы не только ортогональные, но и имеют единичную длину, то базис из таких векторов называют `ортонормированным`. В таком случае мы можем сказать, что $z_1, …, z_j$ образует `ортонормированный базис` в пространстве исходных векторов.\n\nПонятие ортогональности касается не только векторов, но и матриц.\n\n\u003e 💡`Ортогональная матрица` — это квадратная матрица, столбцы (или строки) которой образуют ортонормированный базис.\n\nНапример ортогональной матрицы:\n\n$$Q=\\begin{pmatrix} 0 \u0026 1 \\\\ -1 \u0026 0 \\end{pmatrix} \n$$\n\nТо есть это матрица, у которой:\n\n* Каждый столбец имеет единичную длину (норму).\n* Любые два различных столбца ортогональны друг другу.\n\n{% cut \"**Пример**\" %}\n\n**Единичная норма.** И действительно, если мы рассмотрим, например, $L_2$-нормы векторов матрицы $Q$, они будут равны единицам:\n\n$$q_1=\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad \\|q_1\\|_2=\\sqrt{0^2+(-1)^2}=\\sqrt{1}=1;\\\\ q_2=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\|q_2\\|_2=\\sqrt{1^2+0^2}=\\sqrt{1}=1. \n$$\n\n**Ортогональность.** Если мы рассмотрим скалярное произведение векторов матрицы Q, то оно окажется нулевым. Это означает, что векторы ортогональны друг другу:\n\n$$\\langle q_1, q_2\\rangle=0\\cdot1+(-1)\\cdot0=0 \n$$\n\n{% endcut %}\n\nФормально матрица $Q \\in \\mathbb{R}^{n \\times n}$ называется ортогональной, если $Q\\cdot Q^T=I$, где $I$ — единичная матрица. Это равносильно тому, что $Q^{-1} = Q^T$.\n\nГлавная идея, стоящая за ортогонализацией, — это преобразование линейно независимого набора векторов в ортогональный базис. Это упрощает вычисления, проекции и интерпретацию данных, особенно в машинном обучении и анализе признаков.\n\nБлагодаря этой ортогональности, например, главные компоненты в анализе главных компонент (PCA) удобно интерпретировать: каждый из них фиксирует новое направление в пространстве данных, не зависящее от предыдущих. Это упрощает анализ и визуализацию сложных многомерных данных.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_2_6a94073d9e.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Главные компоненты как ортогональные направления.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГлавные компоненты как ортогональные направления. На графике оси $x_1$ и $x_2$ соответствуют исходным признакам, а векторы $z_1$ и $z_2$ — главным компонентам. Они взаимно ортогональны и указывают направления наибольшей и второй по значимости дисперсии в данных соответственно\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ целом, работа с данными в ортонормированном базисе предпочтительна, потому что:\n\n* Упрощаются вычисления: скалярные произведения, проекции и координаты векторов легко вычисляются.\n* Повышается численная устойчивость: особенно при решении систем уравнений и в регрессии. Например, в [Principal Component Regression (PCR)](https://en.wikipedia.org/wiki/Principal_component_regression) признаки сначала преобразуют с помощью PCA в ортогональный базис, чтобы устранить [мультиколлинеарность](https://en.wikipedia.org/wiki/Multicollinearity) (ситуация, когда некоторые признаки линейно зависимы).\n* Геометрическая интерпретация становится ясной: векторы единичной длины и под прямыми углами легко анализировать.\n* Устраняется избыточность: переход к ортонормированному базису позволяет оставить только независимые направления и отбросить линейно зависимые компоненты.\n\nТаким образом, ортонормированный базис — это не просто удобство, а важный инструмент для надёжного и эффективного анализа данных.\n\nНо как такой базис получить на практике? На руках у нас обычно не ортогональные, а просто линейно-независимые признаки, то есть столбцы исходной матрицы данных, компоненты градиента, векторы состояний.\n\nЧтобы воспользоваться всеми преимуществами ортонормированной системы, нужно преобразовать эти векторы, не потеряв информации. Классический алгоритм, который делает это последовательно и конструктивно, — это метод Грама — Шмидта, который мы сейчас разберём.\n\n## Метод Грама — Шмидта\n\nДопустим, у нас есть набор *линейно независимых* векторов $\\{v_1, v_2,..., v_k\\}$. Исходя из этого набора векторов, мы хотим получить ортогональные векторы $\\{f_1, f_2,...,f_k\\}$. Для этих целей существует метод Грама — Шмидта. Это алгоритм, позволяющий из любого набора линейно независимых векторов построить ортогональный базис.\n\nАлгоритм довольно прост.\n\n### Шаг №1\n\nПриравниваем один из векторов к $f_1$. Например:\n\n$$f_1=v_1 \n$$\n\n### Шаг №2\n\nДля каждого нового вектора $v_{p+1}$ вычитаем проекции на все ранее построенные векторы $f_i$:\n\n$$f_{p+1}=v_{p+1}-\\sum\\limits_{i=1}^p\\frac{\\langle v_{p+1},f_i\\rangle}{\\langle f_i,f_i \\rangle}\\cdot f_i \n$$\n\nМы уже знаем формулу под знаком суммы — это формула проекции, которую мы рассматривали чуть [выше](https://education.yandex.ru/handbook/math/article/proektsii-ugli-i-ortogonalnost#proekciya-i-svyaz-so-skalyarnym-proizvedeniem). То есть все остальные векторы являются результатом корректировки соответствующих векторов $v$ на проекции на уже полученные векторы ортогонального базиса:\n\n$$f_2 = v_2 - \\frac{\\langle v_{2}, f_1 \\rangle}{\\langle f_1, f_1 \\rangle} \\cdot f_1 \n$$\n\n$$f_{3} = v_{3} - \\sum_{i=1}^2 \\frac{\\langle v_{3}, f_i \\rangle}{\\langle f_i, f_i \\rangle} \\cdot f_i \n$$\n\n$$... \n$$\n\n$$f_{k} = v_{k} - \\sum_{i=1}^{k-1} \\frac{\\langle v_{k}, f_i \\rangle}{\\langle f_i, f_i \\rangle} \\cdot f_i \n$$\n\nЭто обеспечивает ортогональность $f_{p+1}$ ко всем предыдущим $f_1, \\dots, f_p$.\n\nС помощью метода Грама — Шмидта можно получить не только ортогональный базис, но и ортонормированный. Для этого векторы следует масштабировать, разделив получаемый вектор на его длину. Векторы ортонормированного базиса обозначим как $\\{q_1,q_2,...,q_k\\}$:\n\n$$q_1 = \\frac{f_1}{\\|f_1\\|_2}\n$$\n\n$$q_2 = \\frac{f_2}{\\|f_2\\|_2}\n$$\n\n$$...\n$$\n\n$$q_k = \\frac{f_k}{\\|f_k\\|_2}\n$$\n\n### Пример\n\nПусть у нас есть два линейно независимых (в двумерном пространстве это значит, что они не лежат на одной прямой) не ортогональных вектора $v_1$ и $v_2$:\n\n$$v_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} \n$$\n\nВ двумерном пространстве векторы $v_1$ и $v_2$ выглядят следующим образом.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_3_faf10a23ac.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Векторы $v_1$ и $v_2$\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВекторы $v_1$ и $v_2$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nИдём по алгоритму метода Грама — Шмидта.\n\n**Шаг №1**\n\nБерём первый вектор без изменений:\n\n$$f_1=v_1=\\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} \n$$\n\n**Шаг №2**\n\nВычитаем из $v_2$ сумму проекций вектора $v_2$ на уже полученные ранее векторы $f$. Мы получили лишь один вектор $f_1$, потому вычитаем лишь одну проекцию:\n\n$$\\mathrm{proj}_{f_1}v_2 = \\frac{\\langle v_2, f_1 \\rangle}{\\langle f_1, f_1 \\rangle} \\cdot f_1 = \\frac{3 \\cdot 0 + (-3) \\cdot 2}{(-3) \\cdot (-3)} \\cdot \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}= \n$$\n\n$$ \\frac{0 - 6}{9} \\cdot \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = -\\frac{2}{3} \\cdot \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\n\n$$f_2 = v_2 - \\mathrm{proj}_{f_1}v_2 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n$$\n\nВот мы и получили ортогональный базис. С геометрической точки зрения это означает , что из вектора $v_2$ была удалена составляющая вдоль $f_1$, в результате чего он повернулся на угол $\\alpha$ до прямого угла с $f_1$. При этом изменилась и длина вектора: новый вектор $f_2$ стал короче и оказался перпендикулярен $f_1$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_4_318f6cdb0a.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Векторы $v_1$, $v_2$ и ортогональный базис\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВекторы $v_1$, $v_2$ и ортогональный базис\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nИз нашего примера в двумерном пространстве можно понять, что в пространствах большей размерности метод Грама — Шмидта действует итеративно: каждый новый вектор $v_k$ поочерёдно «разворачивается» относительно уже построенных векторов ${f_1, f_2, ..., f_{k-1}}$ так, чтобы стать ортогональным каждому из них.\n\nГеометрически это означает, что из вектора $v_k$ вычитается его проекция на подпространство, натянутое на ${f_1, ..., f_{k-1}}$ — то есть на все ранее построенные направления. В результате получается новый вектор $f_k$, который перпендикулярен этому подпространству и, следовательно, всем предыдущим векторам $f_i$.\n\nТеперь применим нормирование на длину векторов, чтобы получить из ортогонального базиса ортонормированный:\n\n$$\\|f_1\\|_2 = \\sqrt{0^2 + (-3)^2} = 3 \\\\[1em]\nq_1 = \\frac{f_1}{\\|f_1\\|_2} = \n\\begin{pmatrix} \\frac{0}{3} \\\\ \\frac{-3}{3} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\n\n$$\\|f_2\\|_2 = \\sqrt{3^2 + 0^2} = 3 \\\\[1em]\nq_2 = \\frac{f_2}{\\|f_2\\|_2} = \n\\begin{pmatrix} \\frac{3}{3} \\\\ \\frac{0}{3} \\end{pmatrix} =\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n\n{% cut \"Пример реализации метода Грама — Шмидта на `Python`:\" %}\n\n```Python\nimport numpy as np\n\ndef gram_schmidt(V):\n   Q = []\n   for v in V:\n       for q in Q:\n           v = v - np.dot(v, q) * q\n       v = v / np.linalg.norm(v)\n       Q.append(v)\n   return np.array(Q)\n\n# Пример\nV = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]])\nQ = gram_schmidt(V)\nНадо print(\"Ортонормированный базис:\\\\n\", Q)\n```\n\n{% endcut %}\n\nПри работе с реальными данными нас обычно интересует не только сам набор ортонормированных векторов $q_1,\\dots,q_k$, но и то, как они соотносятся с исходной матрицей признаков $A$.\n\nЕсли применить метод Грама — Шмидта к каждому столбцу $A$, столбцы-векторы превращаются в ортонормированный набор $Q$, а все коэффициенты проекций, вычитавшиеся по ходу алгоритма, аккуратно собираются в верхнетреугольную матрицу $R$. Так естественно возникает `QR-разложение`:\n\n$$A \\;=\\; Q\\,R, \n$$\n\nгде $Q$ уже готов к проекциям, а $R$ — к быстрым обратным ходам. Рассмотрим эту факторизацию подробнее.\n\n## QR-разложение\n\nЧтобы повысить численную устойчивость, решать плохо обусловленные задачи, определять размерность данных, определять главные направления изменчивости, выявлять зависимые и шумовые компоненты и многое другое, существует разложение матриц.\n\nРазложения матриц — это один из краеугольных камней машинного обучения, и они используются практически повсеместно, даже если это не всегда видно на поверхности.\n\n\u003e 💡`Разложение`, или `факторизация`, — это разделение исходной матрицы на произведение нескольких более простых матриц.\n\n`QR-разложение` — один из ключевых инструментов линейной алгебры, который находит применение в решении линейных систем, задачах метода наименьших квадратов, вычислениях собственных значений и т. д. Его численная стабильность и эффективность делают его ценным методом в широком спектре приложений.\n\n\u003e 💡Формально `QR-разложение` представляет собой матричную факторизацию, при которой матрица $A$ раскладывается на произведение ортогональной матрицы $Q$ и верхнетреугольной матрицы $R$:\n\u003e\n\u003e $$A = QR \n\u003e $$\n\u003e \n\u003e \n\nQR-разложение — это геометрический способ упростить вычисления и повысить численную устойчивость в задачах машинного обучения. Особенно это важно, когда мы решаем задачу линейной регрессии, то есть минимизации функции потерь:\n\n$$\\min |X\\beta - y|^2 \n$$\n\nВместо обращения матрицы $X^TX$, можно использовать QR-разложение:\n\n* Разложим: $X = QR$\n* Подставим: $|QR\\beta - y|^2$\n* После упрощения: $\\beta = R^{-1} Q^T y$\n\nQR-разложение также используется для устранения мультиколлинеарности, устойчивого вычисления решений без обращения матриц, получения ортогонального базиса в методе главных компонент, проецирования произвольного вектора $y$ на пространство столбцов $X$ с помощью матрицы $Q$:\n\n$$\\hat{y}=QQ^Ty \n$$\n\nЭто прямое применение QR-разложения в регрессии, аппроксимации и снижении размерности. Реализуем это разложение сразу на примере. Допустим, у нас есть некая матрица $A$:\n\n$$A=\\begin{pmatrix} 0 \u0026 3\\\\ -3 \u0026 2 \\end{pmatrix} \n$$\n\nЧтобы представить эту матрицу в виде произведения $A = QR$, нужно ортонормировать матрицу $A$. Для этого мы можем воспользоваться методом Грама — Шмидта. Возьмём столбцы нашей матрицы, — те самые векторы $v_1$ и $v_2$, которые мы уже ортонормировали в предыдущем разделе, — и применим к ним этот метод. В результате мы получим матрицу $Q$:\n\n$$Q=\\begin{pmatrix} 0 \u0026 1\\\\ -1 \u0026 0 \\end{pmatrix} \n$$\n\nВывести верхнетреугольную матрицу $R$ из разложения — уже простая задача:\n\n$$A = QR \\implies R=Q^{-1}A=Q^TA \\\\\n$$\n\n$$R=Q^TA=\\begin{pmatrix} 0 \u0026 1\\\\ -1 \u0026 0 \\end{pmatrix}^T\\cdot \\begin{pmatrix} 0 \u0026 3\\\\ -3 \u0026 2 \\end{pmatrix}=\\begin{pmatrix} 0 \u0026 -1\\\\ 1 \u0026 0 \\end{pmatrix}\\cdot \\begin{pmatrix} 0 \u0026 3\\\\ -3 \u0026 2 \\end{pmatrix}= \n$$\n\n$$\\begin{pmatrix} 3 \u0026 -2\\\\ 0 \u0026 3 \\end{pmatrix} \n$$\n\nМы получили QR-разложение матрицы $A$:\n\n$$A=QR= \\begin{pmatrix} 0 \u0026 1\\\\ -1 \u0026 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 3 \u0026 -2\\\\ 0 \u0026 3 \\end{pmatrix} \n$$\n\nРезультат наших вычислений можно проверить:\n\n$$\\begin{pmatrix} 0 \u0026 1\\\\ -1 \u0026 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 3 \u0026 -2\\\\ 0 \u0026 3 \\end{pmatrix}= \\begin{pmatrix} 0 \u0026 3\\\\ -3 \u0026 2 \\end{pmatrix} \n$$\n\nС геометрической точки зрения, QR-разложение переводит исходное признаковое пространство в новую систему координат, где признаки становятся ортонормированными, независимыми и легко интерпретируемыми.\n\nМы берём признаковое пространство и «выпрямляем» его: создаём координатную сетку, где признаки становятся перпендикулярны друг другу и имеют единичную длину. Это делает операции, такие как проекции, приближения или поиск ближайших точек, простыми и наглядными, а геометрию признаков удобной для вычислений и анализа.\n\n{% cut \"Готовые функции `NumPy` и `SciPy` для QR-разложения\" %}\n\n```Python\nimport numpy as np\n\nA = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]])\nQ, R = np.linalg.qr(A)\nprint(\"Q =\\\\n\", Q)\nprint(\"R =\\\\n\", R)\n```\n\n{% endcut %}\n\n## Ортогональная проекция\n\nВ предыдущих разделах мы уже касались понятия `проекция`. Пришло время конкретизировать это понятие и сосредоточиться для наиболее важном для нас типе проекций.\n\nОртогональная проекция — это фундаментальное понятие в линейной алгебре и машинном обучении. Простейший пример — это проецирование некоторого вектора на подпространство (например, плоскость или прямую) таким образом, что результирующая проекция является ближайшей точкой подпространства к изначальному вектору.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_5_6d377b9021.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Пример проекции.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПример проекции. Ортогональная проекция вектора $v$ на прямую $L$. Пунктирной линией показан перпендикуляр от конца вектора $v$ к прямой $L$, соединяющий его с точкой проекции\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nМы уже проделывали это с векторами в предыдущих разделах — это была именно ортогональная проекция. Теперь введём определение:\n\n\u003e 💡`Ортогональная проекция` вектора $a$ на подпространство $W$ — это такой вектор $p \\in W$, который удовлетворяет двум условиям.\n\u003e **Условие №1.** Он находится ближе всего к вектору $a$ среди всех векторов подпространства $W$ (т. е. минимизирует расстояние):\n\u003e\n\u003e $$\\|a-p\\|=\\min_{w\\in W}\\|a-w\\| \n\u003e $$\n\u003e \n\u003e **Условие №2.** Разность $a-p$ ортогональна всему подпространству $W$:\n\u003e\n\u003e $$\\forall w \\in W\\quad \\langle a-p,w \\rangle = 0 \n\u003e $$\n\u003e \n\u003e \n\nПоследний пункт в определении по сути означает путь от вектора $a$ до вектора $p$ и, следовательно, до подпространства $W$. Так как ортогональная проекция проводится перпендикулярно подпространству (что следует из условия кратчайшего расстояния), разность $a-p$ не что иное как кратчайший путь от подпространства $W$ до вектора $a$. Если представить этот путь вектором (такой вектор называют остатком), то его норма будет наименьшей среди всех векторов, соединяющих подпространство $W$ и вектор $a$.\n\nНеобходимость в ортогональных проекциях возникает в задачах, когда нам нужно приблизить сложные данные с помощью простой модели.\n\nНапример, в линейной регрессии мы хотим выразить целевой признак $y$ через линейную комбинацию признаков $X$. Однако почти никогда не бывает так, чтобы признак $y$ лежал точно в подпространстве, порождённом векторами признаков $X$. Тут нам и нужна ортогональная проекция.\n\nОна помогает нам найти такой вектор в подпространстве признаков $X$, который наиболее точно приближает признак $y$ в смысле евклидовой нормы. Именно этот пример мы далее наглядно разберём.\n\n### Геометрическая интерпретация линейной регрессии и метода наименьших квадратов (МНК)\n\nТеперь когда мы знаем, что ортогональная проекция — это кратчайший путь от точки до подпространства, можно сделать вывод, что сама линейная регрессия сводится к поиску такой ортогональной проекции. В задаче линейной регрессии мы хотим найти такой вектор $\\beta$, чтобы линейная комбинация признаков $X\\beta$ максимально приближала вектор целевой переменной $y$:\n\n$$\\hat{y}=X\\beta \n$$\n\nГде $\\hat{y}$ — предсказания модели. Мы хотим найти такое значение параметров $\\beta$-модели, чтобы вектор предсказанных значений $\\hat{y}$ был максимально близок по $L_2$-норме в пространстве, натянутом на столбцы матрицы $X$, к вектору реальных значений целевой переменной $y$:\n\n$$\\beta = \\argmin_\\beta\\|y-\\hat{y}\\|^2_2 \n$$\n\nИменно так формулируется `метод наименьших квадратов` (МНК) — он минимизирует сумму квадратов отклонений между предсказаниями модели и фактическими значениями.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_6_8cdf2f2eb8.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Линия регрессии.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nЛиния регрессии (показана светло синим) и отклонения (вертикальные пунктирные линии) между предсказаниями $\\hat{y}$ и наблюдаемыми значениями (синие точки). Метод наименьших квадратов минимизирует сумму квадратов этих отклонений\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЕсли мы взглянем на определение ортогональной проекции (в частности, условие минимизации расстояния), то увидим, что задача линейной регрессии по методу наименьших квадратов эквивалентна поиску проекции вектора $y$ на подпространство, натянутое на столбцы матрицы $X$, — это все возможные линейные комбинации векторов-столбцов матрицы $X$. Такое пространство обычно обозначают $\\mathrm{span}(x_1, x_2,…, x_n)$, где $x_1, x_2, …, x_n$ — столбцы матрицы $X$.\n\nПри этом $\\hat{y}=X\\beta$ — это и есть сама ортогональная проекция вектора значений целевой переменной $y$ — аналог вектора $p$ в нашем определении ортогональной проекции. Параметры $\\beta$ определяются так, чтобы остаток $y-\\hat{y}$ — аналог разности $a-p$ в определении — имел наименьшую длину. То есть чтобы был перпендикулярен пространству признаков.\n\nПроекцию $y$ можно, разумеется, найти напрямую и без явного вычисления параметров $\\beta$. Однако в машинном обучении и регрессии нас интересует не только приближение конкретного вектора $y$, но и возможность обобщения на новых данных. Именно вектор $\\beta$ позволяет выразить ортогональную проекцию как линейную комбинацию признаков. Без него мы не сможем делать предсказания на новых входах $X_{\\mathrm{new}}$, поскольку сама модель не будет определена:\n\n$$\\hat{y}=X_{\\mathrm{new}}\\beta \n$$\n\nНа рисунке изображен красный вектор $y$ — вектор реальных значений целевой переменной, который мы хотим аппроксимировать при помощи своей модели через два признака $x_1$ и $x_2$. Они образуют признаковое подпространство $\\mathrm{span}(x_1,x_2)$ — плоскость в трёхмерном пространстве.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_8_6_1_c6cac6e345.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация геометрического смысла МНК.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация геометрического смысла МНК\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЧтобы сделать предсказание через эти признаки, нам надо найти такой вектор в подпространстве $\\mathrm{span}(x_1,x_2)$, который будет наиболее близок к реальным значениям целевой переменной, т.е. к вектору $y$. Мы уже знаем, что вектор, наиболее близкий к вектору $y$ в подпространстве $\\mathrm{span}(x_1,x_2)$, — это его ортогональная проекция на это подпространство. Это и есть светло синий вектор $\\hat{y}$.\n\nПерпендикуляр от вектора $y$ к признаковому подпространству $\\mathrm{span}(x_1,x_2)$ проходит через точку $\\hat{y}$ — ортогональную проекцию $y$. Вектор разности $y - \\hat{y}$ показывает наименьшее расстояние от $y$ до подпространства.\n\n***\n\nВ этом параграфе мы сосредоточились на ориентации векторов: вспомнили скалярное произведение и косинусную меру, научились строить ортогональные проекции, а также выпрямлять базисы методом Грама — Шмидта и QR-разложением. Эти инструменты превращают линейные модели в наглядные геометрические операции и делают вычисления устойчивее.\n\nДальше мы планируем поднять планку: перейдём к спектральным методам — разложению матриц на собственные и сингулярные компоненты. Там те же идеи ортогональности и норм проявятся в SVD-, PCA- и спектральном разложении, позволяя оценивать силу направлений, сжимать данные и строить ещё более устойчивые модели.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801948.b4512c0001700010e924886efdb48ab9badaea90?iframe=1\" frameborder=\"0\" name=\"ya-form-13801948.b4512c0001700010e924886efdb48ab9badaea90\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6b:T132e6,"])</script><script nonce="">self.__next_f.push([1,"В предыдущем параграфе мы изучили геометрию пространств признаков: нормы, расстояния, углы и проекции.\n\nТеперь мы поднимемся на уровень выше и рассмотрим структуру этих пространств — с помощью спектральных методов и разложений матриц. Они играют центральную роль в машинном обучении, так как позволяют:\n\n- понять внутреннее устройство данных;\n- находить направления наибольшей изменчивости; \n- решать задачи регрессии и классификации эффективно и устойчиво;\n- понижать размерность без значительных потерь информации;\n- сжимать изображения и другие сигналы;\n- быстро решать линейные системы уравнений.\n\nВ этом параграфе мы обсудим:\n\n- что означают собственные значения и векторы и как их интерпретировать;\n- как с помощью сингулярного разложения (SVD) приближать данные с минимальной потерей качества;\n- зачем нужны разложения QR и Cholesky и как они связаны с численной устойчивостью;\n- как с помощью ранга и числа обусловленности понять, можно ли обучать модель на данных без потерь.\n\nДавайте приступим!\n\n## Собственные значения и векторы, диагонализация, спектральный радиус\n\nДопустим, у нас есть некая квадратная матрица $A$. Вы уже успели ознакомиться с правилами умножения матриц и векторов. И вам точно известно, что если умножить матрицу на вектор, то на выходе мы получим вектор того же размера, на который мы производим умножение матрицы (при соблюдении размерностей для умножения, разумеется):\n\n$$A_{n\\times n}\\cdot a_{n} = b_n \n$$\n\nДля любой квадратной матрицы над комплексными числами существует минимум один такой ненулевой вектор, при умножении данной матрицы на который получается ровно тот же вектор, умноженный на некоторую константу:\n\n$$A\\cdot v = \\lambda \\cdot v, \\quad v\\neq0 \n$$\n\nТакой вектор $v$ называется `собственным вектором` данной матрицы $A$. А константа $\\lambda$ называется `собственным значением` или `собственным числом` данной матрицы $A$.\n\n\u003e 💡`Собственный вектор` — это такой вектор, который сохраняет своё направление при действии матрицы, а собственное значение показывает, во сколько раз он при этом растягивается или сжимается.\n\nЧтобы найти собственные значения и векторы матрицы $A \\in \\mathbb{R}^{n \\times n}$, нужно:\n\n**Шаг №1.** Составить `характеристическое уравнение`:\n\n$$\\det(A-\\lambda I)=0 \n$$\n\nЭто полиномиальное уравнение степени $n$, где $\\lambda$ — собственное значение.\n\n{% cut \"Подробнее про характеристическое уравнение\" %}\n\nПри изучении собственных значений и собственных векторов центральную роль играет характеристическое уравнение. Оно задаёт условие, при котором у квадратной матрицы $A$ существует ненулевой вектор $v$, такой что\n\n$$Av=\\lambda v \n$$\n\nгде $\\lambda$ — скаляр (собственное значение), а $v \\neq 0$ — соответствующий собственный вектор.\n\nИсходное определение можно переписать так:\n\n$$Av=\\lambda v \\iff Av-\\lambda Iv=0\\iff (A-\\lambda I)v=0 \n$$\n\nгде $I$ — единичная матрица такого же размера, что и $A$. Для такой системы ненулевое решение $v$ существует только в том случае, если матрица $A - \\lambda I$ необратима. А матрица необратима тогда и только тогда, когда её определитель равен нулю:\n\n$$\\det(A-\\lambda I) =0 \n$$\n\nЭто уравнение называется `характеристическим уравнением` матрицы $A$, а выражение $\\det(A - \\lambda I)$ называется характеристическим многочленом. Переменная $\\lambda$ в нём играет роль неизвестного, и решение этого многочлена даёт все собственные значения матрицы. Если матрица имеет размер $n \\times n$, то характеристический многочлен является многочленом степени $n$, а его корни, с учётом кратности, составляют спектр матрицы.\n\nСмысл характеристического уравнения состоит в том, что оно описывает все такие значения $\\lambda$, при которых матрица $A - \\lambda I$ теряет обратимость, а линейное отображение $x \\mapsto Ax$ имеет ненулевые направления, которые масштабируются в точности в $\\lambda$ раз. Каждое собственное значение соответствует по крайней мере одному собственному вектору, направление которого не меняется при действии матрицы — лишь масштабируется его длина. Далее в ходе текущего параграфа мы об этом поговорим подробнее.\n\n{% endcut %}\n\n**Шаг №2.** Решить его и найти корни — они и есть собственные значения $\\lambda_1$, $\\lambda_2$, …\n\n**Шаг №3.** Для каждого найденного $\\lambda$ найти собственный вектор:\n\n$$(A-\\lambda I)v=0 \n$$\n\nЭто однородная система, решение которой даёт соответствующее $v$.\n\n### Пример\n\nНапример, у нас есть матрица:\n\n$$A= \\begin{pmatrix} -1 \u0026 -6\\\\ 2 \u0026 6 \\end{pmatrix} \n$$\n\nНайдём для неё собственные значения и векторы.\n\n**Шаг №1.** Сначала составим характеристическое уравнение.\n\n$$\\det(A-\\lambda I)=0\\implies \\begin{vmatrix} -1-\\lambda \u0026 -6\\\\ 2 \u0026 6-\\lambda \\end{vmatrix}=0;\n$$\n\n$$\\\\ (-1-\\lambda)(6-\\lambda)-(-6)\\cdot2=-6-6\\lambda+\\lambda+\\lambda^2+12=\\lambda^2-5\\lambda+6=0 \n$$\n\n**Шаг №2.** Решим уравнение. Оно имеет два корня:\n\n$$D=25-4\\cdot6=1;\\\\\\lambda_1=\\frac{5-\\sqrt{1}}{2}=2,\\quad \\lambda_2=\\frac{5+\\sqrt{1}}{2}=3. \n$$\n\n**Шаг 3.** Для каждого собственного значения найдём собственный вектор. Начнём с $\\lambda_1=2$:\n\n$$\\begin{pmatrix} -1-2 \u0026 -6\\\\ 2 \u0026 6-2 \\end{pmatrix}\\cdot \\begin{pmatrix} x\\\\ y \\end{pmatrix}=0 \\implies \\begin{cases} -3x-6y=0\\\\ 2x+4y=0 \\end{cases}\\implies x=-2y;\n$$\n\n$$\\\\ v_1= \\begin{pmatrix} x\\\\ y \\end{pmatrix}=\\begin{pmatrix} x\\\\ -\\frac{x}{2} \\end{pmatrix}\\implies v_1= \\begin{pmatrix} 2\\\\ -1 \\end{pmatrix}. \n$$\n\nБерём любой ненулевой вектор, являющийся решением уравнения. Теперь для второго собственного числа $\\lambda_2=3$:\n\n$$\\begin{pmatrix} -1-3 \u0026 -6\\\\ 2 \u0026 6-3 \\end{pmatrix}\\cdot \\begin{pmatrix} x\\\\ y \\end{pmatrix}=0 \\implies \\begin{cases} -4x-6y=0\\\\ 2x+3y=0 \\end{cases}\\implies 2x=-3y;\n$$\n\n$$\\\\ v_2= \\begin{pmatrix} x\\\\ y \\end{pmatrix}=\\begin{pmatrix} x\\\\ -\\frac{2x}{3} \\end{pmatrix}\\implies v_2= \\begin{pmatrix} 3\\\\ -2 \\end{pmatrix}. \n$$\n\n**Результат.** Матрица $A$ имеет следующие собственные векторы и значения:\n\n$$v_1=\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad v_2=\\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix};\\quad \\lambda_1=2,\\quad\\lambda_2=3.\n$$\n\n$$\\\\ A\\cdot v_1=\\lambda_1\\cdot v_1\\implies\\begin{pmatrix} -1 \u0026 -6\\\\ 2 \u0026 6 \\end{pmatrix}\\cdot \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}=2\\cdot\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix};\n$$\n\n$$\\\\ A\\cdot v_2=\\lambda_2\\cdot v_2\\implies\\begin{pmatrix} -1 \u0026 -6\\\\ 2 \u0026 6 \\end{pmatrix}\\cdot \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}=3\\cdot\\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}. \n$$\n\n{% cut \"Реализация в Python\" %}\n\nВ `Python` найти собственные значения и векторы можно с помощью библиотек `numpy` и `scipy`:\n\n```Python\nA = np.array([[-1, -6], [2, 6]])\n\nimport numpy as np\n\nvals, vecs = np.linalg.eig(A)\nprint(\"Собственные значения:\", vals)\nprint(\"Собственные векторы:\\\\n\", vecs)\n\nfrom scipy.linalg import eig\n\nvals, vecs = eig(A)\nprint(\"Собственные значения:\", vals)\nprint(\"Собственные векторы:\\\\n\", vecs)\n```\n\n{% endcut %}\n\nМатрица $A$ — это линейный оператор, определённым образом растягивающий, сжимающий и поворачивающий пространство вместе с его объектами. Однако, как следует из равенства, собственные векторы при деформации пространства претерпевают увеличение или уменьшение длины иногда с изменением направления на противоположное, но не испытывают поворота в пространстве.\n\n\u003e 💡Геометрически `собственные векторы` — это направления, в которых действие матрицы проявляется как масштабирование.\n\nИ это масштабирование определяется как раз собственным значением.\n\nНа рисунке ниже показано поведение собственных векторов при изменении пространства матрицей в сравнении с поведением векторов изначального ортонормированного базиса. Собственные векторы сохраняют направление (масштабируются), остальные — поворачиваются и искажаются.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_1_6b570f957b.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Собственные векторы $v_1$ и $v_2$\"\n  \u003e\n  \u003cfigcaption\u003e\n\nСобственные векторы $v_1$ и $v_2$, векторы ортонормированного базиса $w_1$ и $w_2$ при действии матрицы $A=\\begin{pmatrix}1.5 \u0026 0.5\\\\ 1 \u0026 1.5\\end{pmatrix}$.Также на графиках показан единичный круг для визуализации действия матрицы\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПричём если собственным векторам $\\{v_1, v_2, ..., v_n\\}$ матрицы $A\\in \\mathbb{R}^{n\\times n}$ соответствуют *разные* собственные числа $\\{\\lambda_1, \\lambda_2,..., \\lambda_n\\}$, то эти собственные векторы точно линейно независимы. А это значит, что такие собственные векторы могут образовывать базис. И такой базис может быть для нас очень удобен, так как действие нашей матрицы на пространство можно было бы свести к масштабированию базисных векторов. Это существенно упрощает анализ и вычисления.\n\nСтоит уточнить, что обратное не обязательно верно: не обязательно все собственные значения должны быть различны, чтобы говорить о линейной независимости $n$ собственных векторов матрицы $A\\in \\mathbb{R}^{n\\times n}$.\n\nИ действительно, если удаётся найти такой базис из собственных векторов, то вся матрица $A$ в этом базисе будет иметь `диагональный вид` — на диагонали окажутся собственные значения, которые масштабируют каждый соответствующий собственный вектор. Это представление называется `диагонализацией матрицы`.\n\n\u003e 💡`Диагонализацией` называется процесс нахождения соответствующей диагональной матрицы для диагонализируемой матрицы или линейного отображения:\n\u003e\n\u003e $$A=PDP^{-1} \n\u003e $$\n\u003e \n\u003e Где $P$ — матрица, в которой столбцы — собственные векторы матрицы $A$; $D$ — диагональная матрица собственных значений.\n\u003e\n\u003e $$P=\\begin{pmatrix} | \u0026 | \u0026 \\cdots \u0026 |\\\\ v_1 \u0026 v_2 \u0026 \\cdots \u0026 v_n\\\\ | \u0026 | \u0026 \\cdots \u0026 | \\end{pmatrix},\\quad D=\\begin{pmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 0 \u0026 \\cdots \u0026 \\lambda_n \\end{pmatrix} \n\u003e $$\n\u003e \n\u003e Квадратная матрица, которую нельзя диагонализировать, называется `дефектной`.\n\nПочему это верно? Произведение матриц $P$ и $D$ по определению собственных векторов равно следующему:\n\n$$PD=\\begin{pmatrix} | \u0026 | \u0026 \\cdots \u0026 |\\\\ v_1 \u0026 v_2 \u0026 \\cdots \u0026 v_n\\\\ | \u0026 | \u0026 \\cdots \u0026 | \\end{pmatrix}\\cdot \\begin{pmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 0 \u0026 \\cdots \u0026 \\lambda_n \\end{pmatrix}= \n$$\n\n$$\\begin{pmatrix} | \u0026 | \u0026 \\cdots \u0026 |\\\\ \\lambda_1\\cdot v_1 \u0026 \\lambda_2\\cdot v_2 \u0026 \\cdots \u0026 \\lambda_n\\cdot v_n\\\\ | \u0026 | \u0026 \\cdots \u0026 | \\end{pmatrix}= \\begin{pmatrix} | \u0026 | \u0026 \\cdots \u0026 |\\\\ A\\cdot v_1 \u0026 A\\cdot v_2 \u0026 \\cdots \u0026 A\\cdot v_n\\\\ | \u0026 | \u0026 \\cdots \u0026 | \\end{pmatrix}=A\\cdot \n$$\n\n$$ \\begin{pmatrix} | \u0026 | \u0026 \\cdots \u0026 |\\\\ v_1 \u0026 v_2 \u0026 \\cdots \u0026 v_n\\\\ | \u0026 | \u0026 \\cdots \u0026 | \\end{pmatrix}=AP\n$$\n\nЧтобы получить из произведения $AP$ снова матрицу $A$, нужно лишь домножить его на обратную к $P$ матрицу:\n\n$$A=APP^{-1}\\implies PDP^{-1} \n$$\n\nНапример, для нашей матрицы $A$ выполняется условие линейной независимости собственных векторов, так как у них два разных собственных значения. А значит, матрицу $A$ можно диагонализировать (в данном случае совпало, что $P=P^{-1}$, хотя обычно эти матрицы не равны):\n\n$$\\begin{pmatrix} -1 \u0026 -6\\\\ 2 \u0026 6 \\end{pmatrix}= \\begin{pmatrix} 2 \u0026 3\\\\ -1 \u0026 -2 \\end{pmatrix}\\cdot \\begin{pmatrix} 2 \u0026 0\\\\ 0 \u0026 3 \\end{pmatrix}\\cdot \\begin{pmatrix} 2 \u0026 3\\\\ -1 \u0026 -2 \\end{pmatrix}^{-1}= \n$$\n\n$$\\begin{pmatrix} 2 \u0026 3\\\\ -1 \u0026 -2 \\end{pmatrix}\\cdot \\begin{pmatrix} 2 \u0026 0\\\\ 0 \u0026 3 \\end{pmatrix}\\cdot \\begin{pmatrix} 2 \u0026 3\\\\ -1 \u0026 -2 \\end{pmatrix} \n$$\n\n{% cut \"Диагонализация реализована в библиотеках `numpy` и `scipy`:\" %}\n\n```Python\nimport numpy as np\n\nvals, vecs = np.linalg.eig(A) # Собственные векторы\nP = vecs\nD = np.diag(vals)\nA_rec = P @ D @ np.linalg.inv(P)\nnp.allclose(A, A_rec)  # True, если диагонализируемая\n\nfrom scipy.linalg import eig, inv\n\nvals, vecs = eig(A) # Собственные векторы\nP = vecs \nD = np.diag(vals)\nA_rec = P @ D @ inv(P)\n```\n\n{% endcut %}\n\nОднако не всякая матрица допускает диагонализацию (даже над $\\mathbb{C}$). Чтобы матрица была диагонализуема, необходимо, чтобы у неё нашёлся полный базис из линейно независимых собственных векторов. В противном случае построить разложение вида $A=PDP^{-1}$ невозможно.\n\nНо есть важный и очень приятный класс матриц — это `симметричные матрицы`.\n\n\u003e 💡`Симметричная матрица` — это квадратная матрица, которая остается неизменной после транспонирования:\n\u003e\n\u003e $$A=A^T \n\u003e $$\n\u003e \n\u003e \n\nЕсли матрица $A \\in \\mathbb{R}^{n \\times n}$ симметрична и допускает полный набор линейно независимых собственных векторов, то её всегда можно диагонализовать.\n\nТеперь, когда мы вооружились знанием о собственных значениях и векторах матрицы, введём новое понятие — `спектральный радиус`:\n\n\u003e 💡`Спектральный радиус` матрицы $A$ — это максимальное по модулю собственное значение этой матрицы:\n\u003e\n\u003e $$\\rho(A)=\\max_i|\\lambda_i| \n\u003e $$\n\u003e \n\u003e \n\nСпектральный радиус показывает, насколько сильно матрица может растянуть вектор, не меняя его направления. То есть максимальный коэффициент растяжения вдоль собственного направления вектора.\n\nГеометрически это означает, что если мы применим матрицу $A$ к единичной сфере, то её образом будет эллипс. В случае симметричной матрицы $A$ собственные векторы образуют ортогональный базис, и тогда они совпадают с направлениями главных полуосей эллипса. Соответствующие собственные значения $|\\lambda_i|$ дают длины этих полуосей, а направление наибольшего растяжения — вдоль $v_1$ — соответствует спектральному радиусу $\\rho(A)=|\\lambda_1|$.\n\nЕсли же $A$ диагонализируема, но её собственные векторы не ортогональны, то, хотя векторы $v_i$ и масштабируются в $\\lambda_i v_i$, направления полуосей эллипса не совпадают с $v_i$. Это хорошо видно на рисунке ниже: чем меньше угол между $v_i$, тем сильнее различие между их образами и действительными осями эллипса. При этом каждый нормированный собственный вектор всё равно растягивается в своём направлении на величину, равную $|\\lambda_i|$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_2_fc9157e9ba.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Нормированные собственные векторы $v_1$ и $v_2$\"\n  \u003e\n  \u003cfigcaption\u003e\n\nНормированные собственные векторы $v_1$ и $v_2$ до и после действия матрицы $A=\\begin{pmatrix}1.5 \u0026 0.5\\\\ 1 \u0026 1.5\\end{pmatrix}$. Так как векторы $v_1$ и $v_2$ нормированы, они лежат на единичной окружности\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ машинном обучении спектральный радиус особенно важен для анализа двух ключевых процессов.\n\n**Анализ сходимости градиентного спуска.** В задачах оптимизации квадратичных функций (или локально квадратичных приближений) спектральный радиус матрицы Гессиана $H$ ограничивает максимально допустимый шаг градиентного спуска для сходимости.\n\nТо есть спектральный радиус Гессиана функции потерь $H$ [влияет](https://fmin.xyz/docs/methods/fom/GD.html) на скорость сходимости и выбор шага обучения $\\eta$:\n\n$$\\eta\u003c\\frac{2}{\\rho(H)} \n$$\n\n**Анализ стабильности рекуррентных нейронных сетей (RNN).** Чтобы избежать взрыва или затухания градиентов при обучении, [нужно](https://www.jefkine.com/general/2018/05/21/2018-05-21-vanishing-and-exploding-gradient-problems/), чтобы спектральный радиус у скрытого состояния (*hidden state*) $W$ был равен или был меньше единицы:\n\n$$\\rho(W)\\leq1 \n$$\n\nЕсли $\\rho(W)\\ll1$ происходит затухание градиентов. Если же $\\rho(W)\\gg1$, то наоборот, взрыв градиентов.\n\nМы увидели, что собственные значения и векторы позволяют упростить анализ линейного оператора, сведя его действие к масштабированию вдоль определённых направлений.\n\nДо сих пор мы рассматривали разложение, связанное с собственными значениями квадратных матриц (`диагонализация`). Однако на практике далеко не каждая матрица является квадратной или легко диагонализируемой. А в машинном обучении мы почти всегда работаем с прямоугольными матрицами данных — например, когда есть $m$ объектов и $n$ признаков и $m \\ne n$.\n\nВ таких случаях особенно важны универсальные матричные разложения, применимые к любым матрицам и сохраняющие важную информацию об их структуре. Рассмотрим их подробнее.\n\n## Матричные разложения\n\nСреди матричных разложений особое место занимают:\n\n1. Полное сингулярное разложение (англ. SVD, Singular Value Decomposition) — мощный инструмент для анализа структуры данных, сжатия информации и снижения размерности.\n2. QR-разложение — фундаментальный алгоритм в численных методах, часто используемый для устойчивого решения систем линейных уравнений и при построении ортонормированных базисов. Мы уже реализовали его [ранее](https://education.yandex.ru/handbook/math/article/proektsii-ugli-i-ortogonalnost#qr-razlozhenie).\n3. Разложение Холецкого (Cholesky) — специализированное, но эффективное разложение, применимое к симметричным положительно определённым матрицам, например, в регрессии.\n\nЭти разложения не просто способы представить матрицу в виде произведения других матриц. Они позволяют устойчиво и эффективно решать фундаментальные задачи: от решения уравнений и оценки ранга до приближённого восстановления данных и анализа обусловленности. Мы рассмотрим, как работают сингулярное разложение и разложение Холецкого, а также где они используются. Плюс коснёмся темы низкоранговых приближений.\n\n### Полное сингулярное разложение (SVD)\n\nСингулярное разложение мы рассмотрим подробно, так как оно является фундаментальным и универсальным. Для начала введём определение и будем отталкиваться от него.\n\nДля любой матрицы $A \\in \\mathbb{R}^{m \\times n}$ существует разложение вида:\n\n$$A = U \\Sigma V^T\\\\\n$$\n\n$$ A= \\begin{pmatrix} | \u0026 | \u0026 | \\\\ u_1 \u0026 \\cdots \u0026 u_m \\\\ | \u0026 | \u0026 | \\end{pmatrix} \\cdot \\begin{pmatrix} \\sigma_1 \u0026 0 \u0026 \\cdots \\\\ 0 \u0026 \\sigma_2 \u0026 \\cdots \\\\ \\cdots \u0026 \\cdots \u0026 \\cdots \\end{pmatrix} \\cdot \\begin{pmatrix} | \u0026 | \u0026 | \\\\ v_1 \u0026 \\cdots \u0026 v_n \\\\ | \u0026 | \u0026 | \\end{pmatrix}^T\n$$\n\nГде:\n\n- $U \\in \\mathbb{R}^{m \\times m}$ — ортогональная матрица, столбцы которой — левые сингулярные векторы матрицы $A$;\n- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ — прямоугольно-диагональная матрица с неотрицательными сингулярными числами матрицы $A$ по диагонали, расположенными в порядке убывания;\n- $V \\in \\mathbb{R}^{n \\times n}$ — ортогональная матрица, столбцы которой — правые сингулярные векторы матрицы $A$.\n\nТеперь попробуем разобраться в том, что это определение нам говорит. Мы воздействуем на пространство сначала матрицей $V^T$, потом $\\Sigma$, а потом матрицей $U$. Результат этого воздействия — это то же самое, что воздействие матрицы $A$ на исходное пространство.\n\nРазберём более подробно, как именно на пространство и отдельные векторы влияет действие каждой из матриц $U$, $\\Sigma$ и $V^T$, входящих в сингулярное разложение матрицы.\n\n**Матрица** $V$\n\nПо определению мы знаем, что матрица $V$ задаёт новый ортонормированный базис из векторов $v_1, …, v_n$, так как она ортогональна. Это значит, что $V^T$ переведёт любой вектор $x$ в этот новый базис, где векторы $v_1, …, v_n$ будут новыми осями координат.\n\nТо есть, это некоторое вращение исходного пространства вокруг начала координат или же отражение этого пространства относительно какой-то оси. Размерность пространства не меняется и остаётся $\\mathbb{R}^n$. То есть правые сингулярные векторы — это входной ортонормированный базис, в котором матрица действует как простое масштабирование ($\\Sigma$).\n\n**Матрица** $\\Sigma$\n\nПонятно, что диагональная матрица уже после преобразований матрицей $V^T$ будет производить некоторое масштабирование нового базиса. Но в контексте этой матрицы нам надо коснуться двух вещей: что такое сингулярные числа и что будет, если наша матрица $\\Sigma$ не квадратная.\n\nНеквадратные диагональные матрицы бывают высокими (число строк больше числа столбцов) и широкими (число столбцов больше числа строк). Допустим, у нас трёхмерное пространство с некоторым базисом $\\{x, y, z\\}$. Сравним, как повлияют на пространство высокая и широкая диагональные матрицы:\n\n$$\\text{Широкая } \\Sigma:\\quad\\begin{pmatrix} 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 3 \u0026 0 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}= \\begin{pmatrix} x \\\\ 3y \\end{pmatrix};\\\\\n$$\n\n$$\\text{Высокая }\\Sigma:\\quad\\begin{pmatrix} 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 3 \u0026 0 \\\\ 0 \u0026 0 \u0026 2 \\\\ 0 \u0026 0 \u0026 0 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}= \\begin{pmatrix} x \\\\ 3y \\\\ 2z \\\\ 0 \\end{pmatrix}. \n$$\n\nШирокая матрица схлопывает пространственные измерения (в нашем случае трёхмерное в двумерное пространство), масштабируя базис. А высокая — наоборот, перемещает пространство низких размерностей в пространство с большим количеством измерений (из трёхмерного в четырёхмерное), также одновременно масштабируя базис.\n\nДиагональные элементы матрицы $\\Sigma$ называются сингулярными числами матрицы $A$. Их обычно обозначают как $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r \\geq 0$, где $r = \\operatorname{rank}(A)$. Сами числа — это корни из собственных значений матрицы $A^TA$.\n\n$$\\sigma_i = \\sqrt{\\lambda_i}, \\text{ где }\\lambda_i \\text{ — собственное число матрицы } A^TA \n$$\n\nЭто не отрицательные числа, и они всегда упорядочены по убыванию. Геометрически каждое $\\sigma_i$ показывает, во сколько раз матрица $A$ растягивает пространство вдоль соответствующего направления (заданного правым сингулярным вектором из $V$).\n\nТаким образом, матрица $\\Sigma$ масштабирует новый базис и меняет размерность пространства, если является неквадратной: если матрица $\\Sigma$ высокая, пространство переходит в пространство более высоких размерностей, если широкая, то наоборот, в пространство низких размерностей. Пространство размерности $\\mathbb{R}^n$ переходит в пространство размерности $\\mathbb{R}^m$.\n\nМатрица $U$\n\nДействует по аналогии с матрицей $V$ — это тоже поворот (отражение) вокруг центра координат. Размерность пространства не меняется и остаётся $\\mathbb{R}^m$.\n\n***\n\nНиже представлена коммутативная диаграмма SVD, отражающая воздействие разных матриц на пространство.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_3_7c6afff5f1.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Коммутативная диаграмма SVD\"\n  \u003e\n  \u003cfigcaption\u003e\n\nКоммутативная диаграмма SVD\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nА так выглядит рассмотренная нами геометрия на примере квадратной матрицы.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_4_4ffe9e6c62.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация SVD\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация SVD для $A = \\begin{pmatrix}2 \u0026 1 \\\\ 1 \u0026 1.5\\end{pmatrix}$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПоэтапно разберём, что происходит при действии SVD на векторы:\n\n- У нас изначально имеется окружность в двумерном пространстве. На рисунке показаны правые сингулярные векторы матрицы $A$: $v_1$ и $v_2$. Они задают направления, в которых матрица $A$ действует наиболее сильно и слабо.\n- Мы применяем матрицу $V^T$. Происходит переход к координатной системе, в которой $v_1$ и $v_1$$v_2$ — базисные векторы. Это такой базис, в котором действие матрицы $A$ сводится к простому масштабированию по направлениям векторов базиса с помощью диагональной матрицы $\\Sigma$.\n- Применяем матрицу $\\Sigma$, которая масштабирует векторы базиса $e_1$ и $e_2$.\n- Матрица $U$ реализует ортогональное преобразование (поворот и/или отражение), приводя масштабированный эллипс обратно в исходное пространство. Оси эллипса теперь сонаправлены с левыми сингулярными векторами $u_1$ и $u_2$.\n\nВ случае если матрица $A$ высокая (то есть имеет больше строк, чем столбцов), на шаге 2 преобразование с помощью матрицы $\\Sigma$ отображает двумерный объект (единичный круг в $\\mathbb{R}^2$) в более высокое пространство ($\\mathbb{R}^m$, где $m\u003e2$). Однако поскольку $\\Sigma$ имеет только два ненулевых сингулярных значения, результат — это эллипс, лежащий в двумерной плоскости, вложенной в $\\mathbb{R}^m$. Иными словами, размерность фигуры не увеличивается, но она теперь представлена в более высоком пространстве, где третье (и последующие) измерения просто не участвует.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_5_68bb69124d.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация SVD для высокой прямоугольной матрицы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация SVD для высокой прямоугольной матрицы $A = \\begin{pmatrix}2 \u0026 1 \\\\ 1 \u0026 1.5\\\\1 \u0026 -1\\end{pmatrix}$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ случае широких прямоугольных матриц на шаге 2 один или несколько (в зависимости от разницы между количеством строк и столбцов матрицы $A$, на рисунке ниже — один) правых сингулярных векторов уменьшаются до нуля, из-за чего происходит уменьшение размерности пространства.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_6_9f212bb8fc.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация SVD для широкой прямоугольной матрицы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация SVD для широкой прямоугольной матрицы $A = \\begin{pmatrix}2 \u0026 1 \u0026 1\\\\ 1 \u0026 1.5 \u0026 -1\\end{pmatrix}$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**Пример**\n\nРассмотрим пример расчёта SVD-разложения для матрицы $A$ и заодно рассмотрим, как математически находить левые и правые сингулярные векторы.\n\n$$ A= \\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}, \\quad A \\in \\mathbb{R}^{3 \\times 2}.\n$$\n\n**Шаг №1. Сингулярные числа.** Для начала находим матрицу $A^TA$:\n\n$$A^TA= \\begin{pmatrix} 1 \u0026 0 \u0026 1 \\\\ 0 \u0026 1 \u0026 1 \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}= \\begin{pmatrix} 2 \u0026 1 \\\\ 1 \u0026 2 \\end{pmatrix} \n$$\n\nЧтобы найти её собственные значения, выпишем характеристическое уравнение и его корни:\n\n$$\\det\\left(A^TA–\\lambda I\\right)=\\det \\begin{pmatrix} 2-\\lambda \u0026 1 \\\\ 1 \u0026 2-\\lambda \\end{pmatrix}=(2-\\lambda)^2-1=\\lambda^2-4\\lambda+3;\n$$\n\n$$\\lambda_1=3,\\quad\\lambda_2=1. \n$$\n\nТеперь мы можем рассчитать сингулярные числа:\n\n$$\\sigma_1=\\sqrt{\\lambda_1}=\\sqrt{3}, \\quad \\sigma_2=\\sqrt{\\lambda_2}=\\sqrt{1}=1. \n$$\n\n**Шаг №2. Матрица** $V^T$. Находим правые сингулярные векторы — это собственные векторы матрицы $A^TA$. Сначала для собственного числа $\\lambda_1=3$:\n\n$$\\begin{pmatrix} 2-3 \u0026 1 \\\\ 1 \u0026 2-3 \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix}= \\begin{pmatrix} -1 \u0026 1 \\\\ 1 \u0026 -1 \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix}=0\\implies x=y;\n$$\n\n$$\\\\ v_1=\\begin{pmatrix} x \\\\ x \\end{pmatrix}\\implies v_1=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}. \n$$\n\nНе забываем нормировать вектор:\n\n$$v_1=\\frac{1}{\\sqrt{1^2+1^2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}= \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \n$$\n\nТеперь то же самое для второго собственного числа матрицы $A^TA$ - $\\lambda_2=1$:\n\n$$\\begin{pmatrix} 2-1 \u0026 1 \\\\ 1 \u0026 2-1 \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix}= \\begin{pmatrix} 1 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix}=0\\implies x=-y;\n$$\n\n$$\\\\ v_2=\\begin{pmatrix} x \\\\ -x \\end{pmatrix}\\implies v_2=\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix};\\\\ v_2=\\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}= \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. \n$$\n\nИ вот мы получили матрицу $V$:\n\n$$ V=\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\\implies V^T=\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\n\n**Шаг №3. Матрица** $U$. Находим сначала левые сингулярные векторы. Формула для поиска векторов:\n\n$$u_i=\\frac{1}{\\sigma_i}Av_i \n$$\n\nДля $v_1$ и $\\sigma_1 = \\sqrt{3}$:\n\n$$u_1=\\frac{1}{\\sigma_1}Av_1=\\frac{1}{\\sqrt{3}}\\cdot\\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}\\cdot \\frac{1}{\\sqrt{2}}\\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}= \\frac{1}{\\sqrt{6}}\\cdot\\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \n$$\n\nАналогично для $v_2$ и $\\sigma_2 = 1$:\n\n$$u_2=\\frac{1}{\\sigma_2}Av_2=\\frac{1}{1}\\cdot\\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}\\cdot \\frac{1}{\\sqrt{2}}\\cdot \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}= \\frac{1}{\\sqrt{2}}\\cdot\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \n$$\n\nТретий вектор $u_3$ должен быть ортогонален первым двум (не забываем нормировать):\n\n$$u_3=\\frac{1}{\\sqrt{1^2+1^2+(-1)^2}}\\cdot\\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}= \\frac{1}{\\sqrt{3}}\\cdot\\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \n$$\n\nСобираем матрицу $U$:\n\n$$U=\\begin{pmatrix} \\frac{1}{\\sqrt{6}} \u0026 \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{6}} \u0026 -\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{6}} \u0026 0 \u0026 -\\frac{1}{\\sqrt{3}} \\\\ \\end{pmatrix} \n$$\n\n**Результат**\n\n$$U=\\begin{pmatrix} \\frac{1}{\\sqrt{6}} \u0026 \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{6}} \u0026 -\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{6}} \u0026 0 \u0026 -\\frac{1}{\\sqrt{3}} \\\\ \\end{pmatrix}; \\quad \\Sigma=\\begin{pmatrix} \\sqrt{3} \u0026 0 \\\\ 0 \u0026 1 \\\\ 0 \u0026 0 \\end{pmatrix}; \\quad V^T=\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{pmatrix}. \n$$\n\nСингулярное разложение играет фундаментальную роль во многих алгоритмах машинного обучения.\n\nОно лежит в основе анализа главных компонент (PCA), который используется для снижения размерности данных при сохранении наиболее значимой информации. В латентно-семантическом анализе (англ. LSA, Latent semantic analysis) SVD помогает извлекать скрытые смыслы из текстов, выявляя взаимосвязи между словами и документами.\n\nВ рекомендательных системах SVD применяется для матричной факторизации: выявления скрытых предпочтений пользователей и характеристик объектов. Кроме того, SVD используется в сжатии изображений и шумоподавлении, где упрощённое представление данных сохраняет их основную структуру. Благодаря устойчивости и способности выявлять главное SVD стал универсальным инструментом анализа высокоразмерных данных в машинном обучении.\n\n{% cut \"Сингулярное разложение в библиотеках `numpy`, `scipy` и `sklearn`\" %}\n\n```Python\nimport numpy as np\n\nA = np.array([[1, 0], [0, 1], [1, 1]])\nU, S, VT = np.linalg.svd(A, full_matrices=True)\n\nfrom scipy.linalg import svd\n\nA = np.array([[1, 0], [0, 1], [1, 1]])\nU, S, VT = svd(A, full_matrices=True)\n\nfrom sklearn.decomposition import PCA\n\nA = np.array([[1, 0], [0, 1], [1, 1]])\npca = PCA(n_components=2)\npca.fit(A)\ncomponents = pca.components_\nexplained_var = pca.explained_variance_\n```\n\n{% endcut %}\n\nИтак, сингулярное разложение (SVD) позволяет разложить любую матрицу на последовательность простых линейных преобразований: поворот (или отражение), масштабирование вдоль новых координатных осей и обратный поворот. Благодаря этому SVD даёт мощную геометрическую интерпретацию действия матрицы на пространство: каждое сингулярное число определяет, насколько сильно матрица растягивает пространство вдоль соответствующего направления.\n\nОднако, помимо чисто геометрического понимания, это разложение открывает перед нами важную прикладную возможность — сократить сложность матрицы, сохранив при этом основную структуру и информацию. В реальных задачах часто оказывается, что только первые несколько направлений (связанных с наибольшими сингулярными числами) несут большую часть полезной информации, а остальные содержат шум или несущественные детали.\n\nВозникает естественный и крайне полезный вопрос: можно ли отбросить эти «малозначимые» компоненты, упростив матрицу, но не потеряв главное? Именно к этой идее нас подводит низкоранговое приближение, которое позволяет представить сложную матрицу в более компактном и интерпретируемом виде — за счёт усечения её сингулярного разложения.\n\n### Низкоранговые приближения\n\nЧтобы понять, что стоит за этим термином, напомним, что ранг матрицы — это максимальное число линейно независимых строк (или столбцов) или, эквивалентно, число ненулевых сингулярных чисел в её SVD. Геометрически ранг определяет размерность подпространства, в котором находятся все строки или столбцы матрицы. Чем ниже ранг, тем проще геометрия матрицы и тем меньше информации она содержит.\n\nПусть матрица $A \\in \\mathbb{R}^{m \\times n}$ описывает некоторый набор данных: строки соответствуют объектам (например, людям, товарам, наблюдениям), а столбцы — признакам или измерениям. Если применить к ней полное сингулярное разложение, мы получим представление в виде $A = U \\Sigma V^T$, где $U$ и $V$ — ортогональные матрицы, описывающие направления действия, а $\\Sigma$ — диагональная матрица с сингулярными числами, упорядоченными по убыванию.\n\nИменно эти числа определяют «вес» или значимость каждого из направлений преобразования: чем больше значение $\\sigma_i$, тем более важным является соответствующее преобразование вдоль направлений $u_i$ и $v_i$.\n\nИнтуитивно можно сказать, что каждое сингулярное число измеряет, насколько сильно матрица $A$ растягивает пространство вдоль некоторого направления. И если среди всех сингулярных чисел доминируют только первые несколько, а остальные становятся малы, то возникает естественная идея — сохранить только самые важные компоненты, отбросив незначимые.\n\nЭто позволяет не просто упростить матрицу, но и избавиться от возможных шумов и случайных флуктуаций, которые могли исказить структуру данных. Такое приближение реализуется формулой усечённого SVD, в которой мы оставляем только первые $k$ членов разложения:\n\n\u003e 💡Матрицу $A$ можно аппроксимировать матрицей меньшего ранга:\n\u003e\n\u003e $$A\\approx A_k=U_k \\Sigma_k V_k^T \n\u003e $$\n\u003e \n\u003e Где $U_k$, $\\Sigma_k$ и $V_k$ — это усечённые версии соответствующих матриц полного сингулярного разложения, содержащие только первые $k$ столбцов (или диагональных элементов) и отражающие наиболее значимые компоненты исходной матрицы.\n\nПри этом новая матрица $A_k$ имеет ранг не выше $k$, но остаётся максимально близкой к $A$ по норме Фробениуса, то есть она минимизирует сумму квадратов отклонений между соответствующими элементами оригинальной и приближённой матрицы:\n\n$$\\|A-A_k\\|^2_F=\\sum^r_{i=k+1}\\sigma^2_i, \\quad r=\\mathrm{rank}(A) \n$$\n\nГеометрически это означает, что мы проецируем все строки матрицы $A$ на подпространство размерности $k$, в котором сосредоточена основная изменчивость данных.\nНиже на рисунке представлена наглядная иллюстрация работы низкорангового приближения матрицы\n\n$$A=\\begin{pmatrix}1.5 \u0026 0.5\\\\ 1 \u0026 1.5\\end{pmatrix} \n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_7_4314bf2a46.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация низкорангового приближения матрицы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация низкорангового приближения матрицы $A=\\begin{pmatrix}1.5 \u0026 0.5\\\\ 1 \u0026 1.5\\end{pmatrix}$ через собственные векторы\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nСлева: действие матрицы $A$ на единичный круг растягивает пространство вдоль нормированных собственных векторов $v_1$ и $v_2$, превращая круг в эллипс.\n\nСправа: приближение матрицы $A$ матрицей ранга 1 сохраняет только главное направление $v_1$, по которому растяжение наиболее сильное. В результате вся окружность схлопывается в отрезок длины $2\\lambda_1$ вдоль $v_1$ — это геометрическая интерпретация приближённой матрицы ранга 1: $A_1 = \\sigma_1 u_1 v_1^T$.\n\n**Пример**\n\nДля иллюстрации механизма низкорангового приближения рассмотрим простой числовой пример. Пусть матрица $A$ задаёт оценки трёх студентов по двум дисциплинам. Строки матрицы соответствуют студентам, а столбцы — предметам.\n\nВыглядит она следующим образом:\n\n$$A= \\begin{pmatrix} 82 \u0026 85 \\\\ 78 \u0026 83 \\\\ 65 \u0026 70 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2} \n$$\n\nПрименяя к ней полное сингулярное разложение $A=U\\Sigma V^T$, можно увидеть, что первое сингулярное число значительно больше второго:\n\n$$\\sigma_1=189.855,\\quad\\sigma_2=1.503\\implies \\sigma_1\\gg\\sigma_2 \n$$\n\nЭто означает, что почти вся изменчивость данных сосредоточена вдоль одного главного направления.\n\nЕсли мы оставим только первую компоненту в разложении, получим матрицу ранга 1:\n\n$$A_1=\\sigma_1 u_1 V_1^T \n$$\n\nТакая матрица выражает исходные данные через единственный фактор. В этом приближении все строки $A_1$ лежат на одной прямой в пространстве $\\mathbb{R}^2$, а различия между студентами описываются только масштабами этого фактора.\n\nНесмотря на такую грубую аппроксимацию, элементы $A_1$ довольно точно приближают значения $A$, хотя используют вдвое меньше информации. Такой приём применяется, например, для сжатия изображений: изображение представляется в виде матрицы яркости или RGB-канала, и при сохранении лишь первых $k$-сингулярных компонент можно восстановить приближённую версию изображения с минимальной потерей качества.\n\nНа рисунке ниже представлен пример сжатия изображения с помощью низкорангового приближения. Слева вверху — оригинальное изображение, далее — его приближения с рангами 1, 10 и 100. При низком ранге сохраняется только общая структура, при увеличении ранга восстанавливаются детали. Чем выше ранг, тем точнее приближение, но тем выше и объём данных.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_9_8_9eba0ee721.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Влияние ранга сингулярного разложения на качество восстановления изображения\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВлияние ранга сингулярного разложения на качество восстановления изображения\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТаким образом, низкоранговое приближение позволяет:\n\n- Удалять шум (мелкие сингулярные значения соответствуют направлениям слабо выраженной изменчивости).\n- Уменьшать объём хранимых или передаваемых данных.\n- Ускорять последующие вычисления.\n- Извлекать главное: основные закономерности и скрытые факторы в данных.\n\nТакой приём лежит в основе многих методов машинного обучения: от PCA и латентно-семантического анализа до рекомендательных систем. Во всех этих задачах сингулярное разложение позволяет извлекать ключевые скрытые закономерности, устраняя шум и избыточность. Именно поэтому SVD и его приближённые формы играют важнейшую роль в линейной алгебре применительно к анализу данных.\n\nSVD — универсальный и мощный инструмент, применимый к любым матрицам. Однако в реальных задачах, особенно в оптимизации и статистике, часто встречаются матрицы с особыми свойствами: они симметричны и положительно определены. Это открывает путь к более лёгким и вычислительно эффективным методам работы с такими матрицами.\n\nОдним из таких методов является разложение Холецкого, которое позволяет заменить обращение матриц или громоздкие разложения на более быстрые и устойчивые вычисления, используя треугольную структуру. Давайте рассмотрим, как работает разложение Холецкого и где его применение даёт наибольший выигрыш.\n\n### Разложение Холецкого (Cholesky)\n\nРазложение Холецкого применимо только к симметричным положительно определённым матрицам — таким, которые можно считать матричными аналогами положительных чисел.\n\n\u003e 💡Симметричная матрица $A$ называется положительно определённой, если для любого ненулевого вектора $x$ выполняется условие $x^\\top A x\u003e0$. Это эквивалентно тому, что все собственные значения матрицы $A$ строго положительны.\n\nЭто ограничение на первый взгляд может показаться серьёзным, но на практике такие матрицы встречаются довольно часто. Например, матрицы ковариаций, матрицы Гессиана в задачах оптимизации, а также матрицы $X^TX$, возникающие в нормальных уравнениях регрессии, всегда симметричны и часто положительно определены. Для таких матриц $A \\in \\mathbb{R}^{n \\times n}$ существует уникальное представление вида:\n\n$$ A = LL^T\n$$\n\nГде $L \\in \\mathbb{R}^{n \\times n}$ — нижнетреугольная матрица с положительными диагональными элементами.\n\nРассмотрим это разложение на примере. Пусть дана матрица A:\n\n$$A= \\begin{pmatrix} 4 \u0026 2 \\\\ 2 \u0026 3 \\end{pmatrix} \n$$\n\nНайдём нижнетреугольную матрицу $L$ — такую, что $A = LL^T$.\n\nПусть:\n\n$$LL^T= \\begin{pmatrix} l_{11} \u0026 0 \\\\ l_{21} \u0026 l_{22} \\end{pmatrix}\\cdot \\begin{pmatrix} l_{11} \u0026 l_{21} \\\\ 0 \u0026 l_{22} \\end{pmatrix}= \\begin{pmatrix} l_{11}^2 \u0026 l_{11}l_{21} \\\\ l_{11}l_{21} \u0026 l_{21}^2+l^2_{22} \\end{pmatrix} \n$$\n\nПриравниваем к $A$:\n\n$$\\begin{cases} l_{11}^2 = 4\\\\ l_{11} l_{21} = 2\\\\ l_{21}^2+l^2_{22} = 3 \\end{cases}\\implies \\begin{cases} l_{11} = 2\\\\ l_{21} = 1\\\\ l_{22} = \\sqrt{2} \\end{cases} \n$$\n\nРезультат:\n\n$$L=\\begin{pmatrix} l_{11} \u0026 0 \\\\ l_{21} \u0026 l_{22} \\end{pmatrix}= \\begin{pmatrix} 2 \u0026 0 \\\\ 1 \u0026 \\sqrt{2} \\end{pmatrix},\\quad L^T=\\begin{pmatrix} l_{11} \u0026 l_{21} \\\\ 0 \u0026 l_{22} \\end{pmatrix}= \\begin{pmatrix} 2 \u0026 1 \\\\ 0 \u0026 \\sqrt{2} \\end{pmatrix}. \n$$\n\nЭтот результат мы можем проверить:\n\n$$LL^T=\\begin{pmatrix} 2 \u0026 0 \\\\ 1 \u0026 \\sqrt{2} \\end{pmatrix}\\cdot\\begin{pmatrix} 2 \u0026 1 \\\\ 0 \u0026 \\sqrt{2} \\end{pmatrix}= \\begin{pmatrix} 4 \u0026 2 \\\\ 2 \u0026 3 \\end{pmatrix}=A \n$$\n\nРазложение Холецкого особенно эффективно в задачах, где требуется решение симметричных линейных систем — например, в байесовских методах, гауссовских процессах, регуляризованной линейной регрессии (Ridge-регрессии), или при максимизации логарифма правдоподобия. Вычислительная сложность составляет всего $\\mathcal{O}(n^3/3)$, и благодаря треугольной структуре все вычисления сводятся к подстановке без необходимости инверсии матриц.\n\n{% cut \"**Разложение Холецкого в Python**\" %}\n\n```Python\nimport numpy as np\n\nA = np.array([[4, 2], [2, 3]])\nL = np.linalg.cholesky(A)\n\nfrom scipy.linalg import cholesky\n\nA = np.array([[4, 2], [2, 3]])\nL = cholesky(A, lower=True)\n```\n\n{% endcut %}\n\n### Решение линейной регрессии через разложение Холецкого\n\nВ линейной регрессии с матрицей признаков $X \\in \\mathbb{R}^{n \\times d}$ и целевой переменной $y \\in \\mathbb{R}^n$ решение для коэффициентов $\\beta$ обычно выражается через нормальные уравнения:\n\n$$X^T X\\beta=X^T y \n$$\n\nОно получается из квадратичной функции потерь:\n\n$$\\mathcal{L}(\\beta)=\\|y-X\\beta\\| _2^2=(y-X\\beta)^T(y-X\\beta) \n$$\n\nФункция потерь достигает минимума при нулевом градиенте по $\\beta$ — раскрываем скобки по формуле квадрата нормы.\n\n$$\\nabla_{\\beta} \\mathcal{L}(\\beta)=(y - X\\beta)^T(y - X\\beta) = y^T y - y^T X\\beta - (X\\beta)^T y + (X\\beta)^T X\\beta \n$$\n\nПоскольку $(X\\beta)^T y = (y^T X\\beta)^T = y^T X\\beta$ (оба выражения — скаляры), смешанные члены объединяются:\n\n$$y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \n$$\n\nТаким образом, функция потерь имеет следующий вид:\n\n$$\\mathcal{L}(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta \n$$\n\nМинимизируем $\\mathcal{L}(\\beta)$ по $\\beta$. Поскольку $y^T y$ не зависит от $\\beta$, оставляем только те слагаемые, в которых есть $\\beta$. Производная от остальных слагаемых будет нулевой.\n\n$$\\nabla_\\beta \\mathcal{L}(\\beta) = -2 X^T y + 2 X^T X \\beta \n$$\n\nПриравниваем градиент к нулю — в этой точке квадратичная функция потерь достигает своего минимума:\n\n$$-2 X^T y + 2 X^T X \\beta = 0. \n$$\n\nДелим обе части надвое и получаем нормальные уравнения:\n\n$$X^T X \\beta = X^T y. \n$$\n\nЭто и есть классическое аналитическое решение задачи линейной регрессии через минимум среднеквадратичной ошибки.\n\nМатрица $X^T X$ всегда симметрична, и если её столбцы линейно независимы, она положительно определена — то есть как раз пригодна для разложения Холецкого. Вместо обращения матрицы, которое может быть неустойчивым, мы можем записать:\n\n$$X^TX=LL^T \n$$\n\nТогда решение нормальных уравнений можно выразить через две треугольные системы:\n\n1. Сначала решить $Lz = X^Ty$.\n2. Затем решить $L^T\\beta = z$.\n\nЭтот метод численно более устойчив.\n\nДопустим, у нас есть матрица признаков $X$ и вектор целевой переменной $y$:\n\n$$X= \\begin{pmatrix} 1 \u0026 1 \\\\ 1 \u0026 2 \\\\ 1 \u0026 3 \\end{pmatrix},\\quad y= \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}. \n$$\n\nТогда:\n\n$$X^TX=\\begin{pmatrix} 1 \u0026 1 \u0026 1 \\\\ 1 \u0026 2 \u0026 3 \\end{pmatrix}\\cdot\\begin{pmatrix} 1 \u0026 1 \\\\ 1 \u0026 2 \\\\ 1 \u0026 3 \\end{pmatrix}= \\begin{pmatrix} 3 \u0026 6 \\\\ 6 \u0026 14 \\end{pmatrix} \n$$\n\nПостроим правую часть $X^T y$:\n\n$$X^Ty=\\begin{pmatrix} 1 \u0026 1 \u0026 1 \\\\ 1 \u0026 2 \u0026 3 \\end{pmatrix}\\cdot\\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}=\\begin{pmatrix} 5 \\\\ 11 \\end{pmatrix} \n$$\n\nВместо того чтобы решать $\\beta = (X^T X)^{-1} X^T y$, заменим $X^T X$ разложение Холецкого: $X^T X = LL^T$. Мы хотим найти нижнетреугольную матрицу $L$, такую, что:\n\n$$LL^T= \\begin{pmatrix} l_{11} \u0026 0 \\\\ l_{21} \u0026 l_{22} \\end{pmatrix}\\cdot \\begin{pmatrix} l_{11} \u0026 l_{21} \\\\ 0 \u0026 l_{22} \\end{pmatrix}= \\begin{pmatrix} l_{11}^2 \u0026 l_{11}l_{21} \\\\ l_{11}l_{21} \u0026 l_{21}^2+l^2_{22} \\end{pmatrix}= \\begin{pmatrix} 3 \u0026 6 \\\\ 6 \u0026 14 \\end{pmatrix} \n$$\n\nНайдём элементы матрицы $L$:\n\n$$\\begin{cases} l_{11}^2 = 3\\\\ l_{11} l_{21} = 6\\\\ l_{21}^2+l^2_{22} = 14 \\end{cases}\\implies \\begin{cases} l_{11} = \\sqrt{3}\\\\ l_{21} = 2\\sqrt{3}\\\\ l_{22} = \\sqrt{2} \\end{cases}\\implies L=\\begin{pmatrix} \\sqrt{3} \u0026 0 \\\\ 2\\sqrt{3} \u0026 \\sqrt{2} \\end{pmatrix} \n$$\n\nРешаем уравнение $X^T X \\beta = X^T y$ через разложение Холецкого:\n\n$$X^Ty=LL^T\\beta=\\begin{pmatrix} 5 \\\\ 11 \\end{pmatrix} \n$$\n\nОбозначим $z = L^T \\beta$, тогда:\n\n$$L z = X^T y=\\begin{pmatrix} 5 \\\\ 11 \\end{pmatrix} \n$$\n\nПодставим:\n\n$$L z = X^T y=\\begin{pmatrix} \\sqrt{3} \u0026 0 \\\\ 2\\sqrt{3} \u0026 \\sqrt{2} \\end{pmatrix}\\cdot \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}=\\begin{pmatrix} 5 \\\\ 11 \\end{pmatrix}\\implies \\begin{cases} \\sqrt{3}z_1=5\\\\ 2\\sqrt{3}z_1+ \\sqrt{2}z_2=11 \\end{cases}\\implies \n$$\n\n$$\\begin{cases} \\sqrt{3}z_1=5\\\\ \\sqrt{2}z_2=1 \\end{cases}\\implies \\begin{cases} z_1=\\frac{5}{\\sqrt{3}}\\\\ z_2=\\frac{1}{\\sqrt{2}} \\end{cases} \n$$\n\nТеперь решаем $L^T \\beta = z$:\n\n$$\\begin{pmatrix} \\sqrt{3} \u0026 2\\sqrt{3} \\\\ 0 \u0026 \\sqrt{2} \\end{pmatrix}\\cdot\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}=\\begin{pmatrix} \\frac{5}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\implies \\begin{cases} \\sqrt{3}\\beta_0 + 2\\sqrt{3} \\beta_1=\\frac{5}{\\sqrt{3}}\\\\ \\sqrt{2}\\beta_1=\\frac{1}{\\sqrt{2}} \\end{cases}\\implies \n$$\n\n$$ \\begin{cases} \\sqrt{3}\\beta_0 + \\sqrt{3} =\\frac{5}{\\sqrt{3}}\\\\ \\beta_1=\\frac{1}{2} \\end{cases}\\implies \\begin{cases} \\beta_0 + 1 =\\frac{5}{3}\\\\ \\beta_1=\\frac{1}{2} \\end{cases}\\implies \\begin{cases} \\beta_0 =\\frac{2}{3}\\\\ \\beta_1=\\frac{1}{2} \\end{cases}\n$$\n\nМы получили коэффициенты регрессионной прямой без оборачивания матриц.\n\nРазличные способы разложения матриц — от универсального SVD до специализированного разложения Холецкого — позволяют эффективно решать задачи линейной алгебры. Однако при всей их полезности остаётся важный практический вопрос: насколько надёжны полученные решения в присутствии шума, погрешностей округления или неидеальных данных?\n\nВ реальных задачах машинного обучения данные почти всегда содержат ошибки, измерения неточны, а признаки могут быть сильно коррелированы. В таких условиях устойчивость численного метода становится критически важной. Один из ключевых показателей этой устойчивости — число обусловленности матрицы.\n\nЧем хуже обусловлена система, тем менее надёжны будут вычисленные параметры модели. Даже минимальные изменения во входных данных могут привести к сильным колебаниям в решении. Чтобы количественно оценить, насколько чувствительно решение системы $Ax = b$ к изменениям в $b$, и понять, какой метод разложения стоит использовать, введём понятие числа обусловленности.\n\n### Число обусловленности\n\nВсе три разложения — SVD, QR и разложение Холецкого — по-разному связаны с понятием численной устойчивости.\n\nSVD считается наиболее устойчивым методом, поскольку сингулярные числа напрямую измеряют степень «растяжения» матрицы вдоль различных направлений. Если одно из сингулярных чисел мало (почти нуль), то это значит, что в этом направлении матрица сильно сжимает пространство и задача может стать плохо обусловленной: малые изменения во входных данных будут вызывать большие отклонения в решении.\n\nОтношение между максимальным и минимальным сингулярными числами называется числом обусловленности матрицы:\n\n$$k(A)=\\frac{\\sigma_{\\max}}{\\sigma_{\\min}} \n$$\n\nОно показывает, насколько решение $Ax = b$ чувствительно к небольшим изменениям в $b$. Большое число обусловленности — это признак нестабильности (плохой обусловленности), особенно опасной при решении систем, в которых матрица $X^TX$ близка к вырожденной. Если матрица плохо обусловлена, то даже небольшие погрешности в матрице признаков $X$ или целевой переменной $y$ могут привести к большим ошибкам в предсказанных весах $w$. Такое может случиться, если:\n\n- **Признаки сильно коррелированы (мультиколлинеарность).** Если два признака почти линейно зависимы (например, «доход» и «налог»), то в $X^TX$ возникает почти линейная зависимость между столбцами, и матрица становится близкой к вырожденной.\n- **Признаки на разных масштабах.** Если один столбец $X$ содержит значения от $0$ до $1$, а другой — от $1000$ до $100 000$, то масштаб сингулярных значений различается, и это тоже ухудшает обусловленность.\n- **Слишком много признаков (**$n\u003em$ в $X \\in \\mathrm{R}^{m\\times n}$). Если признаков больше, чем объектов, то $X^TX$ точно будет сингулярной — потому что ранг $X$ не может превышать $m$, а значит, у неё будет меньше независимых направлений, чем размерность.\n\nЕсли $X^TX$ плохо обусловлена или сингулярна, то:\n\n- Обратная матрица $X^TX$ либо не существует, либо её вычисление нестабильно.\n- Весовой вектор $w$ становится неустойчивым: малейшие изменения в данных вызывают большие скачки в предсказаниях.\n- Модель может легко переобучиться: она подгоняет шум, а не закономерности.\n\nQR-разложение, так же, как SVD, обладает высокой устойчивостью, особенно потому, что ортогональные матрицы $Q$ не искажают геометрию: они сохраняют длины и углы. Это позволяет избежать накопления ошибок в промежуточных шагах и делает метод пригодным для больших и разреженных систем. Если при QR-разложении в диагональных элементах матрицы $R$ появляются почти нулевые значения, это сигнал о том, что соответствующие признаки линейно зависимы — таким образом, QR помогает также оценить ранг матрицы.\n\nРазложение Холецкого является самым быстрым из трёх, но при этом требует, чтобы матрица была строго положительно определённой. Если в процессе разложения диагональные элементы $L$ становятся нулевыми или мнимыми (что не должно происходить в корректно предобработанных данных), это свидетельствует о вырожденности или почти линейной зависимости столбцов исходной матрицы. Такое поведение — явный признак плохой обусловленности и необходимости регуляризации.\n\n{% cut \"**Число обусловленности и ранг матрицы в Python**\" %}\n\n```Python\nimport numpy as np\n\nA = np.array([[1, 1], [1, 1.0001]])\ncond = np.linalg.cond(A) # Число обусловленности\nrank = np.linalg.matrix_rank(A) # Ранг матрицы\n```\n\n{% endcut %}\n\n***\n\nОтлично, с матричными разложениями разобрались!\n\nДавайте коротко вспомним, о чём шла речь в этом параграфе.\n\n- Мы узнали, что собственные значения и векторы помогают понять внутреннюю структуру матрицы и направление, в котором она растягивает или сжимает пространство.\n- Разобрались, как сингулярное разложение (SVD) позволяет сжать данные, оставив только самые значимые компоненты и минимизировав потерю информации.\n- Поняли, что разложение Холецкого важно для стабильного решения систем уравнений и численной устойчивости вычислений.\n- И, наконец, обсудили, как по рангу и числу обусловленности можно оценить, не потеряем ли мы информацию при обучении модели на данных.\n\nА в следующем параграфе мы обсудим, как упростить анализ данных с помощью снижения размерности.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801950.544ef45d4c1f6480508f991546a9479c6d185451?iframe=1\" frameborder=\"0\" name=\"ya-form-13801950.544ef45d4c1f6480508f991546a9479c6d185451\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6c:Tb5c8,"])</script><script nonce="">self.__next_f.push([1,"Многие задачи машинного обучения сталкиваются с данными высокой размерности: тексты, изображения, временные ряды, графы. Однако полезная информация в таких данных зачастую лежит в значительно меньшем пространстве.\n\nЭто связано с наличием скрытых, или латентных, факторов, которые определяют структуру данных. Методы снижения размерности позволяют выявить эти скрытые зависимости, избавиться от шума и упростить анализ.\n\nВ этом параграфе мы рассмотрим:\n\n- Как метод главных компонент извлекает главные направления изменчивости через SVD-разложение.\n- Как латентно-семантический анализ и NMF извлекают тематики и скрытые факторы.\n- Как матричная факторизация используется в рекомендательных системах для восстановления предпочтений.\n- И как реализовать эти методы на практике.\n\nПриступим!\n\n## Метод главных компонент и SVD\n\nМетод главных компонент (англ. Principal Component Analysis, PCA) — базовый метод линейного снижения размерности, который позволяет найти наиболее вариативные направления в данных. Основная идея — найти такую ортонормированную систему координат, в которой дисперсия данных максимальна на первых осях.\n\nВспомним график, на котором изображены главные компоненты. На графике оси $x_1$ и $x_2$ соответствуют исходным признакам, а векторы $z_1$ и $z_2$ — главным компонентам. Главные компоненты взаимно ортогональны и показывают направления наибольшей и второй по значимости дисперсии в данных соответственно.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_1_e26c8a807d.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Главные компоненты как ортогональные направления наибольшей и второй по значимости дисперсии данных\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГлавные компоненты как ортогональные направления наибольшей и второй по значимости дисперсии данных\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЧтобы это формализовать, поговорим о понятии ковариация.\n\n\u003e 💡`Ковариация` — это мера того, как два признака изменяются совместно.\n\nКовариация ведёт себя следующим образом:\n\n- Если оба признака увеличиваются или уменьшаются одновременно — ковариация положительна.\n- Если один из признаков растёт, а другой уменьшается — ковариация отрицательна.\n- Если между признаками нет линейной зависимости, то ковариация близка к нулю.\n\nДля двух числовых признаков $x$ и $y$ ковариация определяется как:\n\n$$\\mathrm{Cov}(x,y)=\\frac{1}{m}\\sum_{i=1}^m(x_i-\\bar{x})(y_i-\\bar{y}), \n$$\n\nгде:\n\n- $m$ — число объектов;\n- $\\bar{x}$ и $\\bar{y}$ — средние значения по признакам.\n\nЕсли у нас несколько признаков, мы можем вычислить ковариацию для каждой пары признаков — так формируется ковариационная матрица.\n\nПусть $\\bar{X} \\in \\mathbb{R}^{m \\times n}$ — матрица данных, центрированная по столбцам: из каждого признака (столбца) вычтено среднее. В ней $n$ признаков и $m$ объектов. Тогда ковариационная матрица имеет вид:\n\n$$\\mathrm{Cov}(X)=\\Sigma^{\\mathrm{Cov}}=\\frac{1}{m}\\bar{X}^T\\bar{X} \n$$\n\nЭто симметричная матрица размера $n \\times n$ где каждый элемент $\\Sigma_{ij}^{\\mathrm{Cov}}$ есть ковариация между признаками $i$ и $j$.\n\nРешая задачу нахождения собственных векторов этой матрицы $\\Sigma^{\\mathrm{Cov}}$, мы находим главные компоненты. На практике вместо собственных разложений применяют уже знакомое нам [SVD](https://education.yandex.ru/handbook/math/article/spektralnie-metodi-i-matrichnie-razlozheniia#:~:text=%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D0%B5%20%D1%81%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5%20%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20\\(SVD\\))-разложение:\n\n$$\\bar{X}=U\\Sigma V^T, \n$$\n\nгде столбцы $V$ — это главные компоненты, а матрица $\\Sigma$ содержит корни из дисперсий (стандартные отклонения) по направлениям главных компонент.\n\n**Важный нюанс про обозначения.** В этой главе мы встречаем две разные матрицы, которые по традиции обозначаются одинаково — греческой буквой $\\Sigma$:\n\n- $\\Sigma$ в $\\mathrm{Cov}(x,y)=\\Sigma^{\\mathrm{Cov}}=\\frac{1}{m}\\bar{X}^T\\bar{X}\\in \\mathbb{R}^{n\\times n}$ — это ковариационная матрица данных $X \\in \\mathbb{R}^{m \\times n}$, симметричная и положительно полуопределённая. Её собственные значения $\\lambda_i^{\\mathrm{Cov}}$ отражают дисперсии вдоль главных компонент.\n- $\\Sigma$ в $\\bar{X}=U\\Sigma V^T$ — это прямоугольно-диагональная матрица сингулярных чисел размером $m\\times n$. Её диагональные элементы $\\sigma_i$ — это стандартные отклонения проекций данных на главные компоненты (если матрица $X$ центрирована по столбцам). Сингулярные числа матрицы $\\bar{X}$, напомним, — это корни собственных значений матрицы $\\bar{X}^T\\bar{X}$.\n\nЭти два объекта связаны следующим образом: если $\\bar{X}$ центрирована, то собственные значения ковариационной матрицы $\\lambda_i^{\\mathrm{Cov}}$ и сингулярные числа $\\sigma_i$ прямоугольно-диагональной матрицы связаны как $\\frac{\\sigma_i^2}{m}=\\lambda_i^{\\mathrm{Cov}}$.\n\nЭто объясняет, почему иногда говорят, что SVD и PCA — это два взгляда на одно и то же явление: один — через собственные значения, другой — через сингулярные числа.\n\nКвадрат матрицы $\\Sigma$ из SVD напрямую связана с PCA. При разложении $\\bar{X}=U\\Sigma V^T$ матрица $\\bar{X}^T\\bar{X}$, используемая в PCA, раскладывается как:\n\n$$\\bar{X}^T\\bar{X} =\\left(U\\Sigma V^T\\right)^T\\left(U\\Sigma V^T\\right)=V\\Sigma^TU^TU\\Sigma V^T=V\\Sigma^TI\\Sigma V^T=V\\Sigma^T\\Sigma V^T=\n$$\n\n$$V\\Sigma^2 V^T \n$$\n\nЭто следует из свойств ортогональной матрицы $U$: $U^TU=I$. Мы пришли к формуле спектрального разложения через SVD. Таким образом, $\\Sigma^2$ содержит собственные значения матрицы $X^TX$, а после деления на $m$ — дисперсию вдоль главных компонент. Это и есть набор собственных значений (спектр) ковариационной матрицы $\\Sigma^{\\mathrm{Cov}}$. Поэтому сингулярные числа $\\sigma_i$ в SVD позволяют сразу восстановить спектр PCA:\n\n$$\\lambda_i^\\mathrm{Cov}=\\frac{\\sigma_i^2}{m};\\quad \\Sigma^\\mathrm{Cov}=\\frac{1}{m}\\bar{X}^T\\bar{X}=V\\left(\\frac{\\Sigma^2}{m}\\right)V^T. \n$$\n\nМы видим, что дисперсия — это всего лишь частный случай ковариации, когда сравниваем признак с самим собой.\n\n\u003e 💡`Ковариационная матрица` — это обобщение дисперсии для случая многомерных данных.\n\nЧтобы снизить количество компонент до $k$, достаточно оставить первые $k$ главных компонент, тем самым проектируя данные в подпространство размерности $k$:\n\n$$Z=\\bar{X}V_k\\in\\mathbb{R}^{m\\times k}. \n$$\n\nЭто наиболее информативное линейное представление данных.\n\n### Пример\n\nРассмотрим пример. Допустим, у нас есть матрица признаков X, состоящая из трёх наблюдений и двух признаков:\n\n$$X= \\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{pmatrix} \n$$\n\nНайдём проекцию наших данных на две главные компоненты.\n\n**Шаг 1.** Центрирование матрицы $X$. Считаем среднее по столбцам и вычитаем его из каждого элемента соответствующего столбца:\n\n$$\\bar{x}_1=\\frac{1+0-1}{3}=0,\\quad\\bar{x}_2=\\frac{0+1-1}{3}=0;\\\\\n$$\n\n$$\\bar{X}= \\begin{pmatrix} 1-0 \u0026 0-0 \\\\ 0-0 \u0026 1-0 \\\\ -1-0 \u0026 -1-0 \\end{pmatrix}=\\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{pmatrix}. \n$$\n\nТо есть наша матрица уже центрирована.\n\n**Шаг 2.** Нахождение ковариационной матрицы.\n\n$$\\Sigma^\\mathrm{Cov}=\\frac{1}{3}\\bar{X}^T\\bar{X}=\\frac{1}{3}\\cdot \\begin{pmatrix} 1 \u0026 0 \u0026 -1\\\\ 0 \u0026 1 \u0026 -1\\\\ \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{pmatrix}=\\frac{1}{3}\\cdot \\begin{pmatrix} 2 \u0026 1 \\\\ 1 \u0026 2 \\\\ \\end{pmatrix}= \\begin{pmatrix} \\frac{2}{3} \u0026 \\frac{1}{3} \\\\ \\frac{1}{3} \u0026 \\frac{2}{3} \\\\ \\end{pmatrix} \n$$\n\n**Шаг 3.** Найдём матрицу $V$. Для этого надо рассчитать собственные значения и векторы ковариационной матрицы $\\Sigma^\\mathrm{Cov}$. Алгоритм расчёта мы рассматривали ранее: сначала составляем характеристическое уравнение, находим собственные значения, а потом для каждого из них собственные векторы.\n\n$$\\det\\begin{pmatrix} \\frac{2}{3}-\\lambda \u0026 \\frac{1}{3} \\\\ \\frac{1}{3} \u0026 \\frac{2}{3}-\\lambda \\\\ \\end{pmatrix}=\\left(\\frac{2}{3}-\\lambda\\right)^2-\\left(\\frac{1}{3}\\right)^2=\\frac{4}{9}-\\frac{4}{3}\\lambda + \\lambda^2-\\frac{1}{9}=\n$$\n\n$$\\lambda^2-\\frac{4}{3}\\lambda+\\frac{1}{3}=0;\\\\\n$$\n\n$$D=\\left(-\\frac{4}{3}\\right)^2-\\frac{4}{3}=\\frac{16-12}{9}=\\frac{4}{9}\u003e0\\implies\\text{Два корня};\\\\\n$$\n\n$$\\lambda_1=\\frac{\\frac{4}{3}-\\sqrt{\\frac{4}{9}}}{2}=\\frac{\\frac{4}{3}-\\frac{2}{3}}{2}=\\frac{1}{3},\\quad \\lambda_1=\\frac{\\frac{4}{3}+\\sqrt{\\frac{4}{9}}}{2}=\\frac{\\frac{4}{3}+\\frac{2}{3}}{2}=1. \n$$\n\nСобственный вектор для $\\lambda_1=\\frac{1}{3}$:\n\n$$\\begin{pmatrix} \\frac{2}{3}-\\frac{1}{3} \u0026 \\frac{1}{3} \\\\ \\frac{1}{3} \u0026 \\frac{2}{3}-\\frac{1}{3} \\\\ \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\\\ \\end{pmatrix}=0 \\implies \\frac{x}{3}+\\frac{y}{3}=0\\implies v_1= \\begin{pmatrix} x \\\\ -x \\\\ \\end{pmatrix}\\implies\\\\\n$$\n\n$$v_1= \\begin{pmatrix} 1 \\\\ -1 \\\\ \\end{pmatrix} \n$$\n\nНе забываем нормировать вектор по его длине:\n\n$$\\|v_1\\|_2=\\sqrt{1^2+{-1}^2}=\\sqrt{2}\\implies v_1= \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\\\ \\end{pmatrix} \n$$\n\nСобственный вектор для $\\lambda_2=1$:\n\n$$\\begin{pmatrix} \\frac{2}{3}-1 \u0026 \\frac{1}{3} \\\\ \\frac{1}{3} \u0026 \\frac{2}{3}-1 \\\\ \\end{pmatrix}\\cdot \\begin{pmatrix} x \\\\ y \\\\ \\end{pmatrix}=0 \\implies \\begin{cases} -\\frac{x}{3}+\\frac{y}{3}=0\\\\ \\frac{x}{3}-\\frac{y}{3}=0 \\end{cases}\\implies v_2= \\begin{pmatrix} x \\\\ x \\\\ \\end{pmatrix}\\implies\\\\\n$$\n\n$$v_2= \\begin{pmatrix} 1 \\\\ 1 \\\\ \\end{pmatrix};\\\\\n$$\n\n$$\\|v_2\\|_2=\\sqrt{1^2+1^2}=\\sqrt{2}\\implies v_2= \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ \\end{pmatrix} \n$$\n\nИтак, мы нашли матрицу $V$:\n\n$$V=\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}}\\\\ \\end{pmatrix} \n$$\n\n**Шаг 4.** Находим проекцию данных на пространство главных компонент.\n\n$$Z=\\bar{X}V,\\\\ Z= \\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{pmatrix}\\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}}\\\\ \\end{pmatrix}= \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ 0 \u0026 -\\sqrt{2} \\end{pmatrix}\n$$\n\nВ реальных задачах мы не вычисляем ковариационную матрицу и её спектр вручную — всё это делают библиотеки.\n\n{% cut \"Примеры в Python\" %}\n\nНапример, в `numpy` ковариационная матрица централизованных данных легко вычисляется как:\n\n```Python\nimport numpy as np\n\nX_centered = X - X.mean(axis=0)\nSigma = X_centered.T @ X_centered / X_centered.shape[0]\n```\n\nили с помощью встроенной функции:\n\n```Python\nimport numpy as np\n\nSigma = np.cov(X.T, bias=True)\n```\n\nВ `np.cov(X.T)` по умолчанию используется деление на $m−1$, соответствующее несмещённой статистической оценке дисперсии. Однако в PCA принято делить на $m$, так как ковариационная матрица задаёт линейное преобразование, а не оценку параметров. Чтобы получить именно такую матрицу, нужно указать `bias=True` — в этом случае NumPy делит на $m$, как требует определение PCA.\n\nЕсли используется `Sklearn`, всё упрощается ещё сильнее – библиотека сама центрирует и применяет SVD:\n\n```Python\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=k)\nZ = pca.fit_transform(X)  # X автоматически центрируется\n```\n\nКомпоненты доступны через `pca.components_`, а дисперсии (собственные значения ковариационной матрицы) — через `pca.explained_variance_`.\n\n{% endcut %}\n\nPCA продемонстрировал, как с помощью SVD можно свести многомерные числовые данные к компактному представлению, выявляя главные направления изменчивости. Однако спектральные методы анализа не ограничиваются задачами снижения размерности для числовых таблиц. Они находят применение и в более сложных, структурированных данных — например, при извлечении скрытых тем из корпуса документов (LSA, NMF) или при выявлении латентных факторов вкуса пользователей на основе истории оценок (ALS, SVD в рекомендациях).\n\nРассмотрим, как эти методы используют разложения матриц для анализа текстов и других высокоразмерных данных.\n\n## Латентно-семантический анализ и неотрицательное матричное разложение\n\nМетоды, подобные PCA, не ограничиваются только табличными числовыми признаками. В задачах обработки текстов и рекомендаций также широко применяются спектральные методы и разложения матриц.\n\nОдни из ключевых примеров:\n\n- Латентно-семантический анализ (Latent Semantic Analysis, [LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis)).\n- Неотрицательное матричное разложение (Non-negative Matrix Factorization, [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)).\n\nОни позволяют выявлять скрытые темы в текстах, уменьшать размерность и строить интерпретируемые представления объектов и признаков.\n\nДавайте разберём их подробнее.\n\n### LSA\n\nЭто метод понижения размерности в пространстве «документ-термин», который применяется к разреженным матрицам текстов. Идея в том, что термины в языках взаимозависимы, и часто встречаются вместе в похожих контекстах. Мы хотим найти скрытые темы, в которых совместно участвуют слова и документы.\n\nПусть дана матрица $A\\in\\mathbb{R}^{m \\times n}$, где:\n\n- $m$ — количество документов,\n- $n$ — количество терминов,\n- $A_{ij}$ — число вхождения слова $j$ в документе $i$ (или [TF-IDF](https://education.yandex.ru/handbook/math/article/proektsii-ugli-i-ortogonalnost#ortogonalnost-i-ortogonalizaciya:~:text=%D0%9F%D0%BE%D0%B4%D1%80%D0%BE%D0%B1%D0%BD%D0%B5%D0%B5-,TF%2DIDF,-\\(Term%20frequency%2Dinverse)).\n\nТогда латентно-семантический анализ — это просто SVD матрицы $A$:\n\n$$A_k=U_k\\Sigma_k V^T_k \n$$\n\n- Строки в $U_k\\in\\mathbb{R}^{m \\times k}$ — это представление документов в тематическом пространстве;\n- Строки в $V^T_k\\in\\mathbb{R}^{k \\times n}$ — представления терминов в том же пространстве;\n- $\\Sigma_k$ — матрица сингулярных значений, отражающая важность каждой темы (масштаб растяжения по каждой оси). Это даёт вложения как для слов, так и для документов в общее латентное тематическое пространство.\n\nSVD-разложение матрицы «документ-термин» мы показали на рисунке ниже.\n\nОбозначения:\n\n- $U_k \\in \\mathbb{R}^{m \\times k}$ — координаты документов в тематическом пространстве;\n- $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ — масштаб важности тем;\n- $V_k^T \\in \\mathbb{R}^{k \\times n}$ — координаты терминов.\n\nКоличество тем $k \\ll \\min(m, n)$, и разложение позволяет выявить скрытую структуру и семантическую близость между текстами и словами.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_2_51592c10eb.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"SVD-разложение матрицы\"\n  \u003e\n  \u003cfigcaption\u003e\n\nSVD-разложение матрицы «документ-термин» в методе латентно-семантического анализа (LSA): $A\\approx U_k\\Sigma_k V^T_k$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТаким образом, LSA — это PCA для текстов, но применяемый к разрежённой и часто TF-IDF-нормированной матрице.\n\n**Преимущество** LSA заключается в выявлении синонимии и тематической близости между словами и документами, устойчивости к шуму.\n\n**Недостаток** заключается в том, что матрицы $U_k$ и $\\Sigma_k$ могут быть отрицательными, что затрудняет интерпретацию.\n\nПосле снижения размерности (через $V_k^T$) каждый термин представляется вектором в тематическом пространстве. На основе этих векторов можно вычислить семантическую близость между терминами — например, по косинусной мере:\n\n$$r_{ij}=\\cos \\theta= \\frac{\\langle T_i, T_j \\rangle}{\\|T_i\\| \\cdot \\|T_j\\|}, \n$$\n\nгде $r_{ij}$ — тематическое сходство; векторы $T_i$ и $T_j$.\n\nТакой подход позволяет выявить семантически схожие слова, даже если они не встречаются в одних и тех же документах, но проявляют похожую тематическую структуру.\n\nРисунок ниже иллюстрирует получение матрицы семантической близости. Выделенные столбцы $t_i$ и $t_j$ представляют векторы термина $T_i$ и $T_j$. Их косинусная близость $r_{ij} = \\cos(\\theta)$ интерпретируется как семантическое сходство. Полученная матрица «термин-термин» отражает связи между словами на основе их распределения по документам.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_3_d34400fa08.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Переход от матрицы «документ-термин» к матрице семантической близости между терминами\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПереход от матрицы «документ-термин» к матрице семантической близости между терминами\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n{% cut \"Реализация в Python\" %}\n\nLSA реализуется через SVD, обычно применяется к TF-IDF-матрице:\n\n```Python\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(documents)\n\nsvd = TruncatedSVD(n_components=100)  # число тем\nX_lsa = svd.fit_transform(X) # Матрица документов в тематическом пространстве\n\nsvd.components_ # Описание тем через термины\n```\n\n{% endcut %}\n\n### NMF\n\nВ отличие от SVD, NMF требует, чтобы все элементы разложения были неотрицательными:\n\n$$A\\approx WH; \\quad W\\in\\mathbb{R}^{m\\times k}; \\quad H\\in\\mathbb{R}^{k\\times n};\\quad W,H\\geq0. \n$$\n\nГде:\n\n- $A$ — матрица документов и терминов (обычно TF-IDF);\n- $W$ — представление документов через темы;\n- $H$ — представление тем через слова.\n\nКаждая строка $W_i$ — это распределение тем в документе $i$, а каждая строка $H_j$ — распределение слов в теме $j$. Благодаря неотрицательности темы легко интерпретировать: они представляют собой набор характерных слов, а документ — смесь тем.\n\nНиже представлена иллюстрация для NMF. Исходная матрица $A \\in \\mathbb{R}^{m \\times n}$ содержит частоты терминов в документах и приближается произведением двух неотрицательных матриц:\n\n- $W \\in \\mathbb{R}^{m \\times k}$ — распределение тем в документах;\n- $H \\in \\mathbb{R}^{k \\times n}$ — распределение терминов по темам.\n\nКаждая строка $W_i$ интерпретируется как вероятностный профиль документа по темам, а каждая строка $H_j$ — как характерные слова темы. В отличие от SVD, NMF обеспечивает интерпретируемость благодаря неотрицательности компонент.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_4_c7101e646d.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Неотрицательное матричное разложение (NMF) матрицы «документ-термин»\"\n  \u003e\n  \u003cfigcaption\u003e\n\nНеотрицательное матричное разложение (NMF) матрицы «документ-термин»\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nДостоинства NMF в интерпретируемости и сходстве с вероятностными моделями. Минусы в отсутствии ортогональности, итеративном поиске решений.\n\n{% cut \"Реализация в Python\" %}\n\n```Python\nРеализация в Python\nfrom sklearn.decomposition import NMF\n\nnmf = NMF(n_components=100, init='nndsvd')\n\nW = nmf.fit_transform(X)  # документы в темы от X - TF-IDF\nH = nmf.components_       # темы в слова\n```\n\n{% endcut %}\n\nВот таблица, в которой мы сравнили оба метода:\n\n#|\n||\n\n**Характеристика**\n\n|\n\n**LSA**\n\n|\n\n**NMF**\n\n||\n||\n\n**Метод**\n\n|\n\nSVD\n\n|\n\nНеотрицательная факторизация\n\n||\n||\n\n**Интерпретируемость**\n\n|\n\nЗатруднена из-за отрицательных значений\n\n|\n\nЕсть, так как все значения положительны\n\n||\n||\n\n**Геометрия**\n\n|\n\nОртогональное пространство\n\n|\n\nЧастично перекрывающиеся направления\n\n||\n||\n\n**Оптимизация**\n\n|\n\nВычисляется точно через SVD\n\n|\n\nИтеративно, численно\n\n||\n|#\n\nИ PCA, и его тематические аналоги — LSA и NMF — опираются на линейную структуру данных. Они ищут линейные комбинации признаков, которые максимально объясняют вариацию, выявляют тематики или скрытые факторы. Такие методы эффективно работают, когда данные действительно лежат в плоском линейном подпространстве или близки к нему.\n\nОднако в реальных задачах, особенно связанных с изображениями, текстами, биологическими или поведенческими измерениями, структура данных может быть гораздо более сложной.\n\nЧтобы обнаружить нелинейную структуру данных, были разработаны специальные методы снижения размерности, основанные не на линейных преобразованиях, а на сохранении локальной топологии: отношений соседства, плотности и формы кластеров. С этими методами мы сможете познакомиться в главе по [теории вероятностей](https://education.yandex.ru/handbook/math/article/entropiia-perpleksiia-kl-divergentsiia#t-sne).\n\nА пока давайте поймём, как действовать, если перед нами задача не только понять структуру, но и предсказать неизвестные значения.\n\nНапример, в рекомендательных системах мы хотим не только визуализировать предпочтения пользователей, но и восстановить пропущенные оценки: какие фильмы, товары или книги могут заинтересовать человека, — исходя из его поведения и предпочтений других.\n\nВ таких задачах применяется матричная факторизация — подход, который, подобно SVD или NMF, разлагает данные на скрытые компоненты, но делает это с учётом разрежённости данных. В отличие от методов визуализации, здесь важен не столько внешний вид латентного пространства, сколько его способность воспроизводить и дополнять структуру пользовательских предпочтений.\n\n## Матричная факторизация в рекомендательных системах\n\nВ задачах рекомендаций (фильмов, товаров, книг) данные представлены в виде разрежённой матрицы предпочтений: большинство пользователей оценивают лишь небольшую часть объектов.\n\nОднако за этой разрежённостью скрывается структура — предпочтения можно объяснить латентными (скрытыми) факторами, такими как жанры, стили, категории. Матричная факторизация позволяет выявить эти факторы и восстановить недостающие оценки, формируя персонализированные рекомендации.\n\nДопустим, у нас есть количество фильмов и пользователей, которые оценивают эти фильмы. Эти данные можно представить матрицей $R\\in\\mathbb{R}^{m\\times n}$, где:\n\n- $m$ — число пользователей;\n- $n$ — число фильмов;\n- $R_{ij}$ — это оценка пользователя $i$ фильма $j$ либо пропуск, если пользователь $i$ не оценивал фильм $j$.\n\nТакие матрицы очень разрежены: пользователь оценивает лишь малую долю объектов. Задача — восстановить недостающие оценки, то есть предсказать $R_{ij}$, которые пропущены в матрице $R$.\n\nПредположим, что пользователь $i$ обладает вкусовыми признаками: любит драму, боевики, старое кино, комедии и т. д. Эти вкусы можно описать как вектор признаков пользователя:\n\n$$W_i\\in\\mathbb{R}^{k} \n$$\n\nА фильм $j$ можно описать вектором характеристик:\n\n$$H_j\\in\\mathbb{R}^{k} \n$$\n\nГде каждая компонента — насколько фильм выражает ту или иную тему. Идея факторизации в данном случае состоит в том, чтобы представить матрицу $R$ как приближённое произведение этих двух матриц меньшей размерности — матрицы признаков (вкусов) пользователей и матрицы характеристик (тем) фильмов:\n\n$$R\\approx WH;\\quad W\\in\\mathbb{R}^{m\\times k};\\quad H\\in\\mathbb{R}^{k\\times n}. \n$$\n\nГде:\n\n- $W$ — матрица скрытых предпочтений пользователей: строка $W_i$ — это вектор из $k$ латентных факторов, описывающих пользователя $i$;\n- $H$ — матрица скрытых характеристик объектов: столбец $H_j$ — это вектор из $k$ латентных факторов, описывающих фильм $j$.\n\nНа рисунке ниже показана схема разложения разреженной матрицы рейтингов $R\\in \\mathbb{R}^{m\\times n}$ в произведение двух матриц меньшей размерности. $W\\in \\mathbb{R}^{m\\times k}$ — предпочтения пользователей; $H\\in \\mathbb{R}^{k\\times n}$ — характеристики фильмов. Число латентных факторов $k\\ll\\min(m,n)$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_5_5688802a96.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Разложение разреженной матрицы рейтингов $R\\in \\mathbb{R}^{m\\times n}$ в произведение двух матриц меньшей размерности\"\n  \u003e\n  \u003cfigcaption\u003e\n\nРазложение разреженной матрицы рейтингов $R\\in \\mathbb{R}^{m\\times n}$ в произведение двух матриц меньшей размерности\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПараметр $k$ — это число скрытых факторов (или латентная размерность), то есть размерность пространства, в котором находятся и пользователи, и объекты. Обычно $k\\ll\\min(m,n)$.\n\nТакое предположение оправдано эмпирически — предпочтения и свойства имеют внутреннюю структуру. Например, большинство фильмов можно описать комбинацией жанров; предпочтения пользователей часто коррелированы.\n\nПусть у нас есть множество известных оценок $\\Omega \\subseteq \\{1,…,m\\}\\times\\{1,…,n\\}$. При этом $R_{ij}$ известно только для $(i,j)$. Тогда мы минимизируем среднеквадратичную ошибку между предсказанием и наблюдаемыми значениями:\n\n$$\\min_{W,H}\\sum_{(i,j)\\in\\Omega}(R_{ij}-W_i\\cdot H_j)^2 \n$$\n\nчтобы обучить такие вектора, при которых восстановление $\\hat{R}{ij}\\approx R{ij}$ будет точным. Чтобы избежать переобучения, добавляется регуляризатор:\n\n$$\\min_{W,H}\\sum_{(i,j)\\in\\Omega}(R_{ij}-W_i\\cdot H_j)^2+\\lambda\\left(\\|W\\|^2_F+\\|H\\|^2_F\\right) \n$$\n\nГде $\\|W\\|^2_F$ — квадрат нормы Фробениуса матрицы $W$; $\\lambda$ — коэффициент регуляризации.\n\nПосле обучения:\n\n- Строка $W_i$ описывает вкусы пользователя $i$ в скрытом пространстве.\n- Столбец $H_j$ описывает темы\u0026nbsp;/\u0026nbsp;жанровую принадлежность фильма $j$.\n- Cкалярное произведение $W_i \\cdot H_j$ — это предсказанная оценка пользователя $i$ для фильма $j$.\n\nТаким образом, мы переходим от огромной разреженной матрицы к компактному представлению в общем пространстве.\n\nЧтобы визуализировать этот переход, мы взяли датасет [MovieLens](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset?select=rating.csv), уже ставший классикой для Recsys-задач, выбрали самых активных пользователей и самые популярные фильмы и свернули разреженную матрицу реальных рейтингов в компактное латентное пространство. Сначала центрируем $R$ по пользователям (вычитаем их средний рейтинг), получаем факторы $W$ и $H$ через TruncatedSVD, а затем для наглядности отображаем факторы фильмов в 2D с помощью [UMAP](https://education.yandex.ru/handbook/math/article/entropiia-perpleksiia-kl-divergentsiia#eshyo-pro-nelinejnye-metody-ponizheniya-razmernosti-umap).\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_10_6_21b0cad15c.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Слева — фрагмент матрицы\u003e Справа — UMAP-1/2\"\n  \u003e\n  \u003cfigcaption\u003e\n\nСлева — фрагмент матрицы $R$ (MovieLens), цвет — это оценка от 0.5 до 5, белый —\u0026nbsp;отсутствие оценки. Справа — UMAP-1/2: проекция латентных факторов фильмов (после центрирования по пользователям и TruncatedSVD); цвет — жанр\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа графике справа хорошо видно, зачем мы это делали: хаотичная и неполная матрица слева превратилась в осмысленную «карту фильмов». Объекты со схожими свойствами оказались рядом — например, фильмы одного жанра (боевики, драмы, комедии) образовали заметные кластеры. Это наглядно доказывает, что модель действительно нашла скрытую структуру: латентные факторы успешно отразили семантическую близость между фильмами, даже не имея информации о жанрах.\n\nМы убедились, что идея работает. Но как именно найти оптимальные матрицы $W$ и $H$, которые минимизируют ошибку на практике? Прямое решение этой задачи затруднительно.\n\nОдин из подходов для решения подобных задач — это ALS (англ. Alternating Least Squares). Так как прямая минимизация функции потерь, которую мы описали выше, невозможна (она нелинейна относительно $W$ и $H$), ALS производит оптимизацию решения иным способом. В ходе обучения одна из матриц фиксируется, в результате чего оптимизация второй матрицы сводится к задаче линейной регрессии.\n\nШаги алгоритма следующие:\n**Шаг 1.** Зафиксировать $H$, найти $W$ по методу наименьших квадратов.\n**Шаг 2.** Зафиксировать $W$, найти $H$.\n**Шаг 3.** Повторять до сходимости.\n\nКаждый шаг минимизирует квадратичную функцию, что обеспечивает быстрое решение. В ALS на каждом шаге мы решаем независимую задачу МНК для каждой строки матрицы $W$ или $H$, что позволяет эффективно обучать модель даже на больших данных. Метод гарантированно сходится (хотя не обязательно к глобальному минимуму).\n\n{% cut \"Реализация в Python\" %}\n\nДля реализации ALS в `Python` можно использовать библиотеку [implicit](https://github.com/benfred/implicit), работающую с рекомендательными системами:\n\n```Python\nrom implicit.als import AlternatingLeastSquares\nfrom scipy.sparse import csr_matrix\n\nR = csr_matrix(ratings_matrix)  # Разреженная матрица R\nmodel = AlternatingLeastSquares(factors=50, regularization=0.1)\n# Обучение модели\nmodel = AlternatingLeastSquares(factors=50, regularization=0.1)\nmodel.fit(R.T)  # Implicit ожидает объекты по строкам, поэтому транспонируем\n\n# Извлекаем матрицы латентных факторов\nW = model.user_factors # Размерность (m, k) — пользователи\nH = model.item_factors # Размерность (n, k) — объекты\n\n# Предсказание оценки пользователя i для объекта j:\ni, j = 0, 5  # Например, пользователь 0 и объект 5\nscore = np.dot(W[i], H[j])\n```\n\n{% endcut %}\n\nМатричная факторизация — это мощный метод для восстановления структуры и предпочтений в разреженных данных:\n\n- Позволяет предсказывать оценки и давать рекомендации.\n- Раскрывает скрытые факторы в поведении пользователей и свойствах объектов.\n- Широко используется в системах рекомендаций: Netflix, Spotify, Amazon.\n\nСвязь с линейной алгеброй проявляется в формулировке как задачи приближённого разложения матрицы, а также в применении градиентного спуска, регуляризации и факторизационных методов.\n\n***\n\nВ этом параграфе мы рассмотрели методы снижения размерности, которые позволяют выявлять скрытую структуру в данных, сжимать информацию и упрощать модели без существенной потери качества.\nВы узнали:\n\n- как линейные преобразования могут быть использованы для поиска направлений наибольшей изменчивости и скрытых факторов в данных;\n- как метод главных компонент (PCA) позволяет понизить размерность, сохранив максимальную дисперсию;\n- что SVD лежит в основе многих алгоритмов анализа текстов и изображений;\n- как LSA и NMF используют матричные разложения для извлечения тем из коллекции документов;\n- чем отличаются методы с ортогональными и неотрицательными компонентами и как это влияет на интерпретируемость результатов.\n\nВ следующем параграфе мы перейдём от анализа признаков к построению моделей: рассмотрим линейные модели и узнаем, как решать задачи регрессии и классификации, бороться с переобучением с помощью регуляризации и строить устойчивые предсказательные алгоритмы.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801952.4840041bf8e0b9a04ac7fdeebe18e2a6aaf8a519?iframe=1\" frameborder=\"0\" name=\"ya-form-13801952.4840041bf8e0b9a04ac7fdeebe18e2a6aaf8a519\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6d:T7a52,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы рассмотрим линейные модели — один из краеугольных камней машинного обучения. Несмотря на простоту, они лежат в основе многих более сложных алгоритмов. Это мощный инструмент анализа и интерпретации данных.\n\nМы начнём с линейной регрессии, изучим её в матричной форме, а также выведем нормальные уравнения, которые позволяют получить аналитическое решение. Затем перейдём к логистической регрессии — модели классификации, которая также основана на линейной гипотезе, но использует вероятностную интерпретацию.\n\nКлючевой частью этого раздела станет регуляризация. Мы подробно разберем, как гребневая (англ. ridge) и лассо-регрессия (англ. lasso) помогают бороться с переобучением и мультиколлинеарностью, и проанализируем их геометрический смысл.\n\n## Линейная регрессия в матричном виде\n\nЛинейные модели лежат в основе многих алгоритмов машинного обучения. В простейшем случае задача регрессии заключается в том, чтобы аппроксимировать зависимость между входами $X \\in \\mathbb{R}^{m \\times n}$ (признаки) и целевыми значениями $y \\in \\mathbb{R}^{m}$ при помощи линейной функции:\n\n$$\\hat{y}=Xw \n$$\n\nгде:\n\n- $X\\in\\mathbb{R}^{m\\times n}$ — матрица признаков (каждая строка — объект, каждый столбец — признак);\n- $w \\in \\mathbb{R}^n$ — вектор весов, который необходимо подобрать;\n- $\\hat{y}\\in \\mathbb{R}^{m}$ — вектор предсказанных значений целевой переменной.\n\nДля настройки параметров $w$ мы минимизируем среднеквадратичную ошибку (MSE):\n\n$$\\mathcal{L}(w)=\\frac{1}{2}\\|Xw-y\\|^2=\\frac{1}{2}(Xw-y)^T(Xw-y) \n$$\n\nМножитель$\\frac{1}{2}$ удобен для дальнейшего дифференцирования. Эта задача имеет чёткую геометрическую интерпретацию: мы ищем такую точку $\\hat{y}=Xw$ в подпространстве $\\mathrm{Col}(X)\\subset\\mathbb{R}^{m}$ (множество всех линейных комбинаций столбцов матрицы $X$ в пространстве размерности $m$), которая находится на минимальном расстоянии от вектора $y$ по евклидовой метрике. То есть $\\hat{y}$ — это ортогональная проекция $y$ на подпространство, натянутое на столбцы матрицы $X$.\n\nИз линейной алгебры [известно](https://education.yandex.ru/handbook/math/article/proektsii-ugli-i-ortogonalnost#ortogonalnaya-proekciya), что проекция вектора $y$ на подпространство $\\mathrm{Col}(X)\\subset\\mathbb{R}^{m}$ задаётся таким $Xw$, для которого вектор ошибки $y−Xw$ ортогонален подпространству. Это означает, что вектор ошибок ортогонален признаковому пространству — их скалярное произведение равно нулю:\n\n$$X^T(y−Xw)=0 \n$$\n\nЭто условие называется `нормальными уравнениями`, и оно описывает необходимое условие экстремума для задачи наименьших квадратов. Оно также возникает напрямую при взятии градиента от функции потерь:\n\n$$\\nabla_w\\mathcal{L}=\\nabla_w\\left(\\frac{1}{2}(Xw-y)^T(Xw-y)\\right) \n$$\n\nЧтобы найти минимум этой функции, нужно приравнять градиент к нулю. Попробуем расписать подробнее произведение внутри скобок с учётом свойств транспонированных матриц:\n\n$$(Xw-y)^T(Xw-y)=w^TX^TXw-w^TX^Ty-y^TXw+y^Ty=\\\\w^TX^TXw-2y^TXw+y^Ty \n$$\n\nТеперь мы можем найти градиент от $w$ для каждого слагаемого:\n\n- $\\nabla_w\\left(w^TX^TXw\\right)=2X^TX$\n- $\\nabla_w\\left(-2y^TXw\\right)=-2X^Ty$\n- $\\nabla_w\\left(y^Ty\\right)=0$, так как не зависит от $w$.\n\nСледовательно:\n\n$$\\nabla_w\\mathcal{L}=\\nabla_w\\left(\\frac{1}{2}(Xw-y)^T(Xw-y)\\right)=\\\\\n$$\n\n$$\\frac{1}{2}\\left(2X^TXw-2X^Ty\\right)=X^TXw-X^Ty=X^T(Xw-y) \n$$\n\nПриравнивая градиент к нулю, получаем:\n\n$$X^T(Xw-y)=0\\implies X^TXw=X^Ty \n$$\n\nЭто и есть нормальные уравнения — система линейных уравнений относительно $w$. Если матрица $X^TX$ невырождена (то есть обратима), то решение имеет аналитический вид:\n\n$$w=\\left(X^TX\\right)^{-1}X^Ty \n$$\n\nЭто решение интуитивно понятно: оно минимизирует евклидову ошибку между предсказанием $Xw$ и истинными значениями $y$, находя проекцию $y$ на пространство столбцов $X$. Мы [касались](https://education.yandex.ru/handbook/math/article/proektsii-ugli-i-ortogonalnost#ortogonalnaya-proekciya:~:text=%D0%98%D0%BB%D0%BB%D1%8E%D1%81%D1%82%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B3%D0%BE%20%D1%81%D0%BC%D1%8B%D1%81%D0%BB%D0%B0%20%D0%9C%D0%9D%D0%9A) этого в одном из предыдущих разделов. Ниже представлена уже знакомая вам иллюстрация геометрического смысла МНК — проекция вектора целевой переменной на пространство столбцов матрицы признаков.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_11_1_aabd843cd0.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация геометрического смысла МНК.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация геометрического смысла МНК. Расстояние $y-\\hat{y}$ — это не что иное как ошибка\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nОднако здесь же возникает и важный вопрос устойчивости: если признаки скоррелированы или плохо масштабированы, то $X^T X$ может оказаться плохо [обусловленной](https://education.yandex.ru/handbook/math/article/spektralnie-metodi-i-matrichnie-razlozheniia#matrichnye-razlozheniya:~:text=%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D0%B5%20%D1%87%D0%B8%D1%81%D0%BB%D0%B0%20%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8.-,%D0%A7%D0%B8%D1%81%D0%BB%D0%BE%20%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8,-%D0%92%D1%81%D0%B5%20%D1%82%D1%80%D0%B8%20%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F) или даже [сингулярной](https://education.yandex.ru/handbook/math/article/spektralnie-metodi-i-matrichnie-razlozheniia#matrichnye-razlozheniya:~:text=%D0%BD%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B9%2C%20%D1%87%D0%B5%D0%BC%20%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C.-,%D0%95%D1%81%D0%BB%D0%B8,%D0%BF%D0%BB%D0%BE%D1%85%D0%BE%20%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B0%20%D0%B8%D0%BB%D0%B8%20%D1%81%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%B0%2C%20%D1%82%D0%BE%3A,-%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0). Мы ещё вернёмся к этому в разделе про регуляризацию и препроцессинг.\n\n{% cut \"Реализация в Python\" %}\n\nВоспользуемся библиотекой `scikit-learn`:\n\n```Python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Данные\nnp.random.seed(42)\nm = 30  # Количество объектов\nX = 2 * np.random.rand(m, 1)\ny = 3 * X[:, 0] + 2 + np.random.randn(m) * 0.5  # Линейная зависимость + шум\n\n# Обучение линейной регрессии\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Предсказания\nX_grid = np.linspace(0, 2, 100).reshape(-1, 1)\ny_pred = model.predict(X_grid)\n\n# Извлечение параметров регрессионной прямой\nintercept = model.intercept_\ncoef = model.coef_\n\n(intercept, coef[0])  # Свободный член и коэффициент наклона\n```\n\n{% endcut %}\n\n## Логистическая регрессия\n\nВ задачах классификации чаще применяется логистическая регрессия. Она сохраняет линейную зависимость прогноза от признаков, но предсказывает не класс, а вероятность принадлежности к положительному классу:\n\n$$P(y=1 \\mid x) = \\sigma(x^T w) = \\frac{1}{1 + e^{-x^T w}} \n$$\n\nЗдесь $\\sigma(\\cdot)$ — сигмоидальная функция активации, обеспечивающая значения на выходе от $0$ до $1$. График ниже иллюстрирует, как сигмоида превращает линейное предсказание $x^T w \\in \\mathbb{R}$ в вероятность принадлежности к классу $y=1$. Центральная точка $\\sigma(0) = 0.5$ соответствует границе между классами.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_11_2_3159024533.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Сигмоида\"\n  \u003e\n  \u003cfigcaption\u003e\n\nСигмоида\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТаким образом, логистическая регрессия позволяет интерпретировать линейное предсказание $x^T w$ как логарифм отношения правдоподобий двух классов:\n\n$$P(y=0 \\mid x) = 1-P(y=1 \\mid x) = 1-\\frac{1}{1 + e^{-x^T w}}=\\frac{e^{-x^T w}}{1 + e^{-x^T w}};\\\\ \n$$\n\n$$\\ln\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}=\\ln\\frac{1\\cdot(1 + e^{-x^T w})}{(1 + e^{-x^T w})\\cdot e^{-x^T w}}=\\ln\\frac{1}{e^{-x^T w}}=\\ln e^{x^T w}=x^Tw;\n$$\n\n$$\\boxed{\\ln\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}=x^Tw}. \n$$\n\nЭто делает логистическую регрессию дискриминативной вероятностной моделью — она напрямую моделирует $P(y \\mid x)$ и аппроксимирует логарифм отношения правдоподобий двух классов.\n\nГеометрически логистическая регрессия ищет гиперплоскость, разделяющую пространство признаков на области, где вероятность принадлежности к каждому из классов превышает 0.5. В двумерном случае эта гиперплоскость превращается в прямую, а значение $x^T w + b = 0$ соответствует границе принятия решения. На рисунке ниже показано, как модель оценивает вероятности для каждого региона и как эта граница проходит между классами.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_11_3_e61abdbdcf.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Гиперплоскость\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГиперплоскость $x^T w = 0$ — это граница, где $\\sigma = 0.5$. Чем дальше точка от границы, тем ближе вероятность к $0$ или $1$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nФункция потерь в логистической регрессии — это логарифмическая функция правдоподобия (log-loss):\n\n$$\\mathcal{L}(w)=-\\sum_{i=1}^m\\left[y_i\\log\\sigma(x^T_iw)+(1-y_i)\\log(1-\\sigma(x^T_iw))\\right] \n$$\n\nВ отличие от линейной регрессии, здесь нет аналитического решения. Оптимальное значение параметров $w$ ищется итеративно — с помощью градиентного спуска.\n\nЭта функция потерь штрафует модель сильнее, когда уверенное предсказание оказывается неверным, и почти не штрафует, если высокая уверенность соответствует правильному ответу. На графике ниже показано, как log-loss меняется в зависимости от линейного отклика $z=x^Tw$ для случаев $y=1$ (красная линия) и $y=0$ (синяя линия).\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_11_4_66dd2d2cd9.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Логарифмическая функция\"\n  \u003e\n  \u003cfigcaption\u003e\n\nЛогарифмическая функция потерь сильно штрафует ошибки, особенно если модель уверена, но ошибается. Это делает обучение логистической регрессии чувствительным к уверенности в предсказаниях\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЛогистическая регрессия особенно интересна тем, что, несмотря на простоту, она демонстрирует высокую эффективность и полную математическую прозрачность в задачах бинарной классификации. Более того, она легко обобщается на многоклассовый случай (через [softmax](https://en.wikipedia.org/wiki/Softmax_function) — функцию, обобщающую сигмоиду для многоклассового случая) и формирует фундамент для многих других моделей, включая нейросети.\n\n{% cut \"Реализация в Python\" %}\n\nРеализуем логистическую регрессию с помощью библиотеки `scikit-learn`:\n\n```Python\nfrom sklearn.linear_model import LogisticRegression\n\n# Генерация данных для бинарной классификации\nnp.random.seed(1)\nn = 100\nX_class0 = np.random.randn(n // 2, 2) + [1, 1]\nX_class1 = np.random.randn(n // 2, 2) + [4, 4]\nX = np.vstack((X_class0, X_class1))\ny = np.array([0] * (n // 2) + [1] * (n // 2))\n\n# Обучение логистической регрессии\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\n# Выводим предсказания классов и вероятностей для первых 5 точек\npredicted_classes = log_reg.predict(X[:5])\npredicted_probs = log_reg.predict_proba(X[:5])\n```\n\n{% endcut %}\n\nВ линейной и логистической регрессии мы видим одну общую структуру: модель линейна по параметрам, и от её устойчивости и обобщающей способности напрямую зависит качество предсказаний. В следующем разделе мы рассмотрим, как добавление регуляризации позволяет улучшить поведение модели, особенно в условиях переобучения, мультиколлинеарности и плохой обусловленности матрицы $X^T X$.\n\n## Гребневая и лассо-регуляризация как ограничения на нормы весов\n\nРегуляризация — это один из ключевых приёмов для борьбы с переобучением и улучшения устойчивости линейных моделей.\n\nКак мы уже видели, решение задачи линейной регрессии через нормальные уравнения включает в себя обращение матрицы $X^T X$. Однако если признаки скоррелированы или в разных масштабах, эта матрица может быть плохо [обусловленной](https://education.yandex.ru/handbook/math/article/spektralnie-metodi-i-matrichnie-razlozheniia#matrichnye-razlozheniya:~:text=%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D0%B5%20%D1%87%D0%B8%D1%81%D0%BB%D0%B0%20%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8.-,%D0%A7%D0%B8%D1%81%D0%BB%D0%BE%20%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8,-%D0%92%D1%81%D0%B5%20%D1%82%D1%80%D0%B8%20%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F) или даже вырожденной. В таких случаях модель становится чувствительной к шуму в данных и плохо обобщается на новых примерах.\n\nРегуляризация позволяет справиться с этими проблемами, добавляя штраф за избыточную сложность модели. Это означает введение дополнительного слагаемого в функцию потерь, которое зависит от нормы вектора весов $w$. Два наиболее популярных вида регуляризации — гребневая (Ridge) и лассо (Lasso) — отличаются типом нормы, которую они используют.\n\n### Гребневая регрессия (Ridge)\n\nВ гребневой регрессии к исходной функции потерь MSE добавляется штраф на квадрат евклидовой нормы весов ($L_2$-[норма](https://education.yandex.ru/handbook/math/article/geometriia-priznakovogo-prostranstva#:~:text=L%E2%82%82%2D%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%20\\(%D0%95%D0%B2%D0%BA%D0%BB%D0%B8%D0%B4%D0%BE%D0%B2%D0%B0%20%D0%BD%D0%BE%D1%80%D0%BC%D0%B0\\))):\n\n$$\\mathcal{L}_{\\mathrm{ridge}}(w)=\\frac{1}{2}\\|Xw-y\\|^2+\\frac{\\lambda}{2}\\|w\\|^2_2=\\frac{1}{2}(Xw-y)^T(Xw-y)+\\frac{\\lambda}{2}\\|w\\|^2_2 \n$$\n\nЗдесь $\\lambda \\ge 0$ — гиперпараметр регуляризации, который контролирует баланс между точностью на обучающей выборке и «простотой» модели:\n\n- при $\\lambda=0$ мы возвращаемся к обычной линейной регрессии;\n- при больших значениях $\\lambda$ веса $w$ стремятся к нулю.\n\nЭто можно интерпретировать двумя способами.\n**Способ №1** — с точки зрения требований к модели. Мы полагаем, что «хорошая» модель не должна иметь слишком большие значения коэффициентов $w$, и наказываем отклонения от этой гипотезы.\n\n{% cut \"Пример\" %}\n\nДопустим, у нас есть модель с одним признаком $\\hat{y}=wx$. Если $w = 0.5$, то при изменении $x$ на единицу прогноз изменится на $0.5$. Это умеренно. Но если $w = 1000$, то то же самое изменение $x$ приведёт к скачку прогноза на $1000$!\n\nА если $x$ — это, например, рост человека в метрах, и он изменился с $1.70$ до $1.71$, то модель с $w = 1000$ выдаст рост прогноза на $10$, что совершенно неадекватно.\n\nПоэтому регуляризация ограничивает рост весов и заставляет модель вести себя более сдержанно: не реагировать бурно на незначительные флуктуации входа.\n\n{% endcut %}\n\n**Способ №2** — С точки зрения устойчивости: регуляризация стабилизирует обращение $X^T X$, добавляя к нему диагональный сдвиг $\\lambda I$.\n\n***\n\nОптимальное значение параметров в гребневой регрессии можно получить аналитически:\n\n$$w=(X^TX+\\lambda I)^{-1}X^Ty \n$$\n\nТакое решение всегда существует даже тогда, когда $X^T X$ вырождена, так как матрица в скобках положительно определена. При этом, в отличие от обычной линейной регрессии, веса не обязательно обеспечивают точную ортогональную проекцию $y$ на пространство столбцов $X$ — проекция становится сдвинутой в сторону меньших весов.\n\n### Лассо-регрессия (Lasso)\n\nВ лассо-регрессии вместо квадрата нормы применяется сумма модулей весов, то есть $L_1$-[норма](https://education.yandex.ru/handbook/math/article/geometriia-priznakovogo-prostranstva#:~:text=L%E2%82%81%2D%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%20\\(%D0%BC%D0%B0%D0%BD%D1%85%D1%8D%D1%82%D1%82%D0%B5%D0%BD%D1%81%D0%BA%D0%B0%D1%8F%20%D0%BD%D0%BE%D1%80%D0%BC%D0%B0\\)):\n\n\\mathcal{L}_{\\mathrm{lasso}}(w) = \\frac{1}{2} \\|Xw - y\\|^2 + \\lambda \\|w\\|_1 = \\frac{1}{2} \\sum_{i=1}^{m} (x_i^\\top w - y_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j|\n\n\nТакой вид регуляризации поощряет разрежённость весов: при достаточно большом $\\lambda$ многие компоненты $w_j$ становятся точно нулевыми. Это делает лассо особенно полезным, когда мы хотим не просто построить модель, но и отобрать важные признаки.\n\nОднако в отличие от гребневой регрессии, для лассо не существует аналитического решения — задача становится выпуклой, но негладкой (из-за негладкости функции модуля).\n\n### Геометрическая интерпретация\n\nГребневую и лассо-регрессии можно наглядно сравнить с помощью их регуляризационных ограничений.\n\n- В гребневой мы ограничиваем $w$ в $L_2$-норме: множество допустимых решений представляет собой гиперсферу (2D — окружность, в 3D — сфера).\n- В лассо ограничиваем в $L_1$-норме: множество решений — ромбообразное тело (в 2D — ромб, в 3D — октаэдр).\n\nНа рисунке ниже показана геометрическая интерпретация регуляризации.\n\n- Слева (гребневая): ограничение на веса задаётся $L_2$-нормой: $\\|w\\|_2 \\leq C$ (на рисунке для примера $C=1$) — красная окружность.\n- Справа (лассо): ограничение на веса задаётся $L_1$-нормой: $\\|w\\|_1 \\leq C$ (на рисунке $C=2$) — зелёный ромб.\n\nЛинии уровня MSE представляют собой эллипсы, так как MSE — это квадратичная функция. В точке касания с наименьшей линией уровня и границы допустимых весов находится решение.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_11_5_135b8acccb.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрическая интерпретация регуляризации.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрическая интерпретация регуляризации.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ результате оптимум в случае лассо при малых значениях весов чаще оказывается на границе, где некоторые координаты $w_j$ обнуляются — то есть в углах ромба. Поскольку градиенты функции потерь (MSE) направлены к центру, модель выберет в качестве оптимума первый же внешний эллипс, коснувшийся линии ограничения на норму весов.\n\nЧасто именно острые углы ромба, расположенные на осях координат, перехватывают градиенты — и оптимум «прилипает» к ним. Поэтому зануление весов происходит естественным образом.\n\nВ случае гребневой регрессии область ограничений — круг, у которого нет геометрически выделенных направлений: граница гладкая и симметричная. В результате точка касания линии уровня и области допустимых значений почти никогда не совпадают с осями. Все компоненты $w_j$ остаются ненулевыми, но становятся малыми, распределёнными в направлении градиента.\n\u003cbr\u003e\n\n\u003cdiv style=\"border: 1px solid #f0f0f0; padding: 15px; border-radius: 10px; background-color: #f0f0f0;\"\u003e\nТаким образом, гребневая регрессия сглаживает модель, делая веса малыми, но ненулевыми, а лассо приводит к обнулению ненужных весов, производя автоматический отбор признаков.\n\u003c/div\u003e\n\n\u003cbr\u003e\nСуществует также гибридный подход — `Elastic Net`, который комбинирует оба штрафа:\n\n$$\\mathcal{L}_{\\mathrm{elastic}}(w)=\\frac{1}{2}\\|Xw-y\\|^2+\\lambda_1\\|w\\|_1+\\frac{\\lambda_2}{2}\\|w\\|^2_2 \n$$\n\nОн наследует свойства обоих методов: устойчивость гребневой регрессии и разрежённость лассо.\n\nВ линейных моделях регуляризация выступает как средство контроля сложности: она не только уменьшает переобучение, но и улучшает интерпретируемость, снижает чувствительность к шуму, делает решение устойчивым даже при мультиколлинеарности признаков.\n\n{% cut \"Реализация в Python\" %}\n\nНиже —\u0026nbsp;пример регуляризации, реализованный с помощью библиотеки `scikit-learn`:\n\n```Python\nimport numpy as np\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\n\n# Синтетические данные\nnp.random.seed(42)\nn_samples, n_features = 100, 10\nX = np.random.randn(n_samples, n_features)\ntrue_w = np.array([5, -3, 0, 0, 2, 0, 0, 0, 0, 1])\ny = X @ true_w + np.random.randn(n_samples) * 0.5\n\n# Масштабирование признаков\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Ridge-регрессия\nridge = Ridge(alpha=1.0) # Обозначение модели\nridge.fit(X_scaled, y) # Обучение модели\ny_ridge_pred = ridge.predict(X_scaled) # Предсказание\n\n# Lasso-регрессия\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_scaled, y)\ny_lasso_pred = lasso.predict(X_scaled)\n\n# Elastic Net\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic.fit(X_scaled, y)\ny_elastic_pred  = elastic.predict(X_scaled)\n```\n\n{% endcut %}\n\n***\n\nИтак, мы увидели, что линейные и логистические регрессии дают интерпретируемые решения, а регуляризация контролирует сложность и повышает устойчивость модели.\n\nОднако, когда классы неразделимы линейно или граница сложной формы, возможностей линейной гиперплоскости недостаточно. В следующем параграфе мы перейдём к методу опорных векторов (SVM) — линейному по параметрам, но способному строить максимальный зазор и при помощи ядрового трюка формировать нелинейные разделяющие поверхности в исходном пространстве признаков.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801955.83dede9f4adc1fff82e445cf588cbf6fda6d65d2?iframe=1\" frameborder=\"0\" name=\"ya-form-13801955.83dede9f4adc1fff82e445cf588cbf6fda6d65d2\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6e:T8af6,"])</script><script nonce="">self.__next_f.push([1,"Метод опорных векторов (англ. [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine), SVM) — это один из ключевых алгоритмов классификации, сочетающий строгое геометрическое обоснование с хорошими обобщающими свойствами.\n\nВ отличие от логистической регрессии, которая минимизирует логарифмическую функцию потерь, SVM стремится построить решающую границу с максимальным зазором (margin) между классами. При этом он сохраняет линейную структуру модели, а при необходимости легко обобщается на нелинейные зависимости через так называемый ядровой трюк. \n\nВ этом параграфе:\n\n- Мы начнём с постановки задачи SVM в геометрической форме и разберём, как ширина зазора связана с нормой вектора весов.\n- Затем последовательно перейдём к двойственной формулировке, где всё выражается через скалярные произведения и матрицу Грама, и на этом фоне станет понятен принцип ядрового трюка.\n- В завершении обсудим, как понятие VC-размерности помогает оценить обобщающую способность SVM.\n\nПоехали!\n\n## Постановка задачи классификации и её решение\n\nПусть даны обучающие данные $(x_1,y_1),…,(x_m,y_m)$, где $x_i\\in\\mathbb{R}^n$ — векторы признаков, а $y_i\\in\\{-1;+1\\}$ — бинарные метки классов. SVM ищет разделяющую гиперплоскость вида:\n\n$$x^Tw+b=0 \n$$\n\nгде:\n\n- $w\\in\\mathbb{R}^n$ — вектор нормали к гиперплоскости;\n- $b\\in\\mathbb{R}$ — смещение.\n\nНа графике ниже показана гиперплоскость $x^T w + b = 0$ и её нормаль $\\vec{w}$. Вектор $w$ перпендикулярен разделяющей границе и определяет направление, вдоль которого производится классификация.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_1_b85687138f.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Разделяющая гиперплоскость\"\n  \u003e\n  \u003cfigcaption\u003e\n\nРазделяющая гиперплоскость $x^T w + b = 0$ и её нормаль $\\vec{w}$.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nПредсказанный класс для объекта $x_i$ определяется знаком скалярного выражения:\n\n$$\\hat{y}_i=\\mathrm{sign}(x_i^Tw+b) \n$$\n\nЕсли результат положителен, объект классифицируется как принадлежащий классу $+1$; если отрицателен — классу $−1$.\n\nЦель алгоритма SVM — подобрать такие $w$ и $b$, чтобы предсказания совпадали с истинными метками.\n\nЧтобы модель правильно классифицировала объект $x_i$ с меткой $y_i \\in\\{-1,+1\\}$, необходимо, чтобы знак предсказания совпадал со знаком истинной метки. Это можно выразить одним условием:\n\n$$y_i(x_i^Tw+b)\u003e0 \n$$\n\nЭто условие эквивалентно правильной классификации:\n\n- Если $y_i=+1$, нужно, чтобы $x_i^Tw+b\u003e0$, иначе условие выполнено не будет.\n- Если $y_i=-1$, нужно, чтобы $x_i^Tw+b\u003c0$, иначе условие выполнено не будет.\n\nТаким образом, выражение $y_i(x_i^Tw+b)\u003e0$ означает, что объект $x_i$ классифицирован правильно, а знак ответа модели согласован с истинным классом.\n\n\u003cpre\u003e\nЭто — основа всей геометрии линейного классификатора.\n\u003c/pre\u003e\n\nОднако условие $y_i(x_i^Tw+b)\u003e0$ допускает бесконечно много решений: если $(w,b)$ удовлетворяют неравенствам, то и $(cw,cb)$ при любом $c\u003e0$ тоже.\n\nНиже на графике представлено несколько примеров допустимых гиперплоскостей без нормировки. Все разделяют классы правильно, но проходят по-разному. Чтобы выбрать единственную, необходимо зафиксировать масштаб.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_2_f198f9313f.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Примеры допустимых гиперплоскостей без нормировки\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПримеры допустимых гиперплоскостей без нормировки\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЧтобы устранить эту неоднозначность, вводится нормировка:\n\n$$y_i(x_i^Tw+b)\\geq1 \n$$\n\nТеперь ближайшие к границе объекты (опорные векторы) лежат строго на гиперплоскостях $x^T w + b = \\pm1$, а минимальный зазор между ними будет выражен через $\\|w\\|$.\n\nЭто ограничение:\n\n- По-прежнему требует правильной классификации.\n- Фиксирует масштаб — ближайшие к границе объекты (опорные векторы) должны лежать на уровне $\\pm1$, а не где угодно.\n- Гарантирует, что зазор можно выразить через $\\|w\\|$.\n\nТеперь выведем определение опорных векторов:\n\n\u003e 💡`Опорные векторы` — это объекты, попадающие на границу или внутрь разделяющей полосы (зазора), а также объекты, попадающие не в свой класс. Они определяют положение разделяющей гиперплоскости в методе опорных векторов (SVM).\n\nИменно опорные векторы удерживают гиперплоскость на месте — любые другие объекты не влияют на модель. Это одна из причин, почему SVM часто считается «разреженным» методом: решение зависит лишь от подмножества обучающих точек.\n\nНа графике ниже изображена разделяющая гиперплоскость $x^T w + b = 0$ (чёрная), границы зазора $x^T w + b = \\pm 1$ (пунктир), опорные векторы (обведённые). Они определяют решение задачи и лежат точно на границах зазора. Обратите внимание, что опорными векторами являются объекты, которые попали на границы зазора, внутрь него или попали не в свой класс.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_3_f950f2ddc4.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Пример решения задачи бинарной классификации с помощью SVM\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПример решения задачи бинарной классификации с помощью SVM\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n## Геометрический смысл: расстояние до гиперплоскости\n\nРассмотрим, почему задача SVM сводится к минимизации нормы вектора $w$. Для этого нужно понять, что такое расстояние от точки до гиперплоскости. Пусть гиперплоскость задана уравнением:\n\n$$x^Tw+b=0 \n$$\n\nРасстояние от произвольной точки $x\\in\\mathbb{R}^n$ до этой гиперплоскости выражается формулой:\n\n$$\\mathrm{dist}(x,\\mathcal{H})=\\frac{|x^Tw+b|}{\\|w\\|} \n$$\n\nЭта формула выводится следующим образом. Мы ищем проекцию точки $x$ на гиперплоскость $\\mathcal{H}$ вдоль нормали $w$. Искомая проекция имеет вид:\n\n$$\\mathrm{proj_{\\mathcal{H}}}(x)=x-\\lambda w \n$$\n\nГде $\\lambda$ — такое число, что $\\mathrm{proj_{\\mathcal{H}}}(x)\\in\\mathcal{H}$, то есть:\n\n$$(x-\\lambda w)^Tw+b=0 \n$$\n\nРаскрывая скобки, получаем:\n\n$$x^Tw-\\lambda w^Tw+b=0\\implies\\lambda=\\frac{x^Tw+b}{\\|w\\|^2} \n$$\n\nТеперь мы можем найти проекцию $\\mathrm{proj_{\\mathcal{H}}}(x)$:\n\n$$\\mathrm{proj_{\\mathcal{H}}}(x)=x-\\lambda w=x-\\frac{x^Tw+b}{\\|w\\|^2}w\\implies\\lambda w=\\frac{x^Tw+b}{\\|w\\|^2}w \n$$\n\nЗначит, вектор от точки до гиперплоскости имеет длину:\n\n$$\\|x-\\mathrm{proj_{\\mathcal{H}}}(x)\\|=\\|\\lambda w\\|=\\left\\|\\frac{x^Tw+b}{\\|w\\|^2}w\\right\\|=\\left|\\frac{x^Tw+b}{\\|w\\|^2}\\right|\\cdot\\|w\\|=\\frac{|x^Tw+b|}{\\|w\\|} \n$$\n\nТаким образом, расстояние от точки до гиперплоскости зависит только от значения $x^Tw+b$, нормализованного по длине вектора $w$.\n\nПосле нормировки $y_i(x_i^Tw+b)\\geq1$ ближайшие точки лежат на гиперплоскостях:\n\n- $x^Tw+b=1$\n- $x^Tw+b=-1$\n\nРасстояние между этими двумя параллельными гиперплоскостями:\n\n$$\\frac{|1-(-1)|}{\\|w\\|}=\\frac{2}{\\|w\\|} \n$$\n\nТо есть геометрический зазор между классами в модели SVM равен $\\frac{2}{\\|w\\|}$, и задача обучения SVM становится задачей максимизации зазора, что эквивалентно минимизации $\\|w\\|$.\n\nНиже — график, иллюстрирующий зазор и его размер. Он измеряется как расстояние между параллельными гиперплоскостями $x^T w + b = \\pm1$ вдоль нормали $\\vec{w}$ и равен $\\frac{2}{\\|w\\|}$. Это расстояние определяет ширину разделяющей полосы в методе опорных векторов (SVM). Стрелка нормали указывает направление роста скалярного выражения $x^T w + b$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_4_0644e3feac.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Геометрический зазор\"\n  \u003e\n  \u003cfigcaption\u003e\n\nГеометрический зазор как расстояние между $x^T w + b = \\pm1$ вдоль нормали $\\vec{w}$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nС учётом нормировки задача SVM записывается как:\n\n$$\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\quad\\text{при условии}\\quad y_i(x_i^Tw+b)\\geq1 \n$$\n\nЕсли данные не линейно разделимы, то в задачу вводятся переменные-допуски $\\xi_i\u003e0$:\n\n$$\\min_{w,b}\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^m\\xi_i\\quad\\text{при условии}\\quad y_i(x_i^Tw+b)\\geq1-\\xi_i \n$$\n\nГиперпараметр $C\u003e0$ управляет штрафом за ошибки классификации: чем больше $C$, тем менее терпима модель к нарушениям и тем более узким получается зазор. И наоборот, чем меньше $C$, тем шире зазор и тем меньше SVM фокусируется на штрафах.\n\nНа графиках ниже наглядно показано влияние влияние гиперпараметра $C$ на ширину зазора в SVM. Ширина зазора при этом определяет набор опорных векторов. А они, в свою очередь, определяют положение и наклон разделяющей гиперплоскости.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_5_5d1b15464b.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Влияние гиперпараметра\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВлияние гиперпараметра $C$ на ширину зазора в SVM\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНиже мы подробно разберём, как метод опорных векторов (SVM) формулируется в так называемой `двойственной форме` — через лагранжиан, скалярные произведения и матрицу Грама. Это необходимо для понимания `ядрового трюка`, который позволяет обучать нелинейные модели, оставаясь в рамках линейной оптимизации.\n\n## Двойственная задача и матрица Грама\n\nПрежде чем начнём, важное замечание.\n\n\u003e 💡Если вы только начинаете знакомство с SVM, не обязательно вникать во все математические детали: вы всё равно сможете пользоваться SVM и понимать его основные идеи.\n\nСуть метода остаётся той же: мы ищем максимально разделяющую гиперплоскость между классами. Но если вы готовы немного углубиться в теорию оптимизации, этот раздел даст вам более глубокое понимание устройства метода.\n\nЧтобы решать эту задачу эффективно, используют так называемый метод Лагранжа. Суть метода заключается в том, чтобы объединить целевую функцию и ограничения в одну вспомогательную функцию — лагранжиан — и затем искать её стационарные точки.\n\nВводятся множители Лагранжа $\\alpha_i \\geq 0$ для каждого ограничения. Лагранжиан записывается как:\n\n$$\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^m\\alpha_i\\left[y_i(x_i^Tw+b)-1\\right] \n$$\n\nЧтобы перейти к двойственной задаче, мы хотим выразить исходную оптимизацию только через переменные $\\alpha_i$, исключив $w$ и $b$. Для этого мы рассматриваем седловую точку лагранжиана: она должна быть минимумом по $w$ и $b$ при фиксированных $\\alpha$ и максимумом по $\\alpha$ при фиксированных $w, b$:\n\n$$\\min_{w,b}\\max_{\\alpha\\geq0}\\mathcal{L}(w,b,\\alpha) \n$$\n\nМинимум по $(w, b)$ означает, что мы ищем точку, в которой градиент $\\mathcal{L}$ по этим переменным обращается в нуль. Рассмотрим выражение $\\mathcal{L}$ подробнее. Раскроем скобки:\n\n$$\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}w^Tw-\\sum_{i=1}^m\\alpha_i y_i x^T_iw-b\\sum_{i=1}^m\\alpha_i y_i +\\sum_{i=1}^m\\alpha_i \n$$\n\nТеперь вычислим градиент по $w$:\n\n$$\\frac{\\partial\\mathcal{L}}{\\partial w}=w-\\sum_{i=1}^m\\alpha_i y_i x_i \n$$\n\nПриравниваем производную к нулю (так как ищем минимум по $w$):\n\n$$w=\\sum_{i=1}^m\\alpha_i y_i x_i \n$$\n\nАналогично по $b$:\n\n$$\\frac{\\partial\\mathcal{L}}{\\partial w}=-\\sum_{i=1}^m\\alpha_i y_i\\implies\\sum_{i=1}^m\\alpha_i y_i=0 \n$$\n\nИменно эти два уравнения позволяют исключить $w$ и $b$ и перейти к двойственной задаче, которая зависит только от $\\alpha_i$. Мы нашли выражения для $w$ и $b$, при которых $\\mathcal{L}$ достигает минимума. Подставим их обратно в $\\mathcal{L}$, чтобы получить оптимизационную задачу только по $\\alpha$, — это и есть двойственная задача.\n\n$$\\frac{1}{2}\\|w\\|^2=\\frac{1}{2}\\left\\|\\sum_{i=1}^m\\alpha_i y_i x_i\\right\\|^2=\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx^T_ix_j \n$$\n\nДалее рассмотрим линейную часть лагранжиана. Раскрыв скобки, подставим $w$ и учтём, что $\\sum_{i=1}^m\\alpha_i y_i=0$:\n\n$$\\sum_{i=1}^m\\alpha_i\\left[y_i(x_i^Tw+b)-1\\right]=\\sum_{i=1}^m\\alpha_i y_i x^T_iw+b\\sum_{i=1}^m\\alpha_i y_i -\\sum_{i=1}^m\\alpha_i= \n$$\n\n$$\\sum_{i=1}^m\\alpha_i y_i x^T_i\\cdot\\left(\\sum_{j=1}^m\\alpha_j y_j x_j\\right)+0-\\sum_{i=1}^m\\alpha_i=\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx^T_ix_j-\\sum_{i=1}^m\\alpha_i \n$$\n\nТеперь запишем итог, не забывая что линейная часть в лагранжиане была с минусом:\n\n$$\\max_{\\alpha}\\mathcal{L}=\\max_{\\alpha}\\left[\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx^T_ix_j-\\left(\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx^T_ix_j-\\sum_{i=1}^m\\alpha_i\\right)\\right]= \n$$\n\n$$\\max_{\\alpha}\\left[\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx^T_ix_j\\right],\\quad\\text{при }\\alpha_i \\geq 0, \\sum_{i=1}^m \\alpha_i y_i = 0 \n$$\n\nЗдесь возникает ключевое наблюдение: вся задача выражена только через скалярные произведения $x_i^T x_j$. Это приводит нас к понятию матрицы Грама:\n\n$$K\\in\\mathbb{R}^{m\\times m},\\quad K_{ij}=x^T_ix_j \n$$\n\nЕсли все объекты записаны в матрицу $X\\in\\mathbb{R}^{m\\times n}$, то $K=XX^T$. Эта симметричная положительно полуопределённая матрица описывает геометрию выборки: углы, длины и взаимные расположения точек.\n\nПредсказание новой точки $x$ тоже выражается только через скалярные произведения:\n\n$$\\hat{y}(x)=\\mathrm{sign}\\left(\\sum_{i=1}^m\\alpha_i y_i x^T_ix_j+b\\right) \n$$\n\nБольшинство $\\alpha_i$ обнуляется, и только несколько объектов $x_i$, у которых $\\alpha_i\u003e0$, влияют на решение — они и являются опорными векторами.\n\nРассмотрим следующий рисунок, который иллюстрирует ключевые компоненты двойственной задачи SVM — матрицу Грама и геометрию обучающих объектов в пространстве признаков. Советуем открыть его в отдельной вкладке, чтобы он был перед глазами: мы будем разбирать его до конца раздела.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_6_bc36d571f6.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Двойственная задача SVM\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДвойственная задача SVM: матрица Грама $K = XX^T$ (слева) и геометрия обучающих объектов с гиперплоскостью и зазором (справа). Опорные векторы ($\\alpha_i \u003e 0$) отмечены на обеих панелях\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nСлева представлена матрица Грама $K = XX^T$, а справа — расположение точек $x_i$ в пространстве $\\mathbb{R}^2$, разделяющая гиперплоскость и границы зазора.\n\nКаждая ячейка $K_{ij}$ матрицы содержит скалярное произведение между парами объектов:\n\n$$K_{ij}=xx^T \n$$\n\nТаким образом, эта матрица отражает взаимную геометрию объектов: направление и относительную длину их векторов. Положительные значения (оттенки красного) соответствуют близким по направлению векторам, отрицательные (синие) — противоположно направленным. Симметричность $K$ отражает симметричность скалярного произведения.\n\nНад отдельными столбцами подписано $\\alpha \u003e 0$ — это означает, что соответствующий объект $x_i$ является опорным вектором. Только такие объекты получают ненулевое значение $\\alpha_i$ в решении двойственной задачи и участвуют в предсказании новой точки $x$:\n\n$$ \\hat{y}(x)=\\mathrm{sign}\\left(\\sum_{i\\in SV}\\alpha_i y_i x^T_ix_j+b\\right)\n$$\n\nГде $i\\in SV$ означает, что учитываются только опорные векторы SV (Support Vectors).\n\nТаким образом, левая часть рисунка показывает, что вся двойственная задача выражается через скалярные произведения и только несколько строк и столбцов (опорные векторы) действительно участвуют в модели.\n\nСопоставляя обе части рисунка, мы можем сделать вывод:\n\n- Какие именно объекты оказываются опорными векторами (в обеих частях рисунка они выделены).\n- Как они взаимодействуют с другими через матрицу скалярных произведений.\n- Почему в двойственной формулировке SVM мы можем отказаться от явного $w$ и $b$ и оперировать только $\\alpha$, $y$, и $K$.\n\nТаким образом, рисунок подчёркивает центральную идею двойственной оптимизации: всё обучение сводится к манипуляции скалярными произведениями между входными объектами, и только несколько ключевых точек действительно влияют на результат.\n\nБлагодаря тому, что вся модель зависит только от $x_i^T x_j$, мы можем заменить это выражение на произвольное ядро $K(x_i, x_j)$ и тем самым перейти к нелинейным признаковым пространствам, не покидая двойственной формы. Матрица Грама тогда становится ядровой матрицей, а алгоритм — ядровым SVM.\n\n## Ядровой трюк: нелинейные разделяющие границы\n\nЕсли данные неразделимы линейно, их можно отобразить в другое пространство — возможно, гораздо большей размерности — с помощью отображения $\\phi(x)$. Тогда все скалярные произведения заменяются на $\\phi(x_i)^T \\phi(x_j)$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_7_367a4d20a6.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация ядрового трюка\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация ядрового трюка: в исходном пространстве классы не разделимы линейно (слева), но после отображения $\\phi(x)$ становится возможна линейная разделяющая гиперплоскость (справа)\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНо если существует функция $K(x, x') = \\phi(x)^T \\phi(x')$, то можно вычислять это произведение напрямую, без знания $\\phi$. Это и есть ядровой трюк.\n\n$$K(x,x')=\\langle\\phi(x),\\phi(x')\\rangle \n$$\n\nГде $\\phi(x)$ — отображение признаков в более богатое пространство. При этом явно вычислять $\\phi$ не нужно: достаточно знать значения $K(x,x')$.\n\nТипичные ядра:\n\n- Полиномиальное $K(x,x')=(x^Tx'+c)^d$.\n- Гауссовское (RBF): $K(x,x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right)$.\n- Сигмоидальное: $K(x,x') = \\tanh\\left(kx^Tx'+\\theta\\right)$.\n\nМодель остаётся линейной в новом пространстве, но может реализовывать нелинейную границу в исходном. Все вычисления идут через матрицу Грама $K$, составленную из ядер.\n\nНа рисунке ниже представлен пример применения SVM для линейно неразделимых данных с использованием SVM с полиномиальным ядром второй степени. Опорные векторы показаны окружностями, пунктир обозначает границы зазора $f(x)=\\pm1$, жирная линия — границу классов $f(x)=0$.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_8_f5333dd29f.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Нелинейная граница\"\n  \u003e\n  \u003cfigcaption\u003e\n\nНелинейная граница, построенная SVM с полиномиальным ядром степени 2\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n## Обобщающая способность и VC-размерность\n\nНаконец, важный вопрос: если можно использовать произвольно сложное отображение, не приведёт ли это к переобучению? Теория обобщения отвечает на это с помощью понятия размерности [Вапника — Червоненкиса](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%92%D0%B0%D0%BF%D0%BD%D0%B8%D0%BA%D0%B0_%E2%80%94_%D0%A7%D0%B5%D1%80%D0%B2%D0%BE%D0%BD%D0%B5%D0%BD%D0%BA%D0%B8%D1%81%D0%B0) (VC-размерности).\n\n\u003e 💡`VC-размерность` — это мера сложности класса бинарных классификаторов, основанная на том, сколько различных размеченных выборок класс может идеально классифицировать.\n\nЛинейный классификатор в $\\mathbb{R}^n$ имеет VC-размерность $n+1.$ Например, любые три точки, не лежащие на одной прямой, в двумерном пространстве могут быть разделены на два класса всеми возможными способами (их всего $2^3=8$). Но не всякие четыре точки в двумерном пространстве могут быть разделены линейным классификатором. Поэтому VC-размерность линейного классификатора в двумерном пространстве $\\mathbb{R}^2$ равна $3$.\n\nВнизу показана визуализация VC-размерности: три точки можно разделить линейной гиперплоскостью при любой разметке классов (слева), а для четырёх точек (справа) не существует гиперплоскости, корректно разделяющей все возможные разметки.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/4_12_9_25d72ff000.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация VC-размерности\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация VC-размерности\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nVC-размерность отражает способность модели подстраиваться под шум. Чем выше VC-размерность, тем более гибкой (и потенциально более склонной к переобучению) является модель.\n\nПереобучение возникает, когда модель слишком хорошо запоминает обучающую выборку, включая шум или нерелевантные детали, и плохо обобщает на новых данных. Например, полиномиальный классификатор высокой степени имеет огромную VC-размерность и может идеально подогнаться под выборку, но при этом не улавливать общие закономерности.\n\nПри использовании ядровых методов мы фактически работаем в пространстве гораздо большей (а иногда и бесконечной) размерности. Это может увеличить VC-размерность. Но SVM остаётся устойчивым к переобучению за счёт максимизации зазора: среди всех границ модель выбирает ту, что наиболее проста геометрически.\n\nТо есть максимизация зазора — это форма регуляризации, аналогичная ограничению нормы весов в линейной регрессии. Она снижает эффективную сложность модели, даже если номинальная VC-размерность высокая из-за ядра. SVM в целом не зависит от всей выборки, а только от ее малой части — опорных векторов, и потому он не склонен подгоняться к каждой точке обучающей выборки.\n\n{% cut \"Реализация в Python\" %}\n\nВот как реализовать метод SVM с помощью библиотеки `scikit-learn`:\n\n```Python\nfrom sklearn.svm import SVC\nimport numpy as np\n\n# Пример обучающих данных\nnp.random.seed(0)\nX_pos = np.random.randn(10, 2) + [2, 0]\nX_neg = np.random.randn(10, 2) + [-2, -2]\n\nX = np.vstack([X_pos, X_neg])\ny = np.array([1]*10 + [-1]*10)\n\n# Обучение линейного SVM\nmodel = SVC(kernel='linear', C=1.0)\nmodel.fit(X, y)\n\nw = model.coef_[0]     # Вектор весов\nb = model.intercept_[0]  # Смещение\nsv = model.support_vectors_ # Опорные векторы\n\nprint(f\"w = {w}, b = {b}\")\n\n# Предсказание для новой точки\nx_new = np.array([[4, 4]])\nprediction = model.predict(x_new)\nprint(f\"Предсказанный класс для {x_new} = {prediction[0]}\")\n```\n\n{% endcut %}\n\n***\n\nМы увидели, что SVM строит разделяющую гиперплоскость с максимальным зазором, — это эквивалентно минимизации нормы весов. В двойственной форме всё сводится к скалярным произведениям и матрице Грама, а ядровой трюк позволяет получать нелинейные границы, оставаясь в линейной оптимизации. На решение влияют лишь опорные векторы, а сложность модели контролируется параметром $C$ и выбором ядра, его параметров.\n\nНа практике качество SVM критически зависит от свойств признаков: масштабов, центра и выбросов. Поскольку и скалярные произведения, и ядровые расстояния (например, в RBF-ядре) чувствительны к масштабам, несбалансированные признаки ведут к ухудшению зазора, нестабильному набору опорных векторов и сложной настройке гиперпараметров.\n\nПоэтому следующий шаг — препроцессинг признаков: центрирование и стандартизация, Min-Max и робастное масштабирование, а также их влияние на обусловленность $X^\\top X$, устойчивость решения и скорость сходимости.\n\nВ следующем параграфе мы разберём эти приёмы и покажем, как правильно встраивать их в конвейер обучения, чтобы избежать утечек и получить предсказуемый выигрыш в качестве.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801961.443d754db9c5b82702de310e3f78ddc18dcae7ce?iframe=1\" frameborder=\"0\" name=\"ya-form-13801961.443d754db9c5b82702de310e3f78ddc18dcae7ce\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"6f:T66e1,"])</script><script nonce="">self.__next_f.push([1,"Мы уже рассмотрели мощные линейные модели, но чтобы они работали эффективно и предсказуемо, нужно подготовить для них топливо — качественные данные.\n\nПрепроцессинг признаков — это не просто технический шаг, а фундаментальный этап, который напрямую влияет на устойчивость модели, скорость её обучения и качество итоговых предсказаний.\n\nВ этом параграфе мы разберём, почему масштабирование так важно для алгоритмов, основанных на геометрии и градиентах. Мы изучим ключевые техники:\n\n- Центрирование\n- Стандартизацию\n- Масштабирование в заданный диапазон\n- Робастное скалирование, устойчивое к выбросам\n\nА главное — увидим, как эти методы улучшают обусловленность матриц и ускоряют сходимость моделей на практике.\n\n## Центрирование\n\nЦентрирование означает приведение каждого признака к нулевому среднему значению. Для матрицы $X \\in \\mathbb{R}^{m \\times n}$ это означает вычитание из каждого столбца его среднего значения:\n\n$$\\bar{X}_{ij} = X_{ij} - \\bar{x}_j, \\quad \\text{где } \\bar{x}_j = \\frac{1}{m} \\sum_{i=1}^{m} X_{ij}\n$$\n\nЦентрирование не изменяет форму данных, но смещает их «центр масс» в начало координат. Это упрощает геометрию задачи и устраняет ненужное смещение. Особенно важно это при наличии свободного члена $b$ в модели линейной регрессии:\n\n$$y=Xw+b \n$$\n\nЕсли признаки не центрированы, то $b$ поглощает как истинное смещение, так и вклад средних значений признаков, отличных от нуля ($\\bar{x}^\\top w$). После центрирования $b$ становится чистым сдвигом по целевой переменной, и его можно даже убрать из модели, если одновременно центрировать и $y$.\n\nКроме того, центрирование упрощает многие аналитические выводы: после него ковариационная матрица становится просто $X^TX/m$, что позволяет точнее анализировать линейную зависимость признаков, их вклад в регрессию и проводить спектральный анализ (например, PCA).\n\n{% cut \"Реализация в Python\" %}\n\nЦентрирование можно выполнить вручную или с помощью функции `StandardScaler` из библиотеки `sklearn`, установив `with_std=False`:\n\n```Python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Пример DataFrame\nX = pd.DataFrame({\n    'height': [160, 170, 180],\n    'weight': [55, 65, 75]\n})\n\n# Центрирование (без масштабирования)\nscaler = StandardScaler(with_mean=True, with_std=False)\nX_centered = scaler.fit_transform(X)\n\nprint(pd.DataFrame(X_centered, columns=X.columns))\n```\n\n{% endcut %}\n\n## Стандартизация\n\nЦентрирование убирает смещение, но не устраняет проблему несопоставимости признаков. Если один столбец измеряется в миллиметрах, а другой в тоннах, то веса модели будут неравномерно штрафоваться регуляризатором и градиенты по признакам будут иметь разный масштаб. Это приводит к нестабильному обучению, медленной сходимости и искажённой интерпретации значений $w$.\n\nРешением является стандартизация, то есть приведение каждого признака к нулевому среднему и единичной дисперсии:\n\n$$\\hat{X}_{ij} = \\frac{X_{ij} - \\bar{x}_j}{s_j}, \\quad \\text{где } s_j = \\sqrt{\\frac{1}{m-1} \\sum_{i=1}^{m} (X_{ij} - \\bar{x}_j)^2}\n$$\n\nРезультирующие признаки становятся безразмерными, и все имеют одинаковое статистическое влияние. Это критически важно при использовании регуляризации. Рассмотрим, например, гребневую регрессию:\n\n$$\\min_w\\|Xw-y\\|^2+\\lambda\\|w\\|^2 \n$$\n\nЕсли признаки имеют разные масштабы, то одинаковый штраф на $w_j$ будет по-разному воздействовать на каждый признак. После стандартизации все компоненты становятся равноправными, и выбор $\\lambda$ приобретает смысл.\n\nВ контексте градиентного спуска стандартизация приводит к тому, что уровни функции потерь становятся ближе к симметричной «параболической чаше», а не вытянутым эллипсам. Это резко ускоряет сходимость и упрощает выбор гиперпараметров обучения.\n\n{% cut \"Реализация в Python\" %}\n\nВот классическая нормализация: вычитание среднего и деление на стандартное отклонение. Реализуется той же функцией `StandardScaler`, но с параметрами по умолчанию\n\n```Python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(pd.DataFrame(X_scaled, columns=X.columns))\n```\n\nЕсли используется `torch`, то для статической нормализации признаков (например, при передаче табличных данных в нейросеть) можно использовать `torch.nn.functional.batch_norm`:\n\n```Python\nimport torch\nimport torch.nn.functional as F\n\nX_tensor = torch.tensor(X.values, dtype=torch.float32)\nmean = X_tensor.mean(dim=0)\nstd = X_tensor.std(dim=0, unbiased=False)\n\nX_norm = F.batch_norm(X_tensor, running_mean=mean, running_var=std**2, training=False)\n\nprint(X_norm)\n```\n\n{% endcut %}\n\nВ рамках нейросетей `BatchNorm` (Batch Normalization) используется не как статический препроцессинг, а как обучаемый слой, встроенный внутрь архитектуры. Его основная задача — стабилизировать и ускорить обучение за счёт нормализации активаций внутри сети. Это достигается за счёт вычитания среднего и деления на стандартное отклонение по мини-батчу:\n\n$$x’=\\frac{x-\\mu_{\\mathrm{batch}}}{\\sigma_{\\mathrm{batch}}+\\varepsilon} \n$$\n\nгде $\\mu_{\\text{batch}}, \\sigma_{\\text{batch}}$ — среднее и стандартное отклонение, рассчитанные в текущем батче, а $\\varepsilon$ — маленькая постоянная для численной устойчивости. После нормализации применяется обучаемый аффинный слой:\n\n$$y=\\gamma x’+\\beta \n$$\n\nГде $\\gamma$ и $\\beta$ — параметры, обучающиеся градиентным спуском. Это позволяет сети при необходимости вернуть нужный масштаб и смещение.\n\n{% cut \"Реализация в Python\" %}\n\nПри `training=True` слой использует статистики текущего батча. При `training=False`, он использует усреднённые оценки, накопленные в ходе обучения (так называемые `running_mean` и `running_var`), что обеспечивает стабильность на этапе инференса.\n\nВ нашем случае, когда мы просто хотим нормализовать входные признаки вручную, без обучения, мы явно передаём `running_mean` и `running_var` и устанавливаем `training=False`. Это полностью эквивалентно классической стандартизации признаков, как в функции `StandardScaler` из `sklearn`, но на уровне `torch.Tensor`:\n\n```Python\nimport torch.nn.functional as F\n\nX_norm = F.batch_norm(X_tensor, running_mean=mean, running_var=std**2, training=False)\n```\n\nТаким образом, мы используем `BatchNorm` не как обучаемый слой, а как удобный способ реализовать стандартизацию на тензорах в `PyTorch` без необходимости писать ручное деление.\n\n{% endcut %}\n\n## Масштабирование\n\nЦентрирование и стандартизация стремятся устранить смещение и выровнять дисперсии признаков. Однако они не контролируют диапазон значений. В некоторых задачах важно, чтобы признаки строго укладывались в заданные интервалы, например от $0$ до $1$ или от $-1$ до $1$. Это особенно актуально в тех случаях, когда:\n\n- Модель чувствительна к абсолютным границам (например, нейросети с сигмоидами или тангенс-гиперболическими активациями).\n- Признаки входят в формулы расстояний (например, $L_1$ или $L_2$-метрики в $k$-ближайших соседях, KMeans).\n- Важны относительные масштабы между признаками (например, в визуализации или интерпретации значений).\n- Данные представлены разреженными кодами (например, one-hot), и нужно совместить их с плотными числовыми признаками.\n- Модель не устойчива к выбросам, но требуется мягкая нормализация без удаления точек (например, при подаче на автоэнкодер или на регуляризованную регрессию).\n\nВ таких ситуациях применяется масштабирование признаков — то есть линейное преобразование, которое сжимает значения каждого признака в определённый диапазон. Наиболее популярный вариант — приведение к интервалу $[0, 1]$:\n\n$$x’=\\frac{x-x_{\\min}}{x_{\\max}-x_{\\min}} \n$$\n\nЭтот подход называется Min-Max-нормализацией. Если необходимо изменить диапазон на $[a, b]$, можно использовать обобщённую формулу:\n\n$$x’=a+\\left(\\frac{x-x_{\\min}}{x_{\\max}-x_{\\min}}\\right)(b-a) \n$$\n\nПреимущество такого преобразования — простота и интерпретируемость: каждый нормализованный признак сохраняет относительный порядок значений и находится в строгих границах. Это предотвращает «взрыв» градиентов в нейросетях и делает входные данные пригодными для визуального анализа.\n\nОднако у Min-Max-нормализации есть и слабые стороны:\n\n- Она чувствительна к выбросам: одна аномальная точка может сжать всю шкалу признака.\n- Она не центрирует данные: среднее значение признака после нормализации зависит от формы распределения.\n- Она не приводит дисперсии к единице и не устраняет корреляцию между признаками.\n\nТаким образом, Min-Max-нормализация — это отдельный тип препроцессинга, отличный от центрирования и стандартизации. Её следует использовать, когда модель требует контроля над диапазоном признаков, а не их распределением.\n\n{% cut \"Реализация в Python\" %}\n\nДля масштабирования признаков в Python используется класс `MinMaxScaler` из библиотеки `sklearn.preprocessing`. Вот пример, где каждый признак преобразуется в интервал $[0, 1]$:\n\n```Python\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n# Пример исходных данных\nX = pd.DataFrame({\n    'feature_1': [12, 24, 36],\n    'feature_2': [100, 150, 300]\n})\n\n# Применяем масштабирование\nscaler = MinMaxScaler()  # По умолчанию диапазон [0, 1]\nX_scaled = scaler.fit_transform(X)\n\n# Оборачиваем обратно в DataFrame для наглядности\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\nprint(X_scaled_df)\n```\n\nЕсли требуется изменить диапазон, например, на $[-1, 1]$, это делается так:\n\n```Python\nscaler = MinMaxScaler(feature_range=(-1, 1))\n```\n\nРезультат сохраняет порядок и пропорции между значениями, но гарантирует, что минимум и максимум каждого признака будут строго заданы.\n\n{% endcut %}\n\n## Робастное скалирование\n\nОднако бывают случаи, когда данные содержат выбросы — редкие, но экстремальные значения. Если такой выброс попадает в вычисление среднего или стандартного отклонения, то вся процедура стандартизации теряет смысл: масштаб искажается, и модель плохо обобщается.\n\nВ таких ситуациях применяется робастная стандартизация — масштабирование на основе медианы и межквартильного размаха:\n\n$$X_{ij}=\\frac{X_{ij}-\\mathrm{median}(X_j)}{\\mathrm{IQR}(X_j)},\\quad\\text{где }\\mathrm{IQR}=Q_3-Q_1, \n$$\n\nгде $Q_1$ и $Q_3$ — первый и третий квартили соответственно.\nТакая нормировка нечувствительна к единичным выбросам, поскольку медиана и квартили являются робастными статистиками. Робастное скалирование особенно полезно при анализе распределений с тяжёлыми хвостами, в задаче регрессии с шумными источниками данных или в предварительном отборе признаков перед более строгими моделями (например, лассо или SVM).\n\n{% cut \"Реализация в Python\" %}\n\nРобастное скалирование используется при наличии выбросов. Вместо среднего и стандартного отклонения используется медиана и IQR (межквартильный размах):\n\n```Python\nfrom sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nX_robust = scaler.fit_transform(X)\n\nprint(pd.DataFrame(X_robust, columns=X.columns))\n```\n\n{% endcut %}\n\n***\n\nЧтобы вам было проще увидеть разницу между методами препроцессинга, мы собрали информацию о них в одну табличку:\n\n#|\n||\n\n**Метод**\n\n|\n\n**Формула**\n\n|\n\n**Цель применения**\n\n||\n||\n\nЦентрирование\n\n|\n\n$$\\bar{X}_{ij} = X_{ij} - \\bar{x}_j,\n$$\n\n$$\\bar{x}_j = \\frac{1}{m} \\sum\\limits_{i=1}^{m} X_{ij}.\n$$\n\n|\n\nУстранение смещения, упрощение структуры матриц $X^\\top X$, интерпретация $b$ в линейных моделях\n\n||\n||\n\n**Стандартизация**\n\n**(`StandardScaler`)**\n\n|\n\n$$\\hat{X}_{ij} = \\frac{X_{ij} - \\bar{x}_j}{s_j},\n$$\n\n$$s_j = \\sqrt{\\frac{1}{m-1} \\sum\\limits_{i=1}^{m} (X_{ij} - \\bar{x}_j)^2}.\n$$\n\n|\n\nОбеспечение единичной дисперсии, улучшение сходимости, регуляризация\n\n||\n||\n\n**Масштабирование**\n\n**(`MinMaxScaler`)**\n\n|\n\n$$x' = a + \\left( \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\right) (b - a)\n$$\n\n|\n\nКонтроль над границами значений, визуализация, совместимость с сигмоидой\n\n||\n||\n\n**Робастное скалирование**\n\n**(`RobustScaler`)**\n\n|\n\n$$X_{ij} = \\frac{X_{ij} - \\mathrm{median}(X_j)}{\\mathrm{IQR}(X_j)},\n$$\n\n$$\\mathrm{IQR} = Q_3 - Q_1.\n$$\n\n|\n\nЗащита от выбросов, устойчивое масштабирование при тяжёлых хвостах\n\n||\n|#\n\n## Обусловленность матрицы $X^{T}X$ и её численная стабильность\n\nОдна из центральных проблем, которую решает препроцессинг, — это обусловленность матрицы $X^{T}X$. Напомним, что обусловленность матрицы $X$ определяется как отношение её максимального и минимального сингулярных значений:\n\n$$k(X)=\\frac{\\sigma_{\\max}}{\\sigma_{\\min}} \n$$\n\nВ контексте квадратичного выражения $X^TX$ часто используют обусловленность уже этой матрицы, которая выражается как отношение наибольшего и наименьшего собственных значений матрицы $X^TX$:\n\n$$k(X^TX)=\\left(\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\\right)^2=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \n$$\n\nТаким образом, плохо обусловленная матрица $X$ приводит к ещё худшей обусловленности $X^TX$, что делает задачу регрессии неустойчивой и чувствительной к малейшим возмущениям в данных.\n\nПлохо обусловленная матрица возникает, когда признаки скоррелированы (линейно зависимы) или имеют несопоставимые масштабы. Это приводит к огромной чувствительности решения к шуму, численным ошибкам при вычислении $w = (X^TX)^{-1}X^Ty$ и даже к невозможности инверсии.\n\nПрепроцессинг, особенно стандартизация, позволяет сбалансировать признаки, устранить мультиколлинеарность и привести $X^TX$ к более устойчивой, хорошо обусловленной форме. В этом случае сингулярные числа матрицы становятся ближе по величине и инверсия не приводит к взрывному росту ошибки.\n\nФактически все задачи, в которых фигурирует $X^TX$, включая гребневую регрессию, PCA, SVM и даже стохастический градиентный спуск (SGD), выигрывают от хорошо подготовленной матрицы признаков.\n\n## Скорость и устойчивость обучения\n\nНаконец, качественный препроцессинг напрямую влияет на скорость сходимости итеративных методов оптимизации. Градиентный спуск работает эффективнее, когда функция потерь имеет симметричную, изотропную форму. Без нормализации градиенты будут двигаться вдоль узких вытянутых канавок (градиентные «долины»), что приводит к зигзагообразной траектории и медленному прогрессу.\n\nПосле стандартизации признаки становятся эквивалентными по шкале, а градиент быстрее указывает в направлении минимума. Это особенно критично в задачах с большим числом признаков и высокой корреляцией между ними.\n\nПримером может служить логистическая регрессия с градиентным спуском. Без нормализации её обучение может занять в разы больше эпох или вовсе застрять на плато. Аналогично при использовании стохастических оптимизаторов (SGD, Adam) качество препроцессинга влияет на величину шага обучения и необходимость в адаптивных техниках.\n\nТаким образом, этап препроцессинга нельзя рассматривать как необязательный или второстепенный. Он формирует основу всей дальнейшей модели: влияет на математическую корректность, численную стабильность, интерпретируемость, скорость обучения и качество предсказаний. Без него линейные модели теряют свою мощь как простые, надёжные и воспроизводимые инструменты машинного обучения.\n\n***\n\nКлючевые выводы о препроцессинге просты:\n\n- выбирайте преобразование, исходя из задачи и распределений (стандартизация по умолчанию, Min-Max — когда важен диапазон, робастные методы — при выбросах);\n- обучение любых скейлеров делайте только на обучающей части, применяя затем к валидации/тесту (исключаем утечки);\n- объединяйте препроцессинг и модель в единый пайплайн, чтобы кросс-валидация и подбор гиперпараметров учитывали весь конвейер;\n- улучшенная обусловленность и согласованные масштабы делают решения устойчивее, регуляризацию — осмысленнее, а оптимизацию — быстрее.\n\nВ этом параграфе мы убедились, что предобработка признаков — это не второстепенный, а обязательный этап построения качественных моделей. Мы рассмотрели ключевые техники: центрирование, стандартизацию, масштабирование и робастное скалирование — и поняли, как они напрямую влияют на численную устойчивость моделей, улучшая обусловленность матриц, и на скорость сходимости градиентных методов.\n\nСледующий параграф — заключительный. В нём подведём итоги всей главы.\n\n\u003cbr\u003e\n\u003c/br\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13801964.fcd2b3f3eeca61bde483777e263474ba2d5a710a?iframe=1\" frameborder=\"0\" name=\"ya-form-13801964.fcd2b3f3eeca61bde483777e263474ba2d5a710a\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"70:T132f,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы собрали воедино линейно-алгебраические инструменты, которые чаще всего встречаются в анализе данных и машинном обучении, и показали их на практических задачах.\n\n- Освежили базу: векторы и матрицы, операции (сложение, умножение, транспонирование), свойства следа и их роль в регуляризации и упрощении выражений.\n- Научились решать системы линейных уравнений: метод Гаусса, LU-разложение (и узнали случаи, когда им пользоваться); обсудили смысл определителя.\n- Разобрали ортогонализацию и QR-разложение (включая Грама — Шмидта) и увидели, как через ортогональные проекции формулируется и решается МНК-регрессия.\n- Ввели нормы и расстояния, обсудили обусловленность и устойчивость вычислений и разобрали, почему масштаб признаков влияет на сходимость алгоритмов.\n- Перешли к спектральным методам: разобрали собственные значения/векторы и диагонализацию; сингулярное разложение (SVD) как основу низкоранговых приближений, сжатия и поиска структуры.\n- Показали, как из SVD получается метод главных компонент (PCA) для снижения размерности. На практике рассмотрели, как по спектру выбирать число компонент и что означает объяснённая дисперсия.\n- Рассмотрели разложения для текстов и скрытых тем: LSA и NMF, их интерпретируемость и ограничения.\n- Разобрали матричную факторизацию в рекомендательных системах: как разреженная матрица рейтингов сворачивается в общие латентные факторы пользователей и объектов\n- Рассмотрели работу SVM, вывели двойственную задачу и обсудили ядровой трюк, позволяющий работать в высокоразмерных пространствах без явного преобразования признаков.\n- Завершили предобработкой признаков: центрирование, стандартизация, робастное масштабирование и их влияние на устойчивость и качество моделей.\n\nТеперь у вас есть целостное представление о том, как методы линейной алгебры применяются в современных алгоритмах анализа данных, и вы умеете использовать эти инструменты для повышения устойчивости, интерпретируемости и эффективности моделей в практических задачах.\n\nОсвоив геометрию признакового пространства, вы научились работать с данными в непрерывных пространствах, представляя их векторами и находя в них геометрические структуры. Однако многие задачи в машинном обучении — от подбора признаков до настройки моделей — сводятся к работе с конечными наборами и выбору из огромного числа вариантов. Чтобы научиться оценивать сложность таких задач и понимать, почему полный перебор часто невозможен, понадобится аппарат комбинаторики.\n\nТак что в следующей главе мы погрузимся в мир множеств, перестановок и сочетаний, чтобы разобраться, как принципы подсчёта лежат в основе настройки моделей и помогают осознать то самое «проклятие размерности»."])</script><script nonce="">self.__next_f.push([1,"71:Tfea,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы погрузимся в основы комбинаторики — ключевого раздела математики для оценки сложности алгоритмов, понимания методов перебора и принципов работы многих моделей машинного обучения.\n\nХотя комбинаторика применяется в анализе данных не так явно, как линейная алгебра или теория вероятностей, её знание помогает глубже понять суть таких явлений, как `комбинаторный взрыв` и `проклятие размерности`. Но обо всём по порядку.\n\nВот о чём пойдет речь далее:\n\n- Множества и операции над ними. Мы начнём с фундамента — множеств, подмножеств и ключевых операций, таких как объединение, пересечение и декартово произведение. Это язык, на котором говорит комбинаторика.\n- Основные правила подсчёта. Разберём правила суммы и произведения — простые, но мощные инструменты для решения широкого круга задач, от оценки количества вариантов пароля до подсчёта комбинаций гиперпараметров в моделях.\n- Классические комбинаторные формулы. Вы познакомитесь с перестановками, размещениями и сочетаниями. Мы разберёмся, в чём разница между ними и как с их помощью вычислять количество способов выбрать или упорядочить объекты с повторениями или без.\n- Применение в машинном обучении. Мы наглядно покажем, как комбинаторные принципы лежат в основе методов подбора гиперпараметров (например, Grid Search), задач отбора признаков и как они объясняют проклятие размерности — одну из центральных проблем в работе с многомерными данными.\n\nПрочитав эту главу, вы сможете:\n\n1. **Применять базовые комбинаторные методы.** Вы научитесь использовать основные формулы и правила для подсчёта числа возможных конфигураций в практических задачах.\n1. **Оценивать сложность задач.** Вы сможете понять, почему полный перебор вариантов часто невозможен, и научитесь оценивать масштабы комбинаторного взрыва.\n1. **Глубже понимать алгоритмы машинного обучения.** Знания из этой главы помогут вам лучше разобраться в принципах работы методов отбора признаков, настройки гиперпараметров и осознать важность симметрий данных для борьбы с проклятием размерности.\n\nМы предполагаем, что вы знакомы с базовыми математическими операциями. Наша цель — не просто изложить формулы, а показать стоящую за ними логику и их связь с реальными задачами анализа данных.\n\nПриступим!\n"])</script><script nonce="">self.__next_f.push([1,"72:Ta1be,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы заложим фундамент для понимания комбинаторики и её роли в анализе данных. Вот что нас ждёт:\n\n- **Множества и операции над ними.** Вспомним, что такое множества (а это основной объект комбинаторики) и какие операции над ними можно выполнять.\n- **Ключевые правила подсчета.** Изучим главные принципы: правило суммы, правило произведения и принцип Дирихле.\n- **Классические комбинаторные формулы.** Разберёмся в перестановках, размещениях и сочетаниях — с повторениями и без.\n- **Биномиальные коэффициенты.** Увидим, как число сочетаний связано с треугольником Паскаля и формулой бинома Ньютона.\n\nМы последовательно изучим эти темы, а в конце, чтобы теория не казалась оторванной от реальности, рассмотрим красивый пример из мира машинного обучения. Но прежде чем погружаться в формулы, давайте ответим на главный вопрос.\n\n## Зачем вообще нужна комбинаторика\n\nПредставьте, вам нужно собрать команду из подходящих соискателей. Или придумать надёжный пароль. Или придумать новый формат хранения данных о пользователях вашего продукта. Во всех подобных задачах встаёт один и тот же вопрос: а сколько разных конфигураций объектов вообще может быть?\n\nИ ответ на этот вопрос, а также на множество других, даёт именно комбинаторика.\n\u003cbr\u003e\n\n\u003c!DOCTYPE html\u003e\n\n\u003chtml lang=\"ru\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eЦитата автора\u003c/title\u003e\n    \u003cstyle\u003e\n        .container {\n            border: 1px solid #f5f5f5;\n            padding: 20px;\n            border-radius: 15px;\n            width: 100%;\n            max-width: 100%;\n            box-sizing: border-box;\n            background-color: #f5f5f5;\n        }\n        .author-info {\n            display: flex;\n            align-items: center;\n            gap: 25px;\n            margin-bottom: 20px;\n        }\n        .author-avatar {\n            width: 75px;\n            height: 75px;\n            border-radius: 50%;\n            object-fit: cover;\n        }\n        .author-name {\n            font-weight: bold;\n            margin: 0;\n        }\n        .author-position {\n            margin: 0;\n            color: #666;\n        }\n        .text-section {\n            margin-top: 15px;\n            line-height: 1.5;\n        }\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"container\"\u003e\n        \u003cdiv class=\"author-info\"\u003e\n            \u003cimg src=\"https://yastatic.net/s3/education-portal/media/zagruzhennoe_0a4430199d.webp\" alt=\"Аватар А. М. Райгородский\" class=\"author-avatar\"\u003e\n            \u003cdiv\u003e\n                \u003cp class=\"author-name\"\u003eА. М. Райгородский\u003c/p\u003e\n                \u003cp class=\"author-position\"\u003eроссийский математик\u003c/p\u003e\n            \u003c/div\u003e\n        \u003c/div\u003e\n        \u003cdiv class=\"text-section\"\u003e\n            \u003cp\u003e\u003ci\u003e\"Комбинаторика — это наука о том, как можно комбинировать различные объекты, как можно их сочетать. И это, с одной стороны, наука о том, как посчитать количество каких-то комбинаций, а с другой стороны — наука о том, как найти какую-нибудь, как говорят, экстремальную комбинацию, то есть комбинацию с какими-то оптимальными свойствами.\"\u003c/i\u003e\u003c/p\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\nВнимательный читатель *заметит*, что техники подсчёта пригодятся далеко за пределами чистой математики: в криптографии, биоинформатике, машинном обучении и много где ещё. Конечно, явно комбинаторика применяется в анализе данных и машинном обучении гораздо реже, чем линейная алгебра или же теория вероятностей.\n\nНо знание основ комбинаторики существенно улучшает понимание самой сути машинного обучения, принципов работы широко используемых алгоритмов и методов: решающих деревьев, ансамблей моделей, нейронных сетей и др.\n\nМы постараемся разобрать все основные понятия и методы, чтобы слова *комбинаторный взрыв* или *power set* не вызывали недоумения, и начнём мы с самого главного — с понятия множества.\n\n## Множества, отношения между множествами и основные понятия\n\n### Множество\n\nОдним из центральных понятий в комбинаторике является `множество`.\n\n\u003e 💡`Множество` $A=\\{a_1,\\dots,a_n\\}$ — это набор уникальных объектов, порядок которых не имеет значения, важно лишь их наличие или отсутствие.\n\nОбъекты, входящие в множества, обычно называют *элементами множества* или *точками множества*. В литературе элементы множества обычно заключают в фигурные скобки.\n\n### Отношения между множествами\n\n**Включение.** Обозначается как $\\subseteq$. Если все элементы множества $B$ принадлежат и множеству $A$, то множество $B$ *включено в множество* $A$, или, что эквивалентно, $A$ *включает в себя* $B$:\n\n$$B \\subseteq A \\;\\;\\; \\text{или} \\;\\;\\; A \\supseteq B.\n$$\n\n{% cut \"Пример\" %}\n\nПредставим, что у нас есть:\n\n- Множество $A$ — все студенты университета.\n- Множество $B$ — все студенты факультета математики.\n\nПоскольку каждый студент факультета математики ($B$) является также студентом университета ($A$), то множество $B$ включено в множество $A$ ($B⊆A$).\n\n{% endcut %}\n\nВажно заметить, что все элементы любого множества входят в него, а значит, множество включено в само себя: $A \\subseteq A$.\n\n**Равенство.** Обозначается как $=$. Если множество $B$ включено в множество $C$, а множество $C$ включено в множество $B$, т. е. они содержат одни и те же элементы, то они равны:\n\n$$B \\subseteq С \\; \\text{и} \\; С \\subseteq B \\Leftrightarrow B = C. \n$$\n\n{% cut \"Пример\" %}\n\nБухгалтерия расположена в кабинете 69. В ней работают три сотрудника. Тогда:\n\n- Множество $B$ (сотрудники бухгалтерии) $=$ $\\{ Татьяна, Лариса, Глеб \\}$\n- Множество $С$ (обитатели кабинета 69) $=$ $\\{Глеб, Лариса, Татьяна\\}$\n\nПроверка условия:\n\n- $B⊆C$ — каждый сотрудник бухгалтерии ($B$) находится в кабинете 69 ($C$).\n- $C⊆B$ — все люди в кабинете 69 ($C$) являются сотрудниками бухгалтерии ($B$).\n\nПоскольку $B⊆C$ и $C⊆B$, значит, эти множества равны, несмотря на разный порядок элементов.\n\n{% endcut %}\n\n\u003e 💡**Примечание**\n\u003e\n\u003e Иногда отдельно выделяют отношения строгого включения множеств:\n\u003e\n\u003e $$B \\subseteq A \\; \\text{и} \\; B \\neq A \\Leftrightarrow B \\subset A, \n\u003e $$\n\u003e \n\u003e т. е. все элементы $B$ принадлежат $A$, но при этом множества не равны. Т. к. строгость включения не всегда обозначается явно, в литературе знак $\\subset$ иногда используется для любых включений.\n\n### Подмножество\n\nВместе с понятием множества необходимо ввести и понятие `подмножества`.\n\n\u003e 💡`Подножеством` множества $A$ является любое множество, все элементы которого входят в $A$.\n\nТаким образом, любое множество, включённое в множество $A$, будет являться его подмножеством, включая само множество $A$!\n\nИногда выделяют понятие собственного подмножества: $B$ является собственным подмножеством $A$, если оно является подмножеством $A$ и эти множества не совпадают, т. е.:\n\n$$B \\subseteq A \\; \\text{и} \\; B \\neq A \\Leftrightarrow B \\subset A, \n$$\n\nчто полностью соответствует определению строгого включения, которое мы привели выше.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_1_3a6ef30d41.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Диаграмма Эйлера — Венна.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДиаграмма Эйлера — Венна для $B \\subset A$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n### Пустое множество\n\nОстается лишь ввести понятие `пустого множества`, которое стоит несколько обособленно. Пустое множество обозначается как $\\emptyset$ и буквально означает множество, в котором не содержится ни одного элемента.\n\nВажно:\n\n\u003e 💡`Пустое множество` $\\emptyset$ — это подмножество любого другого множества.\n\nВнимательно отнеситесь к этому факту, т. к. практика показывает, что он может показаться неочевидным. Однако он ещё не раз нам пригодится, например когда мы будем говорить о *множестве всех подмножеств*.\n\n### Пример отношений между множествами\n\nВ целях иллюстрации введем несколько множеств: $A = \\{1,2,3,4,5\\}$, $B = \\{1, 2, 3\\}$, $C = \\{3, 2, 1\\}$ и $D = \\{q, w, e\\}$.\n\nЗаметим, что множества $B$ и $C$ равны: $B = C$, т. к. порядок элементов не имеет значения. Также оба множества являются собственными подмножествами множества $A$:\n\n$$B \\subset A \\; , \\; C \\subset A . \n$$\n\nМножество $D$ никак не связано с остальными.\n\n### Способы задания множеств\n\nМножества могут задаваться как простым перечислением всех элементов, так и описанием их свойств. До сих пор мы пользовались лишь первым способом, явным образом указывая все элементы.\n\nНапример:\n\n- Множество всех букв русского алфавита: $\\{а, б, в, г, д, е, ё, ж, з, и, й, к, л, м, н, о, п, р, с, т, у, ф, х, ц, ч, ш, щ, ъ, ы, ь, э, ю, я\\}$,\n- Множество $A = \\{собака, кошка, попугай\\}$ — (это просто множество, заданное явным образом, не ищите в нём скрытого смысла).\n\nЧтобы задать множество через описание, необходимо задать свойства всех его элементов явным образом. Обычно таким образом задаются именно подмножества: $C = \\{x\\in X | \\text{какие-то свойства}\\;x\\}$, например:\n\n- Множество всех положительных чисел $\\{x \\in \\mathbb{R}|x \u003e 0\\}$, где $\\mathbb{R}$ — множество действительных чисел.\n- Множество всех чётных чисел: $\\{x \\in \\mathbb{Z}|x\\;\\text{mod}\\;2 = 0\\} = \\{2k|k \\in \\mathbb{Z}\\}$, которое можно задать несколькими способами, оба приведённых определения верны. Здесь $\\mathbb{Z}$ — множество целых чисел.\n- Множество всех строк в латинском алфавите длиной не более $5$, содержащих хотя бы одну букву q (формульную запись подобного множества опустим, т. к. она требует введения дополнительных обозначений и не является обязательной).\n\nЗаданием множеств через описание часто пользуются, и вы почти наверняка уже с ним встречались, например когда говорили об области значений функции:\n\n$$Y = \\{y| y = f(x), x \\in X\\} . \n$$\n\n### Конечные и бесконечные множества\n\nМножества могут включать различное число элементов.\n\n`Конечным` множеством будем называть множество, количество элементов которого конечно. Соответственно, `бесконечным` множеством будем называть то, количество элементов которого бесконечно.\n\n{% cut \"Примечание\" %}\n\nЕсли мы говорим о бесконечных множествах, важно также разделять `счётные` и `несчётные` множества. `Счётным` множеством называется то, все элементы которого можно взаимно однозначно сопоставить с множеством натуральных чисел $\\mathbb{N}$. Если же это невозможно, множество называется `несчётным`.\n\nНапример, множество всех целых чисел $\\mathbb{Z}$ и даже множество рациональных чисел $\\mathbb{Q} = \\{x|x = \\frac{a}{b}, \\;\\;\\; a \\in \\mathbb{Z}, b \\in \\mathbb{N}\\}$ являются счётными. Данный факт доказывается в курсах введения в математический анализ, ознакомиться с ним можно, например, в лемме 4 [здесь](https://homepage.mi-ras.ru/~podolskii/files/lecture6a.pdf#page=5\u0026zoom=150,0,650).\n\n{% endcut %}\n\n### Мощность множества\n\n`Мощностью множества` будем называть число его элементов и обозначать $|A|=n$. В этом параграфе мы будем говорить преимущественно о конечных множествах, соответственно, их мощностью будет число.\n\n💡В разных дисциплинах термин `мощность` иногда заменяют словами `кардинальность` или просто `размер`. Мы будем пользоваться первым вариантом для единообразия.\n\n### Пример множеств разной мощности\n\nПусть $A$ — множество всех учащихся начальной школы, общее число которых равно 97, а $B$ — множество всех первоклассников, которых 32. Тогда $|A|=97$, а $|B|=32$, и согласно определению $B$ является собственным подмножеством $A$, т. е. $B\\subset A$.\n\nЗаметим, что мощность собственного подмножества конечного множества всегда меньше мощности самого множества.\n\nТакже обратим внимание на пустое множество $\\emptyset$. Его мощность, конечно, равна нулю: $|\\emptyset| = 0$, т. к. оно не содержит ни одного элемента.\n\nНаконец-то мы ввели основные понятия, которые повсеместно используются в комбинаторике и получили широкое распространение за её пределами. Теперь мы полностью готовы к обсуждению операций над множествами, которые рассмотрим далее.\n\n## Основные операции над множествами\n\nРассмотрим основные операции над множествами: пересечение, объединение, разность, симметрическую разность (XOR), а также декартово произведение.\n\nА чтобы вам было легче воспринимать теорию — покажем эти операции на примерах из жизни.\n\nОбращаем ваше внимание: множества $A$ и $B$ будут использоваться как абстрактные множества, а множества, связанные с примерами, мы постараемся называть с использованием других латинских букв.\n\nКомпанию нам составит мальчик Петя, который очень хочет найти себе друзей в новой школе.\n\nОбозначим множество всех людей в школе за $S$ (от слова school). Множество одноклассников Пети $C$ является собственным подмножеством всех людей в школе: $C \\subset S$. Также есть множество всех мальчиков $M$ и множество всех девочек $F$. Множество всех учителей обозначим как $T$, а учителей, которые преподают у класса Пети, как $T_C$.\n\nЕщё раз, для удобства:\n\n- $S = \\{\\text{все люди школы}\\}$.\n- $C = \\{\\text{одноклассники Пети}\\}$.\n- $M = \\{\\text{все мальчики школы}\\}$ .\n- $F = \\{\\text{все девочки школы}\\}$.\n- $T = \\{\\text{все учителя школы}\\}$.\n- $T_C = \\{\\text{учителя, преподающие у класса Пети}\\}$, $T_C \\subset T$.\n\n### Пересечение множеств\n\nПусть Петя ищет, с кем же он мог бы поиграть с футбол после уроков. Петя хочет, чтобы это были его одноклассники, чтобы с ними сблизиться. Девочки играть в футбол не хотят. Получается, множество людей, которых стоит позвать играть в футбол, состоит из мальчиков и при этом одноклассников Пети.\n\n`Пересечением множеств` будем называть множество элементов, принадлежащих одновременно и к $A$, и к $B$:\n\n$$A \\bigcap B = \\{x|x \\in A \\; \\text{и} \\; x \\in B\\} \n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_2_7ff37b9cf1.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Диаграмма Эйлера — Венна.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДиаграмма Эйлера — Венна для $A \\bigcap B$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ случае с Петей именно людей из пересечения множеств $C$ и $M$ стоит звать играть в футбол:\n\n$$C \\bigcap M = \\{x|x\\text{ – мальчик} \\; \\text{и} \\; x \\text{ – одноклассник Пети}\\} \n$$\n\nОбратим внимание, что множества мальчиков $M$ и девочек $F$ не содержат общих элементов, как и множества одноклассников $C$ и учителей $T$. Их пересечениями будет пустое множество $\\emptyset$, с которым мы уже сталкивались ранее:\n\n$$M \\bigcap F = C \\bigcap T = \\emptyset. \n$$\n\nВ общем случае, если любые два множества не содержат общих элементов, их пересечением будет пустое множество $\\emptyset$. Также про такие множества говорят, что они не пересекаются.\n\nПересечение пустого множества с любым другим возвращает пустое множество.\n\n### Объединение множеств\n\nТеперь Петя хочет подарить вкусные ягоды с дачи в честь начала учебного года всем одноклассникам $C$ и учителям своего класса $T_C$. Тогда общее множество людей, которым нужно подарить ягоды, — это объединение множеств $C$ и $T_C$.\n\n`Объединением множеств` $A$ и $B$ будем называть множество элементов, принадлежащих хотя бы к одному из них, т. е. к $A$, к $B$ или к обоим множествам сразу:\n\n$$A \\bigcup B = \\{x|x \\in A \\; \\text{или} \\; x \\in B\\}.\n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_3_a8f0ec16fa.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Диаграмма Эйлера — Венна.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДиаграмма Эйлера — Венна для $A \\bigcup B$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ случае с Петей угощать ягодами нужно будет объединение множеств $C$ и $T_C$:\n\n$$C \\bigcup T_C = \\{x|x \\text{ – одноклассник Пети} \\; \\text{или} \\; x \\text{ – учитель класса Пети} \\}.\n$$\n\nОбратим внимание, что объединение всех мальчиков и всех девочек школы включает в себя вообще всех людей школы:\n\n$$M \\bigcup F = S.\n$$\n\nТакже уделим внимание пустому множеству: объединение пустого множества с любым другим возвращает то же самое множество:\n\n$$A \\bigcup \\emptyset = A \\;\\;\\; \\forall A. \n$$\n\nВообще, пустое множество можно воспринимать как нейтральный элемент для операции объединения множеств.\n\nТакже посчитаем количество людей, которых Петя собрался угостить. В конкретно данном примере мы бы получили нужное число, сложив число одноклассников $|C|$ и число учителей $|T_C|$, ведь множества учеников и учителей не пересекаются, но в общем случае это неверно! Если бы множества пересекались, то мы бы учли людей из пересечения дважды. Общая же формула будет иметь вид:\n\n$$|C \\bigcup T_C| = |C| + |T_C| - |C \\bigcap T_C|.\n$$\n\n### Разность множеств\n\nВ этом году ягод выросло много, и маме Пети стало обидно, что Петя не угостил других учителей. Поэтому она решила угостить тех учителей, кого Петя обошёл стороной. Для этого нам нужно исключить из множества всех учителей $T$ тех, кого уже угостил Петя, т. е. учителей его класса $T_C$.\n\n`Разностью множеств` $A$ и $B$ будем называть множество элементов $A$, которые при этом отсутствуют в $B$:\n\n$$A \\setminus B = \\{x|x \\in A \\; \\text{и} \\; x \\notin B\\}.\n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_4_4734c0ca52.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Диаграмма Эйлера — Венна.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДиаграмма Эйлера — Венна для $A \\setminus B$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТ. е. маме Пети нужно зайти к множеству учителей:\n\n$$T \\setminus T_C = \\{x|x \\text{ – учитель} \\; \\text{и} \\; x \\text{ не учит класс Пети} \\}.\n$$\n\nОбратим внимание, что разность множеств некоммутативна, т. е. чувствительна к порядку операций:\n\n$$A \\setminus B \\neq B \\setminus A.\n$$\n\nНапример, в случае с учителями:\n\n$$T_С \\setminus T = \\{x|x \\text{ учит класс Пети} \\; \\text{и} \\; x \\text{ – не учитель} \\} = \\emptyset,\n$$\n\nмножество людей, которые учат класс Пети, но при этом учителями не являются, очевидно, пусто, в то время как учителя других классов присутствуют в школе.\n\nТеперь посчитаем количество учителей, которых хочет угостить мама Пети: это все учителя, кроме тех, кто учит класс Пети. Или же, в виде формулы:\n\n$$|T \\setminus T_C| = |T| - |T \\bigcap T_C|.\n$$\n\nВ терминах же абстрактных множеств мощность разности имеет вид:\n\n$$|A \\setminus B| = |A| - |A \\bigcap B|.\n$$\n\n### Симметрическая разность\n\nК концу четверти Петя подружился с одноклассниками и даже пошел в секцию по футболу. Сборная команда школы как раз победила в важных соревнованиях, и теперь всех игроков хотят поздравить, устроив им сюрприз. Введём еще несколько множеств: $C_{\\text{all}}$ — всех людей, кто ездил на соревнования от школы (тренер, медицинский работник и сама команда), $C_{\\text{team}}$ — непосредственно команда, игравшая в футбол, и $J$ — все школьники, которые ходят в секцию. Для удобства приведём их отдельным списком:\n\n- $C_{\\text{all}} = \\{\\text{все люди, которые ездили на соревнования}\\}.$\n- $C_{\\text{team}} = \\{\\text{игроки команды, победившей на соревнованиях}\\}$.\n- $J = \\{\\text{все школьники, записанные в секцию}\\}.$\n\nЛегко заметить, что $C_{\\text{team}} \\subset C_{\\text{all}}$, т. к. все игроки победившей команды ездили на соревнования, а $J \\bigcap C_{\\text{all}} = C_{\\text{team}}$, ведь из ездивших на соревнования лишь школьники играли в футбол в зачёте.\n\nЧтобы понять, кого именно привлечь к планированию сюрприза, нужно позвать всех школьников из секции и тех, кто ездил на соревнования, но не самих победивших игроков. Или, завершая эту длинную подводку, нам нужно найти симметрическую разность двух множеств.\n\n`Симметрической разностью` множеств $A$ и $B$ будем называть множество элементов, которые принадлежат строго к одному из множеств:\n\n$$A \\triangle B = (A \\setminus B) \\bigcup (B \\setminus A) = (A \\bigcup B) \\setminus (A \\bigcap B).\n$$\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_5_f33137907a.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Диаграмма Эйлера — Венна.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nДиаграмма Эйлера — Венна для $A \\triangle B$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nСимметрическую разность также называют `исключающим «или»`, а в программировании и машинном обучении часто используется англоязычное название XOR.\n\nОбратим внимание, что симметрическая разность также является коммутативной: порядок множеств в данной операции не важен.\n\nВ случае с Петей нас интересует симметрическая разность множества $J$ и $C_\\text{all}$:\n\n$$J \\triangle C_{\\text{all}} = (J \\bigcup C_{\\text{all}}) \\setminus (J \\bigcap C_{\\text{all}}) = (J \\bigcup C_{\\text{all}}) \\setminus C_{\\text{team}}, \n$$\n\nв которую входят тренер, медработник и все школьники из секции, не участвовавшие в соревновании.\n\nЕсли же понадобится посчитать количество людей, которые будут готовить сюрприз, то:\n\n$$|J \\triangle C_{\\text{all}}| = |J| + |C_{\\text{all}}| - 2 |(J \\bigcap C_{\\text{all}})|. \n$$\n\nВ терминах абстрактных множеств мощность симметрической разности имеет вид:\n\n$$|A \\triangle B| = |A| + |B| - 2 |(A \\bigcap B)|. \n$$\n\n### Декартово произведение\n\nОсобняком стоит операция декартова произведения, но из-за её широкой применимости не сказать о ней в этом разделе мы не можем.\n\nОтойдём от школьных приключений Пети и рассмотрим более привычную для машинного обучения ситуацию: необходимо перебрать значения гиперпараметров и найти оптимальные значения для рассматриваемой задачи, например коэффициент и вид регуляризации для линейной модели. Пусть у нас есть два множества:\n\n- $R = \\{L_1, L_2\\}$ — тип регуляризации.\n- $L = \\{0.1, 0.5, 2.0\\}$ — значения коэффицента регуляризации.\n\nДля каждого типа регуляризации нужно перебрать все указанные значения коэффициента регуляризации. Именно здесь нам поможет декартово произведение.\n\n`Декартово произведение` множеств $A$ и $B$ есть множество всех возможных упорядоченных пар, где первый элемент принадлежит множеству $A$, а второй — множеству $B$:\n\n$$A \\times B = \\{(a, b)|a \\in A, \\; b \\in B\\}. \n$$\n\nВ случае с примером выше мы получим шесть различных наборов значений гиперпараметров. Их проще всего представить в виде таблицы:\n\n#|\n||\n\n**Reg type\\\\Reg coefficient**\n\n|\n\n$0.1$\n\n|\n\n$0.5$\n\n|\n\n$2.0$\n\n||\n||\n\n$L_1$\n\n|\n\n$(L_1, 0.1)$\n\n|\n\n$(L_1, 0.5)$\n\n|\n\n$(L_1, 2.0)$\n\n||\n||\n\n$L_2$\n\n|\n\n$(L_2, 0.1)$\n\n|\n\n$(L_2, 0.5)$\n\n|\n\n$(L_2, 2.0)$\n\n||\n|#\n\nВажно заметить, что декартово произведение по формальным критериям не является коммутативной операцией, т. к. пары в итоговом множестве упорядочены. На практике, например, если вам нужно перебрать все возможные комбинации гиперпараметров и вы знаете, на каком месте стоит какой из них, порядок роли не играет.\n\n### Сводная таблица операций над множествами\n\nДля удобства приведём все рассмотренные операции в единой таблице:\n\n#|\n||\n\n**Операция**\n\n|\n\n**Формула**\n\n|\n\n**Коммутативна**\n\n|\n\n**Мощность результата**\n\n||\n||\n\nПересечение\n\n|\n\n$A \\cap B$\n\n|\n\nДа\n\n|\n\n$\\|A\\| + \\|B\\| - \\|A \\cup B\\|$\n\n||\n||\n\nОбъединение\n\n|\n\n$A \\cup B$\n\n|\n\nДа\n\n|\n\n$\\|A\\| + \\|B\\| - \\|A \\cap B\\|$\n\n||\n||\n\nРазность\n\n|\n\n$A \\setminus B$\n\n|\n\nНет\n\n|\n\n$\\|A\\| - \\|A \\cap B\\|$\n\n||\n||\n\nСимметрическая разность (XOR)\n\n|\n\n$A \\triangle B$\n\n|\n\nДа\n\n|\n\n$\\|A\\| + \\|B\\| - \\|2(A \\cap B)\\|$\n\n||\n||\n\nДекартово произведение\n\n|\n\n$A \\times B$\n\n|\n\nНет\n\n|\n\n$\\|A\\| \\cdot \\|B\\|$\n\n||\n|#\n\nОперации над несколькими множествами\n\nДо сих пор мы работали лишь с парами множеств и именно для них вводили все основные операции. Однако на практике приходится работать сразу с несколькими. Операции объединения и пересечения ведут себя ожидаемым образом:\n\n$$A \\bigcup B \\bigcup C = \\{x|x \\in A \\; \\text{или} \\; x \\in B \\; \\text{или} \\; x \\in C\\},\n$$\n\n$$A \\bigcap B \\bigcap C = \\{x|x \\in A \\; \\text{и} \\; x \\in B \\; \\text{и} \\; x \\in C\\} \n$$\n\nРазность и симметрическая разность используются именно для пар, так что обобщать их на несколько множеств не будем.\n\nДекартово же произведение может быть применено для троек, четвёрок и т. д. множеств:\n\n$$A \\times B \\times C= \\{(a, b, c)|a \\in A, \\; b \\in B, \\; c \\in C\\}. \n$$\n\nЗдесь мы можем впервые для данного повествования заметить явление `комбинаторного взрыва`: даже если мощность каждого отдельного множества небольшая — мы всё ещё говорим о конечных множествах, — мощность их декартова произведения может быть очень большой. Например, если у нас 10 гиперпараметров, каждый из которых может принимать всего по 5 значений (множество значений $P_i$), то итоговое количество комбинаций, которые придётся перебрать, составляет:\n\n$$\\prod_{i=1}^{10} |P_i| = \\prod_{i=1}^{10} 5 = 5^{10} = 9\\;765\\;625, \n$$\n\nчто на практике зачастую не представляется возможным.\n\n### Связь с событиями в теории вероятностей\n\nВнимательные читатели могут обратить внимание, что работа с множествами очень похожа на работу с событиями в теории вероятностей, и это неспроста.\n\nСобытия в теории вероятностей — также множества, просто они обладают дополнительными свойствами, которые необходимы, чтобы пользоваться математическим аппаратом теории вероятностей.\n\nНапример, вводится понятие `меры`, для каждого события $A$ есть его отрицание $\\not A$, которое очень похоже на дополнение множества, но мы опустили это определение в данном параграфе. События также могут пересекаться, объединяться и др. Более подробно вы можете прочитать об этом в главе, посвященной [теории вероятностей](https://education.yandex.ru/handbook/math/article/veroiatnostnoe-prostranstvo). Надеемся, вы обнаружите для себя много общего и это поможет вам лучше понять математические основы машинного обучения и анализа данных.\n\n***\n\nИтак, мы зафиксировали язык, на котором разговаривает комбинаторика.\n\nОпределили множества, подмножества и пустое множество, обсудили мощность, а также базовые операции: пересечение, объединение, разность, симметрическую разность и декартово произведение.\n\nПоняли, как эти простые понятия помогают аккуратно описывать и сравнивать наборы объектов. Наконец, увидели связь с событиями в теории вероятностей: там мы работаем с теми же множествами, но дополнительно вводим меру, чтобы говорить о вероятностях.\n\nТеперь, когда мы умеем корректно описывать, что именно считаем, перейдём к тому, как это считать. В следующем параграфе мы формализуем два краеугольных правила — правило суммы (это когда варианты исключают друг друга) и правило произведения (когда выборы независимы), разберем принцип Дирихле, а затем выведем и сопоставим классические формулы для перестановок, размещений и сочетаний — с повторениями и без. Это даст нам компактный и практичный калькулятор для последующих задач.\n\n\u003cbr\u003e\n\u003c/br\u003e\n"])</script><script nonce="">self.__next_f.push([1,"73:Tafe3,"])</script><script nonce="">self.__next_f.push([1,"В предыдущем параграфе мы научились разговаривать на языке множеств.\n\nВ этом — перейдём к подсчётам: соберём базовый набор правил и формул, из которых складывается подавляющее большинство комбинаторных задач.\n\nЧто вас ждёт:\n\n- два фундаментальных принципа: правило суммы и правило произведения;\n- принцип Дирихле («принцип ящиков») и его применение для быстрых оценок;\n- классические конструкции: перестановки, размещения и сочетания — в версиях с повторениями и без них;\n- аккуратные связи с алгеброй: биномиальные коэффициенты, треугольник Паскаля и формула бинома,\n- мини-кейс из машинного обучения: Weight Agnostic Neural Networks (WANN) и идея эволюционного подбора архитектур (NEAT) как наглядная демонстрация комбинаторного выбора структур без настройки весов.\n\nК концу параграфа вы сможете уверенно переводить текстовые условия в корректные схемы подсчёта, выбирать нужную формулу и быстро прикидывать число вариантов там, где полный перебор невозможен или нецелесообразен.\n\nНачнём с правил и принципов.\n\n## Правило суммы\n\nЕсли объекты выбираются из непересекающихся множеств $A$ и $B$, то количество способов выбрать строго один элемент из $A$ или из $B$ равно $|A|+|B|$.\n\nПростыми словами, сначала мы делаем выбор, из какого множества взять элемент, а затем берём сам объект.\n\nПредставим себе ситуацию, в которой занудный сайт требует добавить ещё один символ к паролю, поскольку его длина слишком мала. Разрешены либо цифры (10 штук), либо буквы в нижнем регистре (33 штуки). Тогда всего мы можем выбрать из $10+33=43$ вариантов. Обратим внимание, что, добавляя лишь один символ, мы увеличиваем количество возможных комбинаций в $43$ раза — именно столько различных вариантов продолжить исходный пароль у нас есть.\n\n## Правило произведения\n\nНо что, если нам нужно выбрать не один элемент, а несколько?\n\nНапример, нужно выполнить последовательность из двух действий: сначала выбрать объект из множества $A$ мощностью в $n$ элементов, а затем объект из множества $B$ мощностью $m$. Независимо от того, пересекаются ли множества, общее число способов будет произведением $n \\cdot m$.\n\nОбратим внимание, что правило произведения вторит декартову произведению: любой возможный выбор будет одной из пар из множества $A \\times B$.\n\nЕсли же нужно выбрать сразу из нескольких множеств, то общее количество вариантов есть произведение мощностей множеств. Для примера давайте оценим количество уникальных автомобильных номеров привычного всем формата «буква, цифра, цифра, цифра, буква, буква» для одного региона. Множество букв включает 12 элементов (т. к. они должны читаться и за рубежом), множество цифр — 10. Следовательно, общее количество уникальных номеров составляет\n\n$$12 \\cdot 10 \\cdot 10 \\cdot 10 \\cdot 12 \\cdot 12 = 1\\,728\\,000. \n$$\n\nСнова видим, что с помощью правила произведения легко посчитать мощность декартова произведения множеств.\n\n## Принцип Дирихле\n\nТакже мы не можем не упомянуть один из наиболее известных принципов в комбинаторике — `принцип Дирихле`, его ещё называют `принципом ящиков`.\n\nПринцип Дирихле гласит: если у вас есть больше объектов, чем ячеек, то хотя бы в одной ячейке должно оказаться два объекта. Или, более формально, если $n+1$ объектов распределить по $n$ ячейкам, то хотя бы в одной ячейке окажутся два объекта.\n\nПринцип Дирихле может показаться очевидным, или, как говорят математики, тривиальным, но в сложных задачах он зачастую помогает перейти от нетривиальных рассуждений (например, геометрических) к элементарной арифметике.\n\nДля иллюстрации давайте рассмотрим простую задачку, которая когда-то встречалась на собеседованиях:\n\n\u003e Докажите, что если взять пять случайных точек внутри квадрата $2\\times2$, то найдутся хотя бы две на расстоянии $\\le\\sqrt2$.\n\nЕсли у вас нет опыта решения подобных задач, то она поначалу может завести в тупик. Но здесь нам как раз поможет принцип Дирихле.\n\nДля удобства нарисуем квадрат:\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_5_1_de8f947b28.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Черный квадрат Малевича с белыми линиями.\"\n\u003c/figure\u003e\n\nОбратим внимание, что размеры квадрата $2 \\times 2$, т. е. он разделяется на четыре единичных квадрата. Так что если внутри квадрата лежат пять точек, то хотя бы две из них попадут в один квадрат — это и есть принцип Дирихле.\n\nДалее решение становится тривиальным. Максимальное расстояние между двумя точками в единичном квадрате достигается, если они лежат в противоположных вершинах и расстояние между ними равно $\\sqrt{1 + 1} = \\sqrt{2}$ — длина диагонали единичного квадрата. Следовательно, расстояние между хотя бы одной парой точек не превышает $\\sqrt{2}$.\n\n## Размещения, сочетания и перестановки\n\nТеперь перейдём к понятиям сочетаний, размещений и перестановок. Обычно именно они ассоциируются со словом «комбинаторика» у людей, когда-то её изучавших. И зачастую они вызывают тоску, т. к. все эти верхние и нижние индексы лишь путают.\n\nНо на самом деле эти числа лишь мощности множеств, которые достаточно просто подсчитать, если вы уже познакомились с операциями над множествами, которые мы обсуждали выше.\n\n### Количество перестановок\n\nРанее мы говорили про неупорядоченные множества. Но в некоторых случаях порядок играет роль — например, при выборе победителей олимпиады для участников крайне важно, кто получил золотую медаль, а кто — бронзовую. Так что для начала рассмотрим понятие перестановок.\n\n\u003e 💡`Количеством перестановок` $P_n$ для множества из $n$ элементов называется количество способов упорядочить его элементы. Важно заметить, что все элементы множества считаются уникальными.\n\nОценить количество перестановок для множества мощностью $n$ достаточно просто:\n\n- Первый элемент мы можем выбрать $n$ способами.\n- Второй — $n-1$ способами: нам доступны все элементы, кроме выбранного на первом шаге.\n- Третий — $n-2$ способами.\n- $\\ldots$\n- Последний — единственным способом.\n\nПолучается, общее число перестановок равно:\n\n$$P_n = n \\cdot (n-1) \\cdot (n-2) \\cdot \\ldots \\cdot 2 \\cdot 1 = n! \n$$\n\nЗдесь мы впервые явно знакомимся с понятием факториала. В рамках комбинаторики будем рассматривать факториал лишь для чисел из множества $\\mathbb{N} \\bigcup \\{0\\}$.\n\nПо определению, для натурального числа $k$ факториал есть перемножение всех натуральных чисел от $1$ до $k$ включительно.\n\nФакториал для $0$ равен единице:\n\n$$0! = 1, \n$$\n\n$$1! = 1, \n$$\n\n$$n! = n \\cdot (n-1)! = n \\cdot (n-1) \\cdot (n-2) \\cdot \\ldots \\cdot 2 \\cdot 1, \\;\\;\\; n \u003e 1. \n$$\n\nНапример, в классе всего из $8$ человек возможно $8! = 40\\;320$ вариантов построений на физкультуре!\n\n### Количество размещений\n\nПусть нам необходимо выбрать $k$ элементов из множества $n$ без повторений, порядок которых важен. Все элементы множества уникальны, т. е. они не могут повторяться.\n\n\u003e 💡`Количеством размещений` $A_n^k$ (читается «$A$ из $n$ по $k$») называется количество способов выбрать $k$ элементов из множества мощностью $n$, порядок которых важен.\n\nНапример, $A_{100}^3$ вариантами можно выбрать топ-3 участников олимпиады с учётом их места — золотая, серебряная и бронзовые медали — из 100 человек. Далее мы явно посчитаем количество размещений для этого примера.\n\nДля оценки числа размещений из $n$ по $k$ без повторений можно воспользоваться простой арифметикой: каждый раз выбирая элемент, мы сужаем область выбора, уменьшаем множество, из которого выбираем, на $1$ элемент. Мы можем воспользоваться правилом произведения:\n\n$$A^k_n = n \\cdot (n-1) \\cdot \\dots \\cdot (n-k+1)=\\frac{n!}{(n-k)!}. \n$$\n\nОбычно число размещений записывают именно в виде финальной формулы через факториал.\n\nТак, в примере выше мы хотели оценить количество способов выбрать трёх победителей из 100 участников соревнований:\n\n$$A_{100}^3 = \\frac{100!}{(100-3)!} = 98 \\cdot 99 \\cdot 100 = 970\\,200 . \n$$\n\nЗаметим, что количество перестановок можно обнаружить как частный случай количества размещений: если рассматривать размещения из $n$ элементов для множества мощностью $n$, получится как раз число перестановок:\n\n$$P_n = A^n_n = n! \n$$\n\n### Количество сочетаний\n\nПусть теперь нам необходимо выбрать $k$ элементов из множества $n$ без повторений, порядок которых не важен. Все элементы множества уникальны, т. е. они не могут повторяться.\n\n\u003e 💡`Количеством сочетаний` $C_n^k$ называется количество способов выбрать $k$ элементов из множества мощностью $n$, порядок которых не важен.\n\nОбратите внимание, что различие между количеством размещений и сочетаний заключается лишь в том, важен ли порядок выбираемых элементов. В остальном они совпадают друг с другом.\n\nДля множества мощностью $k$ доступно $k!$ перестановок, а значит, каждое сочетание соответствует $k!$ различным размещениям. Тогда число сочетаний можно вычислить как:\n\n$$C^k_n = \\frac{A^k_n}{k!}=\\frac{n!}{k!(n-k)!}. \n$$\n\nУпрощая, в примере выше мы выбрали топ-3 участников олимпиады. Для этой одной тройки (сочетания участников) есть $3!$ способов распределить медали (разместить участников).\n\nТак, например, пять случайных членов сборной университета по настольному теннису можно выбрать $C_{50}^5$ способами:\n\n$$C^{5}_{50} = \\frac{50!}{5! \\cdot 45!} = \\frac{46 \\cdot 47 \\cdot 48 \\cdot 49 \\cdot 50}{2 \\cdot 3 \\cdot 4 \\cdot 5} = 2\\;118\\;760. \n$$\n\nЕсли внимательно посмотреть на количество сочетаний $C_n^k$, можно заметить, что оно позволяет оценить количество всех возможных подмножеств мощностью $k$ для множества мощностью $n$.\n\nВ качестве упражнения давайте выведем рекуррентную формулу для сочетаний. Пусть нам доступно сочетание длиной $k \\geq 2$ из $n$ возможных элементов. Выберем отдельный элемент из множества, пусть это будет элемент $a$. Тогда все сочетания делятся на два класса:\n\n- Содержащие элемент $a$. Таких сочетаний может быть $C_{n-1}^{k-1}$, т. к. один элемент уже зафиксирован и в исходном множестве осталось $n-1$ элементов.\n- Не содержащие элемент $a$. Таких сочетаний может быть $C_{n-1}^k$, т. к. один из элементов исходного множества не используется.\n\nТ. к. эти два случая покрывают все возможные ситуации (элемент $a$ либо присутствует, либо нет), верна рекуррентная формула для количества сочетаний:\n\n$$C^k_n = C_{n-1}^{k-1} + C_{n-1}^k. \n$$\n\nТакже стоит обратить внимание на симметрию числа сочетаний, а именно:\n\n$$C^k_n = C_{n}^{n-k}, \n$$\n\nведь при выборе $k$ элементов из множества мощностью $n$ не выбранными остаются $n-k$ элементов. Иначе говоря, мы делим множество на две части размерами $k$ и $n-k$. Логично, что количество способов такого разделения одинаковое для левой и правой части. Этот факт также очевиден из самой формулы числа сочетаний: в знаменателе стоят $k!$ и $(n-k)!$, смена порядка которых не приводит к изменениям.\n\n### Сочетания с повторениями\n\nВ некоторых ситуациях один элемент можно выбрать несколько раз. На практике сочетания с повторениями встречаются не так часто, но в качестве иллюстрации может служить метод [бутстрапа](https://en.wikipedia.org/wiki/Bootstrapping_\\(statistics\\)), который вы встретите или уже встречали в статистике.\n\nЕсли коротко, бутстрап позволяет генерировать новые выборки на основе существующих путем выбирания элементов с возвращением. Порядок элементов в бутстрапированной выборке не важен, один и тот же элемент исходной выборки может попасть в бутстрапированную несколько раз.\n\nНапример, для создания бутстрапированной выборки размером $5$ из общей выборки $X = \\{x_1, x_2, \\ldots, x_{10}\\}$ размером $10$ каждый раз мы выбираем случайный объект из исходной выборки и «возвращаем» его на место, т. е. можем выбрать ещё раз. Таким образом, у нас могут получиться следующие бутстрапированные выборки:\n\n- $X_{\\text{bootstrap}}^1 = \\{x_2, x_7, x_4, x_2, x_9\\}$,\n- $X_{\\text{bootstrap}}^2 = \\{x_2, x_5, x_3, x_8, x_1\\}$,\n- и даже $X_{\\text{bootstrap}}^3 = \\{x_5, x_3, x_5, x_5, x_1\\}$.\n\nДля оценки числа сочетаний с повторениями можно воспользоваться следующей формулой:\n\n$$\\overline{C}^k_n = C^{k}_{n+k-1}. \n$$\n\nДоказательство этого факта требует введения дополнительных теорем и опущено в данном повествовании. Если вы хотите с ним ознакомиться, это можно сделать, например, [здесь](https://mk.cs.msu.ru/images/4/46/Dm_lection1.pdf).\n\n### Размещения с повторениями\n\nРазмещения с повторениями обозначаются как $\\overline{A}^k_n$. Предполагается, что при каждом выборе элемент также «возвращается» обратно в исходное множество и может быть выбран снова.\n\nДля разминки докажем, что число размещений с повторениями вычисляется как\n\n$$\\overline{A}^k_n = n^k. \n$$\n\nДля этого рассмотрим упорядоченный набор размером в $k$ элементов, $k\\geq2$ из исходного множества размером $n$. Отдельно обратим внимание на первый элемент и на все оставшиеся:\n\n- Первый элемент мог быть выбран $n$ способами.\n- Оставшиеся элементы в количестве $k-1$ штук представляют собой один из вариантов размещения $k-1$ элементов из множества $n$ элементов, т. е. могут быть выбраны $\\overline{A}^{k-1}_n$ способами согласно определению.\n\nСледовательно,\n\n$$\\overline{A}^k_n = n \\cdot \\overline{A}^{k-1}_n. \n$$\n\nДля случая $k=1$ видим, что один элемент можно выбрать ровно $n$ способами, следовательно, общее число размещений с повторениями по индукции равно:\n\n$$\\overline{A}^k_n = \\prod_{i=1}^k n = n^k, \n$$\n\nЧто и требовалось доказать.\n\nДля иллюстрации данного факта вновь обратимся к паролям. Пусть нам доступен алфавит из $n$ элементов и мы хотим посчитать число уникальных паролей длиной $k$. Символы могут повторяться и в общем случае вообще никак не зависят друг от друга.\n\nА значит, каждый символ в пароле может быть выбран $n$ способами, а для пароля длиной $k$ мы получаем $n^k$ вариантов пароля. С другой стороны, число паролей длиной $k$ в алфавите из $n$ символов — это и есть число размещений с повторениями $\\overline{A}^k_n$.\n\nНапример, если мы говорим только про цифры, то уникальных числовых паролей длиной в $4$ символа всего $10^4 = 10\\;000$, а именно они использовались на мобильных телефонах до недавних пор. Сейчас же по умолчанию используются пароли длиной в шесть символов, и число комбинаций уже $10^6$.\n\nРассмотрим, как меняется сложность пароля в зависимости от его длины и размера алфавита. Например, пароль в алфавите из 26 символов — английский алфавит в нижнем регистре — длиной в 8 символов примерно соответствует по «стойкости» паролю из 6 символов, который использует латиницу нижнего и верхнего регистра, а также цифры.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_6_08e73d15b1.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Количество возможных паролей.\"\n\u003c/figure\u003e\n\nИз графика легко увидеть, что увеличение длины пароля даже на один символ увеличивает количество возможных комбинаций в $n$ раз и, соответственно, повышает безопасность пароля. Правда, люди обычно используют слова или значимые даты, что существенно снижает безопасность, но это уже вопрос психологии и криптографии, а потому его опустим.\n\n### Сводная таблица перестановок, сочетаний и повторений\n\nНаконец, сведём все рассмотренные в данном параграфе сущности в одну таблицу для вашего удобства.\n\n#|\n||\n\n**Обозначение**\n\n|\n\n**Термин**\n\n|\n\n**Чему равно**\n\n|\n\n**Порядок важен**\n\n|\n\n**Возможны повторения**\n\n||\n||\n\n$P_n$\n\n|\n\nПерестановки\n\n|\n\n$n!$\n\n|\n\nда\n\n|\n\nнет\n\n||\n||\n\n$A^k_n$\n\n|\n\nРазмещение из $n$ по $k$ без повторений\n\n|\n\n$\\frac{n!}{(n-k)!}$\n\n|\n\nда\n\n|\n\nнет\n\n||\n||\n\n$C^k_n$\n\n|\n\nСочетание из $n$ по $k$ без повторений\n\n|\n\n$\\frac{n!}{k!(n-k)!}$\n\n|\n\nнет\n\n|\n\nнет\n\n||\n||\n\n$\\overline{C}^k_n$\n\n|\n\nСочетание из $n$ по $k$ с повторениями\n\n|\n\n$C^{k}_{n+k-1}$\n\n|\n\nнет\n\n|\n\nда\n\n||\n||\n\n$\\overline{A}^k_n$\n\n|\n\nРазмещение из $n$ по $k$ с повторениями\n\n|\n\n$n^k$\n\n|\n\nда\n\n|\n\nда\n\n||\n|#\n\nОсобое место среди этих формул занимают сочетания $C_n^k$. Как мы уже увидели, они обладают интересными свойствами. Например: они симметричны и подчиняются рекуррентному соотношению. Оказывается, эти свойства связывают комбинаторику с алгеброй через так называемые `биномиальные коэффициенты`, которые наглядно представляет треугольник Паскаля.\n\nПредставьте, что у вас есть алгоритм для генерации всех возможных путей в игре и на каждом шаге игрок может выбрать одно из двух действий, например «идти налево» или «идти направо» После трёх шагов количество возможных исходов равно $2^3=8$. Но что, если нас интересует, сколько именно путей содержат ровно два «поворота налево»?\n\nЭто задача на биномиальные коэффициенты: число таких путей равно $C_3^2=3$. А если построить треугольник Паскаля, то это число окажется прямо в нужной позиции — и так для любого количества шагов.\n\nПоговорим об этом подробнее!\n\n## Биномиальные коэффициенты и треугольник Паскаля\n\nПочти наверняка вы видели треугольник Паскаля в кабинете математики в школе, и его даже могли упоминать на занятиях. Сам треугольник устроен простым образом: он заполняется сверху вниз, края треугольника всегда состоят из единиц, а каждый элемент в новом ряду есть сумма двух ближайших по горизонтали к нему в предыдущем:\n\n\u003cpre\u003e\n                             1                              \n                          1    1                            \n                        1    2    1                         \n                     1    3    3    1                       \n                   1    4    6    4    1                    \n                1    5   10   10    5    1                  \n              1    6   15   20   15    6    1               \n\u003c/pre\u003e\n\nКак мы уже говорили выше, каждый элемент этого треугольника есть *биномиальный коэффициент* — это $C^k_n$, где $n$ — номер ряда, а $k$ — номер числа в ряде (начиная с $0$):\n\n$$\\begin{array}{ccccccccccccccc} \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 C^0_0 \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 \\\\[6pt] \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 C^0_1 \u0026 \u0026 C^1_1 \u0026 \u0026 \u0026 \u0026 \u0026 \\\\[6pt] \u0026 \u0026 \u0026 \u0026 \u0026 C^0_2 \u0026 \u0026 C^1_2 \u0026 \u0026 C^2_2 \u0026 \u0026 \u0026 \u0026 \\\\[6pt] \u0026 \u0026 \u0026 \u0026 C^0_3 \u0026 \u0026 C^1_3 \u0026 \u0026 C^2_3 \u0026 \u0026 C^3_3 \u0026 \u0026 \u0026 \\\\[6pt] \u0026 \u0026 \u0026 C^0_4 \u0026 \u0026 C^1_4 \u0026 \u0026 C^2_4 \u0026 \u0026 C^3_4 \u0026 \u0026 C^4_4 \u0026 \u0026 \\\\[6pt] \u0026 \u0026 C^0_5 \u0026 \u0026 C^1_5 \u0026 \u0026 C^2_5 \u0026 \u0026 C^3_5 \u0026 \u0026 C^4_5 \u0026 \u0026 C^5_5 \u0026 \\\\[6pt] \u0026 C^0_6 \u0026 \u0026 C^1_6 \u0026 \u0026 C^2_6 \u0026 \u0026 C^3_6 \u0026 \u0026 C^4_6 \u0026 \u0026 C^5_6 \u0026 \u0026 C^6_6 \\\\ \\end{array} \n$$\n\nБиномиальными коэффициентами они называются потому, что именно с такими коэффициентами встречаются слагаемые различных степеней при возведении суммы в степень. Например, для степени 3:\n\n$$(a+b)^3 = 1 \\cdot a^3 + 3\\cdot a^2b + 3 \\cdot ab^2 + 1 \\cdot b^3 \n$$\n\nмы видим, что коэффициенты соответствуют четвертой строке треугольника Паскаля.\n\nВ общем случае для суммы двух элементов в произвольной степени будет верна следующая формула:\n\n$$(a+b)^n = \\sum_{k=0}^n C_n^k a^{n-k}b^k . \n$$\n\nВыше мы уже доказывали симметрию биномиальных коэффициентов (количества сочетаний) и рекуррентное соотношение для них, но в контексте треугольника Паскаля стоит упомянуть их ещё раз:\n\n- Биномиальные коэффициенты симметричны $C^k_n=C^{n-k}_n$, что логично ввиду коммутативности операции сложения: если поменять $a$ и $b$ местами в формуле выше, то коэффициенты при их степенях не изменятся.\n\n- Рекуррентное соотношение появляется логичным образом. Если разложить степень суммы на два множителя:\n\n  $$(a + b)^n = (a + b)^{n-1} \\cdot (a + b) , \n  $$\n  \n  то при раскрытии скобок член $a^{n-k} b^k$ будет складываться из двух слагаемых:\n\n  $$Ma^{n-1-k} b^k \\cdot a + Na^{n-1-(k-1)} b^{k-1} \\cdot b , \n  $$\n  \n  где коэффициенты как раз соответствуют биномиальным коэффициентам меньшей степени:\n\n  $$C^k_n = C^{k-1}{n-1} + C^{k}{n-1}. \n  $$\n  \n  Стоит заметить, что именно рекуррентное соотношение позволяет эффективно вычислять число сочетаний, ведь подсчёт факториалов для больших чисел — нетривиальная задача.\n\n## Иллюстрация из мира машинного обучения\n\nНесмотря на то, что в явном виде комбинаторные задачи — не частые гости в мире машинного обучения, иногда они всё же встречаются. И зачастую это очень красивые задачи, которые показывают, как поиск правильной комбинации элементов может быть важнее, чем точная настройка их параметров.\n\nВозможно, вы уже знакомы с понятием нейронных сетей. Если нет, то вы ещё обязательно познакомитесь с ними. Пока же можно просто сойтись на том, что нейронная сеть — это некоторая сложная математическая формула с множеством настраиваемых параметров — весовых коэффициентов. Любую математическую формулу можно представить в виде графа. Например, для функции $y = \\Big(\\ln(w^\\top x) + b\\Big)^2$ граф будет иметь вид:\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_7_f58a78a810.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"5.3.\"\n\u003c/figure\u003e\n\nгде $w$ и $b$ — как раз настраиваемые весовые коэффициенты, а $x$ — наблюдаемый объект.\n\nКак правило, нейронные сети строятся в виде последовательно расположенных фиксированных блоков, наборов функций, называемых слоями, и настраиваются лишь весовые коэффициенты (или, как их чаще называют, веса).\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_8_d6a7120674.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Нейронная сеть\"\n  \u003e\n  \u003cfigcaption\u003e\n\nНейронная сеть VGG-16, представленная в 2013 году. Каждый слой (за исключением входного Input-слоя) задаёт некоторое отображение из входного представления в некоторое промежуточное, латентное представление\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНо в 2019 году на конференции [NeurIPS](https://weightagnostic.github.io/) была представлена работа [Weight Agnostic Neural Networks](https://weightagnostic.github.io/) (WANNs), в которой авторы решили совсем отказаться от настраиваемых весов. Вместо этого они предложили подбирать граф оптимальной структуры, то есть, по сути, искать математическую формулу, описывающую решение задачи, — практически физический закон! Для упрощения на искомое решение накладывалось ограничение: все связи обладают одним и тем же весовым коэффициентом, который задаётся экспертом извне.\n\nТаким образом, нейронная сеть представляла собой набор операций, которые подбирались комбинаторно. Если существенно упростить описание этой процедуры, то она заключалась в следующем:\n\n- На каждом шаге из модели (графа) получалось множество претендентов — либо за счёт добавления новой операции, либо заменой существующей.\n- Множество претендентов оценивалось на фиксированной выборке, и лишь лучшие из них оставались для следующей итерации, а остальные удалялись\n\nВнимательный читатель заметит, что это описание напоминает эволюционный алгоритм, и будет прав. В работе использовалась именно его модификация, называемая [NEAT](https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf) (англ. NeuroEvolution of Augmenting Topologies). Главный результат этого подхода заключается в том, что итоговые модели обладали на удивление простой структурой, особенно в сравнении с классическими нейросетями для тех же задач. При этом они обеспечивали качественное и, что важнее, устойчивое решение для задач классификации и обучения с подкреплением.\n\nМы не будем здесь углубляться в технические детали алгоритма. Желающие могут обратиться к оригинальной статье, но для её понимания мы рекомендуем сперва ознакомиться с этим хендбуком, а затем с [хендбуком по машинному обучению](https://education.yandex.ru/handbook/ml).\n\nНиже — визуализации WANN в действии на двух модельных задачах.\n\n### Локомоция двуногого робота (Bipedal Walker)\n\nНеобходимо научить двуногого робота ходить, используя информацию с его датчиков: линейную скорость, углы в суставах, наличие контакта с поверхностью, расстояние до препятствия в определённом направлении (lidar) и др.\n\nКак видно на иллюстрации, алгоритм нашёл нейросеть, обладающую очень простой структурой, но при этом успешно управляющую движениями робота.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_9_24364bcab8.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Пример топологии WANN для управления агентом в среде BipedalWalker\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПример топологии WANN для управления агентом в среде BipedalWalker\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа схеме топологии показана найденная архитектура сети, где каждое ребро имеет одинаковый вес, но используются различные функции активации в узлах (`tanh`, `ReLU`, `sin`, `abs`, `inv`, `Gauss`, `step`). Слева расположены входные признаки, описывающие состояние робота и среды — углы и скорости суставов, данные лидара, контакт ног с поверхностью и т. д. Справа — управляющие выходы, отвечающие за сгибание и разгибание бёдер и коленей.\n\nПросто оцените по [видео](https://weightagnostic.github.io/assets/mp4/square_biped.mp4), как такая минималистичная сеть **без настройки параметров** справилась с управлением персонажем.\n\n![gif](https://yastatic.net/s3/education-portal/media/VID_20251128_124233_199_ec81642c25.gif)\n\n### Управление автомобилем в гонках (Car Racing)\n\nВо второй задаче требовалось научить машину проходить гоночную трассу, ориентируясь по данным лидаров, наискорейшим образом. И здесь тоже итоговая архитектура сети оказалась минималистичной, но смогла обеспечить качественное управление.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_2_10_9608e4bfc2.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Архитектура WANN для задачи Car Racing\"\n  \u003e\n  \u003cfigcaption\u003e\n\nАрхитектура WANN для задачи Car Racing\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа рисунке изображена итоговая структура сети. Слева подаются входные признаки, включая данные с лидаров (z₁–z₁₃) и дополнительный bias-сигнал. А справа расположены выходы, управляющие поворотом руля (steer), подачей газа (gas) и тормозами (brakes).\n\nКак и ранее, несмотря на отсутствие настройки индивидуальных весов, полученная модель оказалась способной эффективно управлять машиной на гоночной трассе, ориентируясь только по лидарным данным.\n\nВот демонстрация:\n\n![gif](https://yastatic.net/s3/education-portal/media/VID_20251127_160209_329_eb4305325c.gif)\n\nРабота WANN чудесно иллюстрирует применение комбинаторики (и неградиентных методов оптимизации) в машинном обучении — как их возможности, так и ограничения.\n\nС одной стороны, подбор оптимальной архитектуры, как в WANN и NEAT, может привести к простым и устойчивым решениям, даже без настройки весов. С другой, с ростом сложности архитектуры количество вариантов растёт комбинаторно быстро. И вновь мы сталкиваемся с `комбинаторным взрывом`.\n\nПолучается, без введения дополнительных ограничений, использования эвристик, отсечения наименее перспективных решений получить эффективное решение не получится. Именно поэтому базовое понимание комбинаторики важно, ведь оно позволяет оценить сложность задачи и возможные ограничения, с которыми придётся столкнуться.\n\nКороче говоря, комбинаторика — крайне полезная часть арсенала каждого, кто хочет создавать эффективные алгоритмы и модели.\n\n***\n\nИтак, мы познакомились с основными определениями и понятиями — это фундамент как для комбинаторики и многих других областей:\n\n- правило суммы;\n- правило произведения;\n- принцип Дирихле;\n- шесть классических формул для размещений, сочетаний и перестановок.\n\nЭти понятия пригодятся нам как при работе с графами или статистикой, так и в алгоритмических задачах.\n\nТеперь, вооружившись этим фундаментом, мы готовы решать более сложные задачи.\n\nВ следующем параграфе мы узнаем, как избегать двойного счёта при работе с пересекающимися группами объектов с помощью метода включений-исключений. И поймём, как оценить общее число всевозможных комбинаций — например, признаков в машинном обучении — с помощью множества всех подмножеств."])</script><script nonce="">self.__next_f.push([1,"74:T5d7e,"])</script><script nonce="">self.__next_f.push([1,"Мы уже познакомились с основными понятиями из мира комбинаторики, с правилами суммы и произведения, а также с классическими формулами для размещений и сочетаний.\n\nНаш следующий шаг — научиться аккуратно считать объединения пересекающихся множеств и упорядочивать пространство всех подмножеств. Эти инструменты пригодятся, когда условий и объектов много и легко допустить двойной счёт.\n\nПо дороге увидим, как эти идеи напрямую работают в задачах анализа данных и машинного обучения.\n\nВ этом параграфе вы узнаете:\n\n- Метод включений-исключений: как считать мощность объединений пересекающихся множеств и вывести общую формулу.\n- Множество всех подмножеств: что такое power set, почему его размер равен $2^{|A|}$ и как это связано с отбором признаков.\n\nНо перед тем как мы продолжим, вот вам задачка:\n\nОцените, во сколько раз увеличится число возможных паролей длины 8 из букв латинского алфавита (26 букв) в нижнем регистре и цифр от 0 до 9, если помимо строчных букв могут использоваться и заглавные.\n\n{% cut \"**Ответ (не открывайте сразу; сперва подумайте сами!)**\" %}\n\nОбщее число уникальных символов изначально составляло $26+10 = 36$. Символы могут повторяться в пароле, т. е. общее число возможных паролей составляло $\\overline{A}^8_{36} = 36^8.$\n\nЕсли же мы будем использовать также буквы верхнего регистра, общее число символов станет равным $26\\cdot2+10=62$. Следовательно, уникальных паролей теперь $62^8$, и их количество увеличилось в $(\\frac{62}{36})^8 \\approx 77.3951$ раз.\n\n{% endcut %}\n\n## Метод включений-исключений\n\nДля начала рассмотрим один из центральных методов работы с множествами (или их подмножествами) сложной структуры.\n\nГоворя формально, он позволяет найти мощность объединения конечного числа конечных множеств, которые могут пересекаться между собой. Говоря проще, он позволяет избежать многократного учёта элементов, которые сразу принадлежат нескольким множествам.\n\nНапример, если мы хотим найти мощность объединения множеств $A$ и $B$, то мы не можем просто сложить их мощности, т. к. пересечение $A \\cap B$ будет учтено дважды — его нужно исключить. Т. е. для случая двух множеств\n\n$$|A \\cup B| = |A| + |B| - |A \\cap B| \n$$\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Диаграмма Эйлера — Венна\" src=\"https://yastatic.net/s3/education-portal/media/5_4_1_d5b731ed25.webp\"\u003e\n  \u003cfigcaption\u003e\n  Диаграмма Эйлера — Венна\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНо что делать, если этих множеств больше? Сначала разберемся с тремя множествами на живом примере.\n\n### Пример: три пересекающихся множества\n\nВ ШАД (Школе Анализа Данных Яндекса) читается множество курсов. Для простоты будем считать, что в первом семестре читаются три курса: алгоритмы $A$, машинное обучение $B$ и разработка на Python $C$ — и у нас есть список всех студентов каждого из курсов. Если сложить численность студентов каждого курса, мы наверняка посчитаем кого-то дважды или трижды — люди ходят сразу на несколько курсов. Используя `метод включений-исключений`, мы сможем найти общее число студентов.\n\nСложим мощности $A$, $B$ и $C$, тогда студенты, принадлежащие их пересечениям, будут учтены дважды. Значит, исключим мощности пересечений — тех, кто ходит одновременно на два курса: $|A \\cup B|$, $|A \\cup C|$ и $|B \\cup C|$. Но если студент записан на все три курса, то мы его проигнорировали (три раза учли, три раза исключили). Значит, всех таких студентов нужно учесть явным образом, т. е. добавить мощность множества $A \\cap B \\cap C$. В конечном итоге получается формула:\n\n$$|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cup B| - |A \\cup C| - |B \\cup C| + |A \\cap B \\cap C|.\n$$\n\n{% cut \"Пример на числах\" %}\n\nПусть на трёх курсах у нас такие данные:\n$|A| = 100, |B| = 80, |C| = 60$;\n$|A \\cap B| = 30, |A \\cap C| = 25, |B \\cap C| = 20$;\n$|A \\cap B \\cap C| = 10$.\nТогда по формуле включений-исключений:\n$|A \\cup B \\cup C| = 100 + 80 + 60 - 30 - 25 - 20 + 10 = 175.$\nИтого: на трёх курсах обучаются 175 уникальных студентов.\n\n{% endcut %}\n\n\u003e 💡Примечание: внимательный читатель заметит, что технически найти мощность объединения можно было бы напрямую, объединив все множества и посчитав итоговый размер. В Python, например, это делается буквально одной строчкой: `len(A.union(B).union(C))`.\n\u003e\n\u003e Это верное замечание, но далеко не всегда нам удобно оперировать элементами множеств в явном виде. В одном из примеров далее мы это проиллюстрируем.\n\nЕщё более внимательный читатель заметит некоторую регулярность: мы учитываем мощности самих множеств, вычитаем мощность пересечений пар, добавляем мощность пересечений троек... А что, если множеств не три, а некоторое число $n$? Именно на этот вопрос и отвечает формула включений-исключений в общем виде.\n\n### Формула включений-исключений\n\nДля $n$ конечных множеств $A_1,\\dots,A_n$ верно\n\n$$\\Bigl|A_1\\cup\\dots\\cup A_n\\Bigr| =\\sum_{i}|A_i| -\\sum_{i\u003cj}|A_i\\cap A_j| +\\sum_{i\u003cj\u003ck}|A_i\\cap A_j\\cap A_k| -\\dots+(-1)^{\\,n+1}|A_1\\cap\\dots\\cap A_n|. \n$$\n\nГеометрически данный результат выглядит достаточно очевидным. Ниже в качестве иллюстрации приведена диаграмма Эйлера — Венна для трёх множеств: алфавитов греческого, русского и латинского языков.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"диаграмма Эйлера — Венна для трёх множеств\" src=\"https://yastatic.net/s3/education-portal/media/5_4_2_dd7fafc919.webp\"\u003e\n\u003c/figure\u003e\n\n### Чуть более комбинаторный пример\n\nРассмотрим задачу: сколькими способами можно выбрать число от 1 до 120, которое не делится ни на 2, ни на 3, ни на 5?\n\nОбозначим множества:\n\n$A=\\{x\\in[1; 120]\\}$,\n$A_2=\\{x\\in[1; 120]\\Big|x \\;\\text{mod}\\; 2 = 0\\}$,\n$A_3=\\{x\\in[1; 120]\\Big|x \\;\\text{mod}\\; 3 = 0\\}$,\n$A_5=\\{x\\in[1; 120]\\Big|x \\;\\text{mod}\\; 5 = 0\\}$.\n\nМощности этих множеств легко посчитать — это результат целочисленного деления общего числа элементов на соответственное число:\n\n$$|A_2|=\\lfloor120/2\\rfloor=60, \n$$\n\nгде $\\lfloor a \\rfloor$ обозначает число, округлённое вниз.\n\nАналогичным образом $|A_3|=\\lfloor120/3\\rfloor=40$, $|A_5|=\\lfloor120/5\\rfloor=24$.\n\nТогда мы также легко найдём количество чисел, которые делятся на 2 и на 3: $|A_2\\cap A_3|=\\lfloor120/6\\rfloor=20$.\n\nАналогично для 2 и 5: $|A_2\\cap A_5|=12$, и для 3 и 5: $|A_3\\cap A_5|=8$.\n\nДля всех трёх чисел: $|A_2\\cap A_3\\cap A_5|=\\lfloor120/30\\rfloor=4$.\n\nМы знаем количество чисел не превышающих 120, которые делятся на 2, 3 или 5, обозначим это число за $K$. Т. к. числа могут либо делиться, либо не делиться на заданные, количество чисел, которые на 2, 3 или 5 не делятся, есть $120 - K$.\n\nВ конечном итоге:\n\n$$|A| - |A_2 \\cup A_3 \\cup A_5| =|A| - \\Big(|A_2| + |A_3| + |A_5| - |A_2 \\cap A_3| - |A_2 \\cap A_5| - |A_3 \\cap A_5| + |A_2 \\cap A_3 \\cap A_5|\\Big) =120-(60+40+24-20-12-8+4) =120-(124-40+4) =32. \n$$\n\nИтого: от 1 до 120 всего 32 числа не делятся ни на 2, ни на 3, ни на 5.\n\n**Задание.** Докажите, что в формуле включений-исключений для $n$ множеств ровно $2^{n}-1$ слагаемых.\n\n{% cut \"**Ответ (не открывайте сразу; сперва подумайте сами!)**\" %}\n\nК решению данной задачи можно подойти по-разному. Мы приведём лишь одно из возможных решений.\n\nРассмотрим формулу включений-исключений. Если доступных множеств $n$, то в формуле мы получим:\n\n- $n = C_n^1$ слагаемых от одиночных множеств\n- $C_n^2$ слагаемых от пересечений пар множеств\n- $C_n^3$ слагаемых от пересечений троек множеств\n- $\\ldots$\n- $C_n^n = 1$ слагаемое от пересечения всех множеств\n\nТ. е. общее число слагаемых есть сумма:\n\n$$\\sum_{k=1}^n C_n^k. \n$$\n\nВ предыдущем параграфе мы разбирали треугольник Паскаля. В нём все элементы в каждой строке — это число сочетаний $C_n^k$, где $n$ есть номер строки, а $k$ — номер элемента (но считая с нуля!). Пользуясь свойством биномиальных коэффициентов, а именно:\n\n$$(a+b)^n = \\sum_{k=0}^n C_n^k a^{n-k}b^k. \n$$\n\nОбратим внимание, что суммы практически совпадают. Если положить $a=b=1$, получим:\n\n$$(1+1)^n = 2^n = \\sum_{k=0}^n C_n^k. \n$$\n\nОсталось лишь вычесть единицу ($C_n^0 = 1$), т. к. суммирование идёт не с $0$, а с $1$.\n\n$$\\sum_{k=1}^n C_n^k = 2^n -1. \n$$\n\n{% endcut %}\n\n## Множество всех подмножеств\n\nЗачастую при работе с множествами возникает необходимости оценить количество всех возможных подмножеств. Например, при выборе оптимального набора признаков, что мы рассмотрим в дальнейшем.\n\nВведем формальное определение:\n\nДля конечного множества $A$ `множество всех его подмножеств` обозначается как $\\mathcal{P}(A)$, и его мощность (т. е. количество всех возможных подмножеств) равна $2^{|A|}$. В англоязычной литературе оно часто обозначается как` power set` или `powerset`.\n\nСтоит отметить, что множество всех подмножеств включает и пустое множество $\\emptyset$, и само множество $A$.\n\n### Доказательство\n\nДавайте формально покажем, что множество всех подмножеств состоит ровно из $2^{|A|}$ элементов.\n\nПронумеруем все элементы множества $A$ и сопоставим каждому элементу 1 бит — бинарную величину. Тогда любому подмножеству соответствует одна из строк длиной $|A|$ вида\n\n\u003cpre\u003e\n000111010011...000\n\u003c/pre\u003e\n\nгде $1$ на позиции $i$ соответствует включению $i$-го элемента в множество, а $0$ — невключению.\n\nВсего возможных строк такого вида будет ровно $2^{|A|}$, причём строка из одних нулей соответствует пустому множеству $\\emptyset$, а из одних единиц — множеству $A$.\n\n### Пример из практики — аугментации изображений\n\nКогда обучают модель распознавать картинки, исходных фотографий часто мало. Чтобы не кормить модель одними и теми же кадрами, делают простые преобразования исходных изображений — `аугментации`.\n\nЭто вроде мелких искажений, которые не меняют смысл картинки (кот остаётся котом), но дают модели больше разнообразия: отразили по горизонтали, слегка повернули, чуть обрезали края, поменяли яркость/контраст, добавили немного шума.\n\nПредставьте, что у нас есть 5 независимых аугментаций:\n\n1. Горизонтальный флип (зеркальное отражение кадра относительно вертикальной оси).\n2. Небольшой поворот.\n3. Случайный кроп (обрезка).\n4. Изменение яркости/контраста.\n5. Добавление шума.\n\nЛюбая такая схема аугментаций — это буквально подмножество из этих пяти операций: какие-то включили, какие-то нет.\n\nЗначит, общее число схем — как раз мощность множества всех подмножеств: $2^{5} = 32$.\n\nДля наглядности несколько вариантов из этих $32$:\n\n- $\\varnothing$ - ничего не делать (исходное изображение);\n- $\\{\\text{флип}\\}$;\n- $\\{\\text{флип}, \\text{поворот}\\}$;\n- $\\{\\text{кроп}, \\text{яркость/контраст}\\}$;\n- $\\{\\text{шум}\\}$;\n- $\\{\\text{флип}, \\text{кроп}, \\text{яркость/контраст}, \\text{шум}\\}$;\n- $\\{\\text{флип}, \\text{поворот}, \\text{кроп}, \\text{яркость/контраст}, \\text{шум}\\}$ — все пять.\n\nКаждая строка — это один набор флажков, и ровно такой набор соответствует одному подмножеству исходного множества аугментаций. Отсюда и формула $2^{|A|}$: у каждой из $|A|$ операций два состояния — $включено/выключено$.\n\n\u003e Примечание. В реальных системах порядок и сила аугментаций тоже имеют значение (например, повернуть на 5° или на 10°). Тогда пространство вариантов растет ещё быстрее. Но базовая идея с подсчётом $2^{|A|}$ отлично иллюстрирует именно «включение/выключение» операций как множества подмножеств.\n\nИменно здесь комбинаторика проявляется в ML: когда выбор раскладывается на независимые бинарные решения, число конфигураций равно $2^{N}$. Если у переключателя больше двух режимов, общее число вариантов — произведение мощностей соответствующих множеств. Давайте посмотрим на другой очень значительный пример из машинного обучения.\n\n### Отбор признаков в машинном обучении\n\nВ машинном обучении для получения точного, простого и интерпретируемого решения крайне важно иметь качественное признаковое описание для всех объектов. Но не все признаки одинаково полезны: многие из них могут дублировать друг друга (т. е. признаковое описание может являться избыточным) или же вовсе быть бесполезными. Поэтому часто необходимо оставить лишь наиболее значимые признаки, т. е. возникает задача `отбора признаков`.\n\nПусть нам доступна выборка, где каждый объект описывается 20 признаками. Полный перебор всех комбинаций означает проверку\n\n$$2^{20}=1\\,048\\,576 \n$$\n\nподмножеств — и на каждом подмножестве нужно обучить модель, не забыть про валидационную выборку, а то и провести кросс-валидацию. Очевидно, что даже с современными вычислительными ресурсами эта задача нетривиальна.\n\n\u003e 💡А ведь признаков может быть гораздо больше, в реальных задачах их десятки, сотни или даже сотни тысяч.\n\nБолее того, добавление одного нового признака моментально удваивает количество усилий — теперь каждое изначальное подмножество либо включает его, либо нет. Поэтому на практике, ввиду экспоненциального роста сложности, полный перебор практически никогда не применяется.\n\nК наиболее широко известным методам отбора признаков относятся:\n\n- жадный перебор;\n- методы, которые опираются на использование уже обученной модели (например, [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html));\n- некоторые методы регуляризации, например $L_1$-регуляризация [«отбирает»](https://education.yandex.ru/handbook/ml/article/linear-models) признаки путём зануления их весовых коэффициентов;\n- случайный поиск (особенно если признаков много и потеря некоторых из них не является критичной);\n- байесовская оптимизация и другие методы.\n\nПримечание: ещё раз обратим внимание на интуитивно понятную иллюстрацию количества всех возможных подмножеств: в случае $n$ признаков каждый признак могут либо взять, либо нет, т. е. общее число комбинаций и будет $2^n$.\n\n**Вопрос.** Сколькими способами можно выбрать ровно 5 признаков из 12?\n\n{% cut \"**Ответ (не открывайте сразу; сперва подумайте сами!)**\" %}\n\nДостаточно воспользоваться формулой сочетаний из предыдущей главы: $C_{12}^5$.\n\n{% endcut %}\n\n***\n\nИтак, в этом параграфе мы научились аккуратно считать мощности объединений пересекающихся множеств: от наглядных диаграмм Венна до общей формулы включений-исключений для произвольного числа множеств.\n\nМы увидели, как эта техника помогает отвечать на вопросы о том, сколько объектов не подпадает ни под одно из ограничений, а также познакомились со множеством всех подмножеств и поняли, почему его размер равен $2$ в степени мощности исходного множества. Эти идеи уже прямо работают в практических задачах — например, при оценке пространства вариантов в отборе признаков.\n\nДальше перенесём этот инструментарий в машинное обучение: посмотрим, как считать и сравнивать размеры пространств гиперпараметров, чем отличаются сеточный и случайный поиск, зачем нужна байесовская оптимизация и как всё это связано с проклятием размерности."])</script><script nonce="">self.__next_f.push([1,"75:T9403,"])</script><script nonce="">self.__next_f.push([1,"Комбинаторные приёмы достаточно изящны. Иногда они позволяют найти неочевидное, но очень простое решение для математической задачи. Но гораздо чаще комбинаторика позволяет оценить сложность задачи и потенциального решения и выбрать более подходящий или эффективный вариант.\n\nВ этом параграфе мы поговорим о гиперпараметрах:\n\n- что это такое, чем они отличаются от параметров и почему их подбор дороже;\n- какие существуют методы подбора гиперпараметров;\n- что такое проклятие размерности и комбинаторный взрыв.\n\nК концу параграфа вы сможете прикидывать стоимость перебора, осознанно выбирать стратегию поиска и понимать, когда нужно сужать пространство решений или добавлять структурные предположения.\n\nПриступим!\n\n## Гиперпараметры: что это и чем они отличаются от параметров\n\nВ машинном обучении можно разделить модели на *параметрические* и *непараметрические*.\n\nПримеры параметрических моделей — линейная регрессия, метод опорных векторов, решающее дерево, нейронная сеть. То есть все модели, где настраиваются некоторые параметры (веса, коэффициенты) для лучшего соответствия данным.\n\nПримером непараметрической модели может служить метод ближайших соседей (kNN) — в нём не происходит автоматической настройки чего-либо на основе обучающей выборки.\n\nПомимо параметров, у моделей также выделяют *гиперпараметры*. Гиперпараметры отвечают за более общие свойства моделей: метрика в kNN, вид регуляризации в линейной регрессии, максимальное число листьев в дереве — это всё гиперпараметры.\n\n\u003e 💡Основное отличие гиперпараметров от параметров заключается в том, что перед обучением модели (нахождением оптимальных значений параметров) необходимо зафиксировать гиперпараметры.\n\nГиперпараметры могут быть категориальными (например метрика в kNN), дискретными (число соседей в kNN) или непрерывными (коэффициент регуляризации). Как правило, гиперпараметры выбираются экспертом вручную или же с помощью неградиентных методов оптимизации.\n\n{% cut \"Примечание\" %}\n\nВнимательный читатель заметит, что в моделях машинного обучения есть целая иерархия настраиваемых величин.\n\nНапример, в ансамблях деревьев нужно выбрать размер ансамбля, а каждое дерево, в свою очередь, обладает собственными гиперпараметрами (например, глубина). Подобные гиперпараметры «более высокого порядка» иногда называют «метапараметрами», но данный термин не является общепринятым и, вообще говоря, не имеет смысла: количество уровней гиперпараметров формально не ограничено сверху (хотя на практике редко встречается больше трёх-четырех).\n\nВ некотором смысле даже сам класс используемых моделей можно считать гиперпараметром — например, перед настройкой параметров линейной регрессии нужно принять решение использовать именно линейные модели, а не деревья решений.\n\n{% endcut %}\n\nЗдесь важно отметить, что если мы не выберем гиперпараметры, то, как правило, не сможем настроить параметры — то есть обучить модель. Поэтому проверка каждого набора гиперпараметров требует полного или частичного обучения модели — то есть она крайне затратна для сложных моделей.\n\n## Методы подбора гиперпараметров\n\nСуществует множество методов подбора гиперпараметров — в этой части мы разберём только основные. Как правило, в сложных задачах используются библиотеки (например, `optuna`, которую мы рассмотрим дальше) — они объединяют в себе сразу несколько подходов.\n\nДалее мы будем преимущественно рассматривать численные гиперпараметры, т. к. если гиперпараметр категориальный, то его значения не упорядочены и никак не связаны друг с другом, а значит, других вариантов, кроме его прямого перебора, нет.\n\nПоговорим о следующих методах:\n\n- Перебор по сетке (англ. Grid Search).\n- Случайный поиск (англ. Random Search).\n- Байесовская оптимизация.\n\nА ещё посмотрим, что под капотом у библиотеки `optuna`.\n\n### Перебор по сетке\n\nПусть для модели доступно $m$ гиперпараметров, для $i$-го гиперпараметра задан диапазон допустимых значений $[\\min_i; \\max_i]$. В данном диапазоне выбирается $k_i$ различных значений гиперпараметра. Тогда общее число возможных комбинаций гиперпараметров равно:\n\n$$N_{\\text{grid}}  =  \\prod_{i=1}^{m}k_i. \n$$\n\nНесложно заметить, что количество различных наборов гиперпараметров оценивается как число различных комбинаций элементов из $m$ множеств с мощностями $k_1, k_2, \\ldots, k_m$.\n\nПоиском по сетке такой подход называется потому, что рассматриваемые значения гиперпараметров образуют сеть: каждый узел этой сети соответствует строго одному набору значений гиперпараметров.\n\nНа левой иллюстрации мы видим два гиперпараметра, для каждого из которых перебирается по сетке три значения, что суммарно даёт девять различных вариантов, для каждого из которых оценивается качество модели.\n\nНа правой иллюстрации также перебирается девять различных комбинаций гиперпараметров, но их значения выбираются случайно в допустимых диапазонах — это случайный поиск.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_4_3_c1024c5b61.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"5.4.3.\"\n\u003c/figure\u003e\n\nНапример, мы решаем задачу регрессии и нужно выбрать вид регуляризации ($L_1$, $L_2$) если нужно подобрать значения коэффициента регуляризации $\\lambda$:\n\n$$w^, b^ = \\arg_{w, b}\\min ||w^\\top x + b - y||_2^2 + \\lambda ||w||_2^2.\n$$\n\nПусть мы хотим проверить $5$ различных значений $\\lambda$. Тогда всего нужно будет попробовать $2 \\cdot 5 = 10$ вариаций\n\nШаг сетки не обязательно должен быть одинаковым. Сетка может быть:\n\n- Регулярной. Тогда все отсчёты расположены через равные промежутки. Пусть заданный интервал $[0; 8]$, тогда шаг будет равен $2$ и покроет варианты $\\{0, 2, 4, 6, 8\\}.$\n- Логарифмической. Тогда значение гиперпараметра представляется в виде $a^l$, где $a$ — некоторое основание, а меняется именно показатель степени. Так, в примере выше выгодно воспользоваться $a=10$, а $l$ варьировать от $-2$ до $2$, тем самым среди вариантов будут $\\{0.01,\\;0.1,\\;1,\\;10,\\;100\\}$. Такой подход более предпочтителен, если гиперпараметр существенно влияет, например, на оптимизируемый функционал (как в случае с коэффициентом регуляризации), — он позволяет более точно приблизиться к нужному значению за несколько итераций.\n- Случайной. О ней расскажем чуть позже.\n\nВажно отметить, что при использовании перебора по сетке не обязательно получать результат сразу. Полезно действовать в несколько шагов, особенно если некоторые гиперпараметры требуют больше внимания, чем другие.\n\nНапример, в случае работы с коэффициентом регуляризации можно сначала обнаружить, что оптимальные результаты достигаются при значении коэффициента регуляризации, равном $0.1$, а затем сформировать новую сетку уже вокруг этого значения.\n\n**Задача для самопроверки.** Сколько попыток обучения модели необходимо, чтобы перебрать три гиперпараметра, где первый принимает 7 значений, второй — 12, а третий — 4?\n\n{% cut \"**Ответ (не открывайте сразу; сперва подумайте сами!)**\" %}\n\n$7 \\cdot 12 \\cdot 4 = 336$.\n\n{% endcut %}\n\n### Случайный поиск\n\nВместо перебора по сетке можно случайным образом выбирать значение гиперпараметра из допустимого промежутка (или множества).\n\nДанный подход работает одинаковым образом как с дискретными, так и с непрерывными величинами, и является рабочей альтернативой перебору по сетке. Работа «Случайный поиск для оптимизации гиперпараметров» (англ. [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)) за авторством Джеймса Бергстры и Йошуа Бенджио (это один из «отцов» современного ИИ, в 2018 году он получил премию Тьюринга) показывает, что случайный поиск позволяет быстрее — за меньшее число итераций — найти оптимальный регион значений гиперпараметров.\n\n### Байесовская оптимизация\n\nЕсли развить идею случайного поиска, получится подход, получивший название байесовской оптимизации (англ. bayesian optimization). Полное понимание байесовской оптимизации требует глубоких знаний в области теории вероятностей и байесовского подхода, поэтому ограничимся кратким обзором.\n\nПусть гиперпараметры $\\boldsymbol{\\theta}$ влияют на значение оптимизируемого функционала $q$, а значит, существует условное распределение $p_{\\text{true}}(q|\\boldsymbol{\\theta})$, которое тем не менее нам не известно. Тогда при каждой попытке обучения модели с какими-то конкретными гиперпараметрами $\\boldsymbol{\\theta}i$ \\*\\*мы узнаем, какое именно значение $q_i$ принял оптимизируемый функционал. На основе этой информации мы строим свою аппроксимацию распределения $p_{\\text{approx}}(q|\\boldsymbol{\\theta})$ (называемого апостериорным), которое мы уточняем на каждом шаге и из которого семплируем значения гиперпараметров.\n\nАпостериорное распределение, как правило, задаётся параметрически, и оптимизируются именно его параметры (да, в машинном обучении оптимизация параметров распределения гиперпараметров модели — обычный вторник).\n\nВ конечном итоге каждый шаг подбора гиперпараметров дороже с вычислительной точки зрения, но суммарное число попыток уменьшается в разы.\n\nЕсли коротко подытожить, то в поиске по сетке и случайном поиске мы никак не используем информацию об уже «проверенных» значениях гиперпараметров, а в байесовской оптимизации мы опираемся на результаты уже проведённых экспериментов и рассматриваем ту часть пространства гиперпараметров, в которой вероятность получить лучший результат наивысшая.\n\n### Под капотом библиотеки optuna — комбинаторика и умный поиск\n\nИдеи байесовского выбора новых точек хорошо работают на практике, но современные инструменты идут дальше.\n\nБиблиотека [`optuna`](https://optuna.org/) сочетает вероятностный отбор со структурированием комбинаторного пространства и распределением бюджета экспериментов, что позволяет резко сократить число запусков без потери качества.\n\nПо сути, происходят две вещи: пространство поиска делают более «умным», а слабые варианты отсекают как можно раньше. Давайте посмотрим, как именно этого добиваются.\n\n**Сначала про структуру.** Простейший способ оценить пространство поиска — перемножить число вариантов для каждого гиперпараметра, как мы делали в переборе по сетке выше. Но в реальных задачах всё сложнее: многие гиперпараметры условны и включаются только при определённом выборе других.\n\nЭто кардинально меняет математику подсчёта. Вместо одного большого декартова произведения мы получаем несколько независимых «ветвей» поиска. Общее число комбинаций тогда считается не по правилу произведения, а по правилу суммы — мы складываем размеры пространств для каждой ветки.\n\nБолее формально вместо $N_{\\text{grid}}$ мы получаем\n\n$$ N_{\\text{cond}}=\\sum_{b=1}^{B}\\prod_{j\\in \\mathrm{params}(b)} k^{(b)}_{j},$$\n\nгде $b$ — ветка (набор согласованных выборов), $\\mathrm{params}(b)$ — активные на ней параметры.\n\nРассмотрим довольно классический пример с SVM. Пусть у нас есть параметры:\n\n- Ядро `kernel` $\\in \\{\\text{linear}, \\text{rbf}, \\text{poly}\\}$.\n- Коэффициент регуляризации `C` $\\in \\{0.1, 1, 10, 100\\}$ (4 варианта).\n- Коэффициент ядра `gamma` $\\in \\{10^{-3}, 10^{-2}, 10^{-1}, 1\\}$ (4 варианта, только для `rbf`).\n- Степень полинома degree $\\in \\{2, 3, 4\\}$ (3 варианта, только для `poly`).\n\nНаивный подсчёт дал бы нам $3 \\times 4 \\times 4 \\times 3 = 144$ комбинации, большинство из которых недопустимы (например, `kernel='linear'` и `degree=3`). А правильный способ здесь — это именно сумма по веткам:\n\n$$N_{\\text{cond}}=\\underbrace{4}{\\text{linear: только }C} +\\underbrace{(4\\cdot 4)}{\\text{rbf: }C,\\gamma} +\\underbrace{(4\\cdot 3)}_{\\text{poly: }C,\\text{degree}} =32. $$\n\nВсего 32 допустимые комбинации вместо 144. Этот простой подсчёт показывает, как учёт структуры задачи комбинаторно сжимает пространство поиска.\n\n\u003e 💡Если есть запрещённые сочетания (допустим, нельзя брать A и D вместе) или взаимоисключающие группы, их удобно учитывать правилом произведения и методом включений-исключений: вычитаем запрещённые подпространства и корректируем пересечения.\n\n**Теперь про экономию бюджета.** Тратить ресурсы на обучение заведомо слабых конфигураций — непозволительная роскошь. Для борьбы с этим `optuna` использует `pruning` (отсечение), который работает по принципу турнира на выживание (алгоритм Successive Halving).\n\nПредставьте, что у нас есть 100 конфигураций гиперпараметров — это наши участники турнира на выживание:\n\n- Раунд 1. Мы даём всем 100 участникам минимальный бюджет (например, обучаем всего одну эпоху) и оцениваем их результат.\n- Раунд 2. Мы отбрасываем половину худших, а оставшимся 50 «выжившим» даём удвоенный бюджет (еще несколько эпох).\n- Раунд 3. Снова отбрасываем половину и удваиваем бюджет для оставшихся 25.\n\nИ так далее. До финала (полного бюджета на обучение) доходят лишь несколько самых сильных конфигураций. Интуиция простая: лучше немного потратить на многих и быстро понять, кто перспективен, чем одинаково дорого обучать всех. С точки зрения комбинаторики это буквально отсечение целых ветвей в большом дереве возможных решений.\n\n**Наконец, откуда берутся новые кандидаты.** Вместо того чтобы пробовать точки вслепую, `optuna` использует алгоритм TPE (англ. Tree-structured Parzen Estimator), который работает как опытный рыболов. Он делит уже просмотренные точки на удачные (скажем, верхние 25% по метрике, этот порог задаётся параметром $\\gamma$) и остальные, строит для каждой группы свою вероятностную модель и выбирает новые точки там, где хороший улов встречается чаще, а плохой — реже.\n\nФормально мы получаем две функции плотности:\n\n- $l(x) = p(x \\mid y \\in \\text{топ-}\\gamma)$ — это распределение значений гиперпараметров $x$, характерное для «удачных» результатов.\n- $g(x) = p(x \\mid y \\notin \\text{топ-}\\gamma)$ — распределение, характерное для «неудачных» результатов.\n\nСледующий кандидат для проверки выбирается из области, где вероятность получить хороший результат высока, а плохой — низка. Математически это означает, что TPE ищет точку $x$, которая максимизирует отношение $l(x)/g(x)$.\n\nТаким образом, вместо равномерного обхода всего декартова произведения поиск разумно концентрируется на наиболее перспективных областях.\n\nВместе эти приёмы не отменяют рост пространства решений, но позволяют обращаться с ним бережно: учитывать структуру, быстро зачищать слабые ветви и направлять попытки туда, где это имеет смысл.\n\nА теперь поговорим о том, почему вообще это пространство так быстро разрастается и к каким эффектам это приводит.\n\n## Проклятие размерности и комбинаторный взрыв\n\nВ мире машинного обучения достаточно часто упоминают т. н. «проклятие размерности», но далеко не всегда приводится понятное объяснение, в чём же оно заключается.\n\nТермин «проклятие размерности» принадлежит Ричарду Беллману (1920–1984), американскому математику и основателю динамического программирования. Его мысль была простой: рост размерности сам по себе делает многие наивные методы непрактичными, но это не повод сдаваться.\n\u003cbr\u003e\n\n\u003c!DOCTYPE html\u003e\n\n\u003chtml lang=\"ru\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eЦитата автора\u003c/title\u003e\n    \u003cstyle\u003e\n        .container {\n            border: 1px solid #f5f5f5;\n            padding: 20px;\n            border-radius: 15px;\n            width: 100%;\n            max-width: 100%;\n            box-sizing: border-box;\n            background-color: #f5f5f5;\n        }\n        .author-info {\n            display: flex;\n            align-items: center;\n            gap: 25px;\n            margin-bottom: 20px;\n        }\n        .author-avatar {\n            width: 75px;\n            height: 75px;\n            border-radius: 50%;\n            object-fit: cover;\n        }\n        .author-name {\n            font-weight: bold;\n            margin: 0;\n        }\n        .author-position {\n            margin: 0;\n            color: #666;\n        }\n        .text-section {\n            margin-top: 15px;\n            line-height: 1.5;\n        }\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"container\"\u003e\n        \u003cdiv class=\"author-info\"\u003e\n            \u003cimg src=\"https://yastatic.net/s3/education-portal/media/ff1150e42d25517a20a9a163b34bd16b_ac6f7cef17.webp\" alt=\"Аватар Ричард Беллман\" class=\"author-avatar\"\u003e\n            \u003cdiv\u003e\n                \u003cp class=\"author-name\"\u003eРичард Беллман (1920–1984)\u003c/p\u003e\n                \u003cp class=\"author-position\"\u003eамериканский математик и основатель динамического программирования\u003c/p\u003e\n            \u003c/div\u003e\n        \u003c/div\u003e\n        \u003cdiv class=\"text-section\"\u003e\n            \u003cp\u003e\u003ci\u003e«И хотя это „проклятие“ уже много лет висит над физиками и астрономами, не стоит отчаиваться: значимые результаты можно получить несмотря на него».\u003cbr\u003e\n\u003c/br\u003e“Since this is a curse which has hung over the head of the physicist and astronomer for many a year, there is no need to feel discouraged about the possibility of obtaining significant results despite it”.\u003c/i\u003e\u003c/p\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\n\nЧтобы не быть голословными, проиллюстрируем эту мысль Беллмана на упрощённом (и несколько теоретизированном) примере из замечательной [книжки](https://arxiv.org/abs/2104.13478) «Геометрическое глубокое обучение: решётки, группы, графы, геодезические и калибры».\n\nДля этого нам придётся ввести определение функции, [непрерывной по Липшицу](https://education.yandex.ru/handbook/math/article/predeli-i-neprerivnost-funktsii#:~:text=%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%B2%D1%85%D0%BE%D0%B4%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.-,%D0%9B%D0%B8%D0%BF%D1%88%D0%B8%D1%86%D0%B5%D0%B2%D0%B0%20%D0%BD%D0%B5%D0%BF%D1%80%D0%B5%D1%80%D1%8B%D0%B2%D0%BD%D0%BE%D1%81%D1%82%D1%8C,-%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F): функция $f(x)$ называется непрерывной по Липшицу, если существует такая константа $L\u003e0$, что для любых двух точек $x_1, x_2$ из области определения функции верно следующее выражение:\n\n$$|f(x_1) - f(x_2)| \\leq L \\cdot |x_1 - x_2|. $$\n\nПо сути мы утверждаем, что абсолютное значение производной (а значит, и скорости роста) функции ограничено сверху числом $L$, и функция меняется достаточно плавно.\n\nЗачем нам это определение? Мы сейчас построим очень послушную (гладкую) функцию, которая тем не менее потребует экспоненциально много наблюдений. Липшицевость зафиксирует именно эту «послушность».\n\nТеперь перейдём к проклятию размерности. Для его иллюстрации нам понадобится понятие $d$-мерного единичного гиперкуба — здесь мы понимаем множество $[0,1]^d={\\boldsymbol x=(x_1,\\dots, x_d)\\mid 0\\le x_i\\le 1}$. Проще всего представить его так:\n\n- При $d=1$ это отрезок $[0,1]$ с двумя вершинами-концами: $\\{0,1\\}$.\n- При $d=2$ это квадрат $[0,1]^2$ с четырьмя вершинами: $\\{(0,0), (0,1), (1,0), (1,1)\\}$.\n- При $d=3$ это куб $[0,1]^3$ с восемью вершинами.\n\nИнтуитивная картинка очень простая: представьте, что у нас есть $d$ независимых переключателей $0/1$. Каждая возможная конфигурация этих переключателей — это одна из вершин гиперкуба. По правилу произведения, общее число таких конфигураций (а значит, и вершин) равно $2^d$. Формально множество вершин — это $\\{0,1\\}^d$, а сам гиперкуб — это всё пространство внутри них, то есть множество точек $[0,1]^d$.\n\nРассмотрим теперь липшицеву функцию вида\n\n$$ f(\\boldsymbol x) = \\sum_{j=1}^{2^d} z_j \\phi(\\boldsymbol x - \\boldsymbol x_j), $$\n\nгде $z_j = \\pm 1$, $\\boldsymbol x_j \\in \\mathbb R^d$ расположены в вершинах гиперкуба, а $\\phi$ – липшицева функция с константой $\\varepsilon$, похожая, например, на гауссиану. Здесь $\\phi$ липшицева, значит и вся сумма липшицева; знак $z_j$ задаёт «вверх» или «вниз» в окрестности вершины $\\boldsymbol x_j$.\n\nПоскольку знаки $z_j$ независимы, без наблюдения в вершине $\\boldsymbol x_j$ мы не узнаем, куда «загибается» функция именно там. Следовательно, чтобы корректно её аппроксимировать на всём гиперкубе, нужно минимум $2^d$ наблюдений — по одному на вершину.\n\nМы видим, что их число растёт экспоненциально с ростом размерности пространства, что недопустимо в задачах машинного обучения: в современных задачах размерность пространства может достигать десятков и сотен тысяч (а может быть, и больше).\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/5_4_4_7da4da10fe.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"5.4.4.\"\n\u003c/figure\u003e\n\nПо сути, мы наблюдаем тот же комбинаторный взрыв, но уже в непрерывном представлении: чем больше размерность пространства, тем больше нам нужно примеров для обучения.\n\nЕсли бы мы не пользовались дополнительной информацией о решаемой задаче, будь то физические ограничения, экспертные знания или симметрия данных, то решение сложных задач было бы для нас недоступным.\n\nЧтобы было понятно, в какой задаче возникает проклятие, представим обычный процесс обучения: мы хотим построить классификатор $f:[0,1]^d\\to{1,\\dots,K}$, который по входу $\\boldsymbol x$ предсказывает класс. Если не накладывать дополнительных структурных предположений о данных или о самой $f$ (кроме общей плавности), то для надёжного обобщения нужно покрыть область $[0,1]^d$ достаточным числом примеров, а необходимое число таких точек растёт экспоненциально по размерности $d$. На конкретном примере это видно особенно ясно.\n\nДаже для классификации простой картинки размерами $32\\times32$ (как в наборе данных CIFAR) необходимо работать с пространством размерностью $d=1024$ (пусть изображение приведено в чёрно-белую палитру). Даже если каждый пиксель принимает лишь значения $0$ или $1$ (т. е. по сути является бинарным), формально различных вариантов уже $2^{1024}$, и рассмотреть их все нет никакой физической возможности.\n\nИменно поэтому во всех современных подходах в области глубокого обучения используются модели, которые учитывают симметрии данных. Например, операция свёртки эквивариантна к сдвигу, механизм внимания (англ. self-attention) — к перемешиванию и пр.\n\n***\n\nВ этом параграфе мы разобрали, что гиперпараметры задают форму модели и почему их подбор дорог: каждый новый переключатель комбинаторно раздувает число конфигураций.\n\nСравнили три базовые стратегии: сетку, случайный поиск и байесовскую оптимизацию — и увидели, как практические инструменты вроде `optuna` структурируют пространство (условные ветки), экономят бюджет за счёт досрочного отсечения слабых конфигураций и направляют попытки туда, где есть шанс на выигрыш.\n\nНаконец, мы ввели интуицию проклятия размерности: без дополнительных предположений объём данных, нужных для надёжного обучения в высоких размерностях, растёт экспоненциально. На практике нас выручают структура, симметрии и явные ограничения.\n\nВ мире машинного обучения комбинаторика обычно не даёт ответов на вопрос «как решать эту задачу?». Вместо этого она позволяет оценить сложность задачи, натолкнуть на мысли о поиске дополнительных ограничений и в целом оптимизировать трудозатраты. Многие из упомянутых подходов и тем ещё не раз встретятся вам в следующих главах и в реальных задачах.\n\nВ заключительном параграфе мы соберём ключевые идеи всей главы в одном месте — от языка множеств и правил подсчёта до метода включений-исключений. И зафиксируем, каким набором приёмов вы теперь владеете."])</script><script nonce="">self.__next_f.push([1,"76:T126c,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы прошли путь от базовых строительных блоков — множеств и операций над ними — до понимания глобальных проблем машинного обучения, таких как «проклятие размерности». Вы получили не просто набор формул, а способ мышления, который помогает оценивать сложность задач и понимать масштаб пространства возможных решений.\n\nДавайте подведём итоги и вспомним, какие инструменты теперь есть в вашем арсенале.\n\nЧто вы изучили:\n\n- **Язык множеств.** Умеете оперировать множествами и подмножествами, работать с объединением, пересечением, разностью, симметрической разностью и декартовым произведением. Понимаете, что такое мощность множества и пустое множество.\n- **Фундаментальные правила подсчёта.** Умеете применять правило суммы и произведения для решения комбинаторных задач, а также знаете, как принцип Дирихле помогает быстро получить жёсткую оценку.\n- **Классический инструментарий.** Умеете вычислять перестановки, размещения и сочетания (с повторениями и без). Понимаете их глубокую связь с биномиальными коэффициентами, треугольником Паскаля и формулой разложения бинома.\n- **Продвинутые техники.** Умеете применять метод включений-исключений для аккуратного учёта пересечений. Понимаете, что такое множество всех подмножеств (power set) и почему его размер растёт экспоненциально, удваиваясь с каждым новым элементом.\n\nСамое главное — вы увидели, что комбинаторика в машинном обучении не абстрактная теория, а суровая практика. Теперь вы понимаете:\n\n- **Почему подбор гиперпараметров дорогой.** Видно, как быстро раздувается сетка перебора с каждым новым параметром и как на практике помогают случайный поиск и байесовская оптимизация.\n- **Откуда берётся комбинаторный взрыв.** На примерах отбора признаков, аугментаций и архитектур WANN вы увидели экспоненциальный рост числа конфигураций — полный перебор быстро становится бессмысленным.\n- **Интуиция проклятия размерности.** С ростом числа признаков данных нужно несоразмерно больше. Отсюда и важность структурных предположений и учёта симметрий в моделях.\n\nДо сих пор мы отвечали на вопрос «Сколько вариантов?». Но в реальных задачах так же важно найти ответ на вопрос «Насколько это вероятно?». Поэтому мы переходим от комбинаторики к теории вероятностей: события — это те же множества, а подсчёты превращаются в оценку вероятности.\n\nВ следующей главе мы введём случайный эксперимент и пространство исходов, разберём вероятность и условную вероятность, независимость и формулу Байеса, поговорим о математическом ожидании и дисперсии — и продолжим увязывать всё это с практикой анализа данных и машинного обучения.\n"])</script><script nonce="">self.__next_f.push([1,"77:Teee,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы разберём основы теории вероятности и статистики — важнейших инструментов анализа данных и машинного обучения. Мы рассмотрим ключевые концепции, которые помогут вам понять, как оценивать качество моделей и строить более точные предсказания.\n\nВот о чём пойдёт речь:\n\n- **Вероятностные пространства и аксиомы.** Мы начнём с основ теории вероятностей — изучим вероятностные пространства и основные аксиомы. Понимание этих базовых принципов поможет вам строить корректные вероятностные модели.\n- **Условная вероятность и независимость.** Познакомимся с понятием условной вероятности, а также с формулами полной вероятности и Байеса. На примере наивного Байесовского классификатора мы увидим, как эти знания применяются для реальных задач машинного обучения.\n- **Характеристики случайных величин.** Изучим математическое ожидание, дисперсию, ковариацию и другие числовые характеристики, которые позволяют анализировать распределения и взаимосвязи данных.\n- **Энтропия, перплексия и дивергенция.** Рассмотрим понятия, пришедшие из теории информации, которые позволяют измерять неопределённость и «расстояние» между распределениями. Эти идеи широко применяются в машинном обучении — от решающих деревьев до языковых моделей и методов визуализации.\n\nПрочитав эту главу, вы сможете:\n\n1. **Использовать инструменты для анализа данных.** Вы изучите важнейшие концепции теории вероятности, которые помогут вам оценивать корректность предсказаний и строить вероятностные модели.\n2. **Понимать и применять классификационные алгоритмы.** Освоив условные вероятности и формулу Байеса, вы сможете использовать эти методы для улучшения ваших моделей машинного обучения.\n3. **Оценивать метрики качества моделей.** Вы научитесь интерпретировать метрики, такие как точность и полнота, и применять их для корректной оценки эффективности ваших решений.\n\nДля комфортного чтения главы полезно иметь базовые знания в математике — особенно в работе с функциями и логикой. Опыт работы с данными или машинным обучением поможет лучше прочувствовать примеры, но не является обязательным.\n\nПриступим!"])</script><script nonce="">self.__next_f.push([1,"78:T5735,"])</script><script nonce="">self.__next_f.push([1,"Случайности окружают нас повсюду: от элементарных игр на удачу до сложных систем вроде социальной динамики или финансовых рынков. Чтобы эффективно анализировать и предсказывать события реальной жизни, нам необходимо строить математические модели, которые их описывают.\n\nПоскольку большинство интересующих нас событий имеют `случайную` природу, нам необходим фундамент для построения вероятностных моделей. Таким фундаментом является понятие `вероятностного пространства`.\n\nРассмотрим самую простую ситуацию, в которой возникает случайность: пусть мы подкидываем кубик и у нас может выпасть любое число очков от 1 до 6. Возможные результаты броска, то есть «на кубике выпало $k$ очков», называются `элементарными исходами`.\n\nИногда нас интересует, не сколько именно очков выпало, а было ли количество очков, например, чётным. Такие наборы элементарных исходов называются `случайными событиями`.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Возможные результаты броска, т.е. “на кубике выпало k очков”, называются элементарными исходами\" src=\"https://yastatic.net/s3/education-portal/media/5_2_1_Vozmozhnye_rezultaty_broska_t_e_na_kubike_vypalo_k_ochkov_nazyvayutsya_elementarnymi_ishodami_aa8896b554.webp\"\u003e\n  \u003cfigcaption\u003e\n    Возможные результаты броска, т.е. “на кубике выпало k очков”, называются элементарными исходами\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nМы также можем задаться вопросом, насколько ожидаемо то или иное событие. В этом случае говорят про `вероятность`. Чтобы найти вероятность события, нам нужно посчитать (сложить) вероятности элементарных исходов, из которых оно состоит.\n\nВернёмся к примеру с кубиком. Если кубик `честный`, все элементарные исходы равновероятны. Элементарных исходов всего 6, и сумма их вероятностей равна 1 — на кубике гарантированно выпадет одно из этих чисел. Значит, вероятность каждого элементарного исхода равна $\\frac{1}{6}$. Тогда вероятность наступления события «выпало чётное число очков» равняется $\\frac{1}{2}$:\n\n$$\\mathbb{P}(\\text{“выпало четное число очков”})= \\mathbb{P}(\\text{“выпало 2 очка”}) + \n$$\n\n$$ +  \\mathbb{P}(\\text{“выпало 4 очка”}) + \\mathbb{P}(\\text{“выпало 6 очков”})= \\frac{3}{6}=\\frac{1}{2}.\n$$\n\nЧтобы полностью определить `вероятностное пространство`, нужно описать множество элементарных исходов, множество событий, для которых мы можем посчитать вероятности, и задать вероятность на множестве всех событий.\n\nСейчас мы обсудим, как вероятностное пространство определяют строго математически. Не переживайте, если поймёте не всё сразу, — ниже будут приведены подробные примеры.\n\nИтак, вероятностное пространство — это тройка $(\\Omega, \\mathcal{F}, \\mathbb{P})$. Да, она пишется именно в скобках и читается как «омега», «эф», «пэ». В этой тройке:\n\n- **Ω** — множество произвольной природы, которое мы называем `пространством элементарных исходов` $\\omega \\in \\Omega$. Элементарные исходы — это суть того, что появляется в результате одного эксперимента.\n- $\\mathcal{F} \\subset 2^\\Omega$ — множество всех случайных событий, каждое из которых является подмножеством $\\Omega$ (здесь $2^{\\Omega}$ — это множество всех подмножеств $\\Omega$).  На самом деле в качестве $\\mathcal{F}$ можно взять не любой набор подмножеств $\\Omega$. Это должна быть так называемая $\\sigma$-алгебра (сигма-алгебра), то есть $\\mathcal{F}$ должна удовлетворять следующим условиям:\n  - $\\Omega \\in \\mathcal{F}$. Это означает, что мы всегда можем измерить вероятность события $\\Omega$, которое говорит, что в результате нашего случайного эксперимента хоть что-то произойдёт. Как вы могли догадаться, вероятность такого события будет всегда равна 1.\n  - Если $A \\in \\mathcal{F}$, то $\\bar{A} = \\Omega\\setminus A \\in \\mathcal{F}$.  То есть, если мы можем измерить вероятность того, что событие $A$ произойдёт, то мы также можем измерить вероятность того, что событие $A$ не произойдёт.\n  - Если $\\{А_k\\}_{k=1}^{\\infty} \\in \\mathcal{F}$, то $\\cup_{k \\geq 1} A_k \\in \\mathcal{F}$.  Это условие можно понимать так: если мы можем найти вероятность каждого из событий, то можем найти и вероятность, что хотя бы одно из них произойдёт.\n- $\\mathbb{P}:\\mathcal{F}\\to [0,1]$ — функция, которая каждому событию ставит в соответствие его вероятность, то есть число от 0 до 1. Мы также требуем от $\\mathbb{P}$ выполнения следующих естественных условий:\n  - Вероятность того, что в результате эксперимента что-то произойдёт, равна 1, то есть $\\mathbb{P}(\\Omega) =1$.\n  - Для взаимоисключающих событий вероятность того, что произойдёт хотя бы одно из них, равна сумме их вероятностей. Математически это записывается так: если $\\{А_k\\}_{k=1}^{\\infty} \\in \\mathcal{F} ~$$\\{А_k\\}_{k=1}^{\\infty} \\in \\mathcal{F}$$A_i\\cap A_j=\\varnothing$ при $i \\neq j$, то $\\mathbb{P}(\\cup_{k \\geq 1} A_k)=\\sum_{k\\ge 1}\\mathbb{P}(A_k)$.\n\nНа картинке ниже мы привели наглядный пример вероятностного пространства со множеством элементарных исходов $\\Omega=\\{a,b,c\\}$, а также список всех событий (элементов $\\mathcal{F}$ ) и их вероятностей.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Наглядный пример вероятностного пространства с множеством элементарных исходов\" src=\"https://yastatic.net/s3/education-portal/media/5_2_2_Naglyadnyj_primer_veroyatnostnogo_prostranstva_s_mnozhestvom_elementarnyh_ishodov_Omega_a_b_c_ba2abd5a3f.webp\"\u003e\n  \u003cfigcaption\u003e\n    Наглядный пример вероятностного пространства с множеством элементарных исходов\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЧтобы разобраться с определением вероятностного пространства получше, рассмотрим некоторые частные случаи.\n\n### **Модель классической вероятности**\n\nВернёмся к нашему честному кубику и опишем строго вероятностное пространство. В этом случае:\n\n- $\\Omega =\\{1, 2, 3, 4, 5, 6\\}$,\n- $\\mathcal{F} =2^{\\Omega}$,\n- $\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|}$ для любого $A\\in \\mathcal{F}$.\n\nВ общем случае если $\\Omega$ — конечное множество, а $\\mathcal{F}$ и $\\mathbb{P}$ заданы как в примере с кубиком, то $(\\Omega, 2^{\\Omega}, \\mathbb{P})$ называется `моделью классической вероятности`.\n\nКогда рассматривается некоторое событие $A$, то элементарные исходы, содержащиеся в событии $A$, называют `благоприятными`. Поэтому формулу $\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|}$ можно понимать как долю благоприятных исходов, то есть отношение количества благоприятных исходов к числу всех исходов.\n\nПроиллюстрируем модель классической вероятности ещё одним примером.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Модель классической вероятности\" src=\"https://yastatic.net/s3/education-portal/media/5_2_3_Model_klassicheskoj_veroyatnosti_jpg_bc724e5962.webp\"\u003e\n  \u003cfigcaption\u003e\n  Модель классической вероятности\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа самом деле на конечном множестве можно задать вероятность и по-другому.\n\n### **Конечные вероятностные пространства**\n\nРассмотрим конечное множество:\n\n- $\\Omega=\\{\\omega_1,\\omega_2,\\dots, \\omega_n\\}$\n- $\\mathcal{F} =2^\\Omega$\n\nИ зададим произвольные вероятности элементарных исходов:\n\n- $\\mathbb{P}(\\omega_i)=p_i\\ge 0,\\quad \\sum_{i=1}^n p_i = 1$\n\nЭто задаст однозначно вероятность каждого события $A$.\n\n- $\\mathbb{P}(A)=\\sum_{\\omega\\in A}\\mathbb{P}(\\omega)$\n\nПроиллюстрируем на примере.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Пример вычисления вероятности событий в вероятностном пространстве\" src=\"https://yastatic.net/s3/education-portal/media/karp_94ca9f05ce.webp\"\u003e\n  \u003cfigcaption\u003e\n    Пример вычисления вероятности событий в вероятностном пространстве\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nТакие вероятностные пространства встречаются на практике очень часто. Они появляются тогда, когда мы описываем результаты эксперимента с конечным количеством исходов. Например, когда мы ловим рыбу в пруду, вероятности поймать рыбу разных видов разные. Когда едем на работу, вероятность опоздать не равна вероятности приехать вовремя. А когда играем в дартс, вероятности попадания в каждый из секторов не равны друг другу.\n\n### **Модель геометрической вероятности**\n\nЕщё один важный частный случай общей схемы вероятностного пространства $(\\Omega, \\mathcal{F}, \\mathbb{P})$ — это `модель геометрической вероятности`.\n\nВ этом случае $\\Omega \\subset \\mathbb{R}^N$ — это некоторая геометрическая область (если говорить более строго, то $\\Omega$ — измеримое по Лебегу множество положительной меры). Оказывается, что взять в качестве $\\mathcal{F}$ множество $2^{\\Omega}$ уже нельзя, так как не все подмножества $\\mathbb{R}^N$ оказываются измеримыми в хоть сколько-нибудь разумном смысле.\n\nПоэтому в качестве $\\mathcal{F}$ берут $\\sigma$-алгебру множеств, измеримых по [Лебегу](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%B1%D0%B5%D0%B3,_%D0%90%D0%BD%D1%80%D0%B8_%D0%9B%D0%B5%D0%BE%D0%BD), а для любого $A \\in \\mathcal{F}$ вероятность задают формулой $\\mathbb{P}(A) =\\frac{\\mu(A)}{\\mu(\\Omega)}$, где $\\mu$ — это мера Лебега.\n\nМеру Лебега можно рассматривать как некоторое разумное обобщение длины на прямой, площади на плоскости и объёма в пространстве. Пример: если у плоской фигуры можно измерить площадь, то её мера Лебега — это её площадь.\n\n\u003cfigure\u003e\n  \u003cimg alt=\"5.2.4.\" src=\"https://yastatic.net/s3/education-portal/media/5_2_4_Primer_primeneniya_ravnomernogo_raspredeleniya_dlya_vychisleniya_veroyatnostej_v_klassicheskom_opredelenii_Laplasa_Kommentarij_shtu_e7d6ec5b5b.webp\"\u003e\n  \u003cfigcaption\u003e\n  Применение равномерного распределения для вычисления вероятностей\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cfigure\u003e\n  \u003cimg alt=\"\" src=\"https://yastatic.net/s3/education-portal/media/5_2_5_Izobrazhenie_illyustriruet_princzip_vychisleniya_veroyatnostej_v_dvumernom_ravnomernom_raspredelenii_cherez_otnoshenie_ploshhadej_obla_89558fff93.webp\"\u003e\n  \u003cfigcaption\u003e\n  Вероятности в двумерном равномерном распределении через площадь областей\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nДавайте рассмотрим следующий пример. Пусть мы случайно и независимо выбираем два числа из отрезка \\[0, 2\\]. Попробуем, например, посчитать вероятность, что они будут отличаться друг от друга не менее чем на 1.\n\nВыбранные числа — это пара $(x,y)\\in [0,2] \\times [0,2] \\subset \\mathbb{R}^2$, то есть $\\Omega$ — это квадрат на плоскости. Площадь $\\Omega$ равна 4.\n\nУсловие, что они отличаются друг от друга не менее чем на 1, можно записать как:\n\n$|x-y|\\ge 1 ~\\Longleftrightarrow ~ \\left[ \\begin{gathered} y\\ge 1+x \\\\ y\\le -1+x\\\\ \\end{gathered}\\right.$\n\nНарисуем $\\Omega$ и закрасим интересующую нас область.\n\n![5.2.6. Закрашенная область — два равных прямоугольных треугольника, площадь каждого из них равна ](https://yastatic.net/s3/education-portal/media/5_2_6_Zakrashennaya_oblast_dva_ravnyh_pryamougolnyh_treugolnika_ploshhad_kazhdogo_iz_nih_ravna_0_5_Shtuka_pohozhaya_na_zmejku_eto_omega_836db3c260.webp)\n\nЗакрашенная область — два равных прямоугольных треугольника, площадь каждого из них равна $0.5$. То есть:\n\n$$\\mathbb{P}(\\text{“числа отличаются не менее, чем на 1\"}) = \\frac{0.5 + 0.5}{4} = 0.25. \n$$\n\nРазберём ещё один пример, который часто встречается на практике.\n\n### **Декартово произведение вероятностных пространств**\n\nПусть мы подкидываем нечестную монетку $N$ раз. Нечестная монетка отличается от честной тем, что у неё вероятность выпадения орла равна некоторому числу $p$, не обязательно равному $0.5$.\n\nВероятностное пространство в этом случае будет состоять из всех последовательностей длины $N$ из орлов (О) и решек (Р). В частности, $|\\Omega|=2^N$. Множество $\\Omega$ конечно, поэтому в качестве $\\mathcal{F}$ можно взять $\\mathcal{F}=2^\\Omega$.\n\nТеперь зададим вероятность. Вероятность выпадения орла равна $p$, значит, вероятность выпадения решки равна $q=1-p$. Результаты подбрасываний не зависят друг от друга, поэтому вероятность получить заданную последовательность орлов и решек будет равна произведению вероятностей получить каждый член последовательности при одном подбрасывании (подробнее про независимость мы поговорим в следующем параграфе). Если $N = 3$, то получаем:\n\n$\\mathbb{P}(ООО)=p\\cdot p\\cdot p = p^3$,\n\n$\\mathbb{P}(ООР)=\\mathbb{P}(ОРО)=\\mathbb{P}(РОО)=p^2q$,\n\n$\\mathbb{P}(ОРР)=\\mathbb{P}(РРО)=\\mathbb{P}(РОР)=pq^2$,\n\n$\\mathbb{P}(РРР)=q^3$.\n\nИ мы можем найти, например, следующую вероятность:\n\n$\\mathbb{P}(\\text{``выпало ровно два орла''})=\\mathbb{P}(ООР)+\\mathbb{P}(ОРО)+\\mathbb{P}(РОО)=3p^2q$.\n\nОписанное в этом примере вероятностное пространство является частным случаем более общей конструкции, которая называется `декартовым произведением вероятностных пространств`. Эта конструкция применяется, когда мы создаём вероятностное пространство для нескольких независимых последовательных экспериментов.\n\nИтак, с теорией немножко разобрались. Теперь давайте обсудим, откуда брать вероятностное пространство на практике.\n\n### Частотный подход\n\nПусть мы проводим $N$ раз независимо какой-то эксперимент, результатом которого является один из $n$ исходов $w_1,w_2,\\ldots,w_n$.\n\nНапример, мы проверяем, кликнет ли человек по рекламному баннеру в интернете, увидев его, или нет. В этом случае исходов всего два: либо кликнет, либо нет. Закон больших чисел, про который мы подробно поговорим позже, утверждает, что по мере увеличения числа проведённых экспериментов доля появлений результата $w_i$ стремится к $\\mathbb{P}(w_i)$.\n\nТо есть для построения вероятностной модели можно провести эксперимент очень большое количество раз и взять долю каждого элементарного исхода в качестве его вероятности. Такой подход к построению вероятностного пространства называется `частотным`.\n\nНа иллюстрации ниже — пример, как найти вероятности для кликов по баннеру.\n\n![5.2.7. Пример, как мы проверяем, кликнет ли клиент по баннеру в интернете](https://yastatic.net/s3/education-portal/media/5_2_7_e1adbda617.webp \"Пример, как мы проверяем, кликнет ли клиент по баннеру в интернете\")\n\nИтак, в этом параграфе вы научились строить вероятностные пространства и вычислять вероятности простых событий.  Тем не менее в практике анализа случайных процессов важно учитывать, что события могут быть связаны друг с другом. Именно для этого существуют условная вероятность, формула Байеса и формула полной вероятности, которые мы разберём в следующем параграфе.\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13736524.6a289cad0c8cd32d4fc61c57aec5155a55656ae6/?iframe=1\" frameborder=\"0\" name=\"ya-form-13736524.6a289cad0c8cd32d4fc61c57aec5155a55656ae6\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"79:T6fc3,"])</script><script nonce="">self.__next_f.push([1,"В жизни часто случается, что дополнительная информация радикально меняет нашу оценку вероятности события. Именно это и отражает понятие `условной вероятности`, которое лежит в основе многих статистических методов и алгоритмов машинного обучения.\n\nВ этом параграфе мы познакомимся с условной вероятностью, формулой Байеса и концепцией независимости событий, а затем продемонстрируем их использование на конкретных примерах.\n\nНо начнём чуть издалека.\n\n### Короткая история\n\nПредставьте — мы обучили модель, которая на основе определённых характеристик прогнозирует, болен человек или здоров. Теперь нам нужно проверить точность её прогнозов.\n\nДля этого мы можем взять тестовую выборку пациентов, про которых мы уже знаем, здоровы они или больны, прогнать через модель, — а затем посчитать метрики качества (о них чуть ниже).\n\nСразу заметим, что подобные задачи называются задачами классификации — нам надо распределить объекты по классам. В этом случае объекты — это пациенты, а классов два — «болен» и «здоров».\n\nЕсли считать, что тестовая выборка достаточно большая и репрезентативная, можно построить вероятностное пространство на основе этих данных. В нём будет четыре элементарных исхода:\n\n\u003cfigure\u003e\n  \u003cimg alt=\"Модель предсказывает, болен ли человек или здоров\" src=\"https://yastatic.net/s3/education-portal/media/5_3_1_Model_predskazyvaet_bolen_li_chelovek_ili_zdorov_5754af22a2.webp\"\u003e\n  \u003cfigcaption\u003e\n  Модель предсказывает, болен ли человек или здоров\n   \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВероятностью каждого элементарного исхода мы будем считать долю соответствующих вариантов в нашей выборке.\n\nТеперь поговорим о метриках. Самая простая — это `accuracy`, вероятность правильного предсказания.\n\nДля начала запишем результаты предсказаний модели по тестовой выборке в виде удобной таблицы, которая называется `матрицей ошибок (confusion matrix)`. В каждой клетке этой таблицы записано количество людей из тестовой выборки с фиксированным состоянием здоровья и фиксированным предсказанием модели:\n\n#|\n||\n\n\n|\n\nБолен\n\n|\n\nЗдоров\n\n||\n||\n\nПредсказано, что болен\n\n|\n\n10\n\n|\n\n100\n\n||\n||\n\nПредсказано, что здоров\n\n|\n\n90\n\n|\n\n9800\n\n||\n|#\n\nЧтобы найти вероятность правильного предсказания, нужно количество людей с правильным предсказанием поделить на общее количество людей. У нас получится:\n\n$$Accuracy = \\frac{10+9800}{10+90+100+9800}= 0.981\n$$\n\nЭто отличное значение!\n\nНо при этом мы видим, что модель посчитала большинство больных людей здоровыми. Это показывает, что `accuracy` не стоит применять при большом дисбалансе классов (9900 здоровых и 100 больных).\n\nВидим, что нужны какие-то другие метрики для анализа качества модели. Например, `precision` (точность) и `recall` (полнота). Но, чтобы их посчитать, нам сперва нужно разобраться, что такое `условная вероятность`.\n\n### Условная вероятность\n\nНачнём с теории. Пусть некоторое событие $A$ имеет положительную вероятность $\\mathbb{P}(A) \\neq 0$. Тогда для любого события $B$ мы можем определить его `условную вероятность при условии A` по формуле:\n\n$$\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A)}\n$$\n\nВернёмся к нашей модели. Условные вероятности позволяют ввести следующие метрики:\n\n$$precision=\\mathbb{P}(\\text{предсказание верное | предсказано, что болен}),\n$$\n\n$$recall=\\mathbb{P}(\\text{предсказание верное | болен}).\n$$\n\n`precision` показывает, какая доля больных среди тех, кого модель считает больными, а `recall` — какую долю больных модель правильно определила. Чем ближе эти величины к $1$, тем лучше.\n\n{% cut \"Классическое определение `precision` и `recall`\" %}\n\nВ курсе машинного обучения эти метрики вводят немного иначе. Давайте посмотрим как.\n\nВ задаче бинарной классификации обычно выделяют один из классов, который, собственно, и хочется определять. Его называют положительным (positive). В примере выше это класс «болен». Оставшийся класс — отрицательный (negative). Эти названия совпадают с результатами тестов на болезнь.\n\nКаждый элемент матрицы ошибок принято характеризовать двумя словами, описывающими предсказание модели. Первое говорит, верное было предсказание (true) или нет (false). А второе слово — это просто предсказанный класс.\n\n#|\n||\n\n\n|\n\nБолен\n\n|\n\nЗдоров\n\n||\n||\n\nПредсказано, что болен\n\n|\n\nTrue Positive (TP)\n\n|\n\nFalse Positive (FP)\n\n||\n||\n\nПредсказано, что здоров\n\n|\n\nFalse Negative (FN)\n\n|\n\nTrue Negative (TN)\n\n||\n|#\n\nВ таких обозначениях метрики определяют следующим способом:\n\n$$accuracy=\\frac{TP+TN}{TP+TN+FN+FP} \n$$\n\n$$recall=\\frac{TP}{TP+FN}\n$$\n\n$$precision=\\frac{TP}{TP+FP}\n$$\n\nКак это определение соотносится с тем, что мы дали выше? Давайте проверим, что эти определения совпадают для метрики `recall`.\n\n$$recall=\\mathbb{P}(\\text{предсказание верное | болен})=\n$$\n\n$$\\frac{\\mathbb{P}(\\text{верно предсказано, что болен})}{\\mathbb{P}(\\text{болен})}=\n$$\n\n$$\\frac{TP/(TP+TN+FN+FP)}{(TP+FN)/(TP+TN+FN+FP)} =\\frac{TP}{TP+FN}\n$$\n\nДля метрики `precision` вычисления абсолютно аналогичны.\n\n{% endcut %}\n\nДля модели из примера выше:\n\n$$recall =\\frac{\\mathbb{P}(\\text{предсказание верное}\\cap\\text{болен})}{\\mathbb{P}(\\text{болен})}= \\frac{10/10000}{(10+90)/10000}= 0.1, \\quad \n$$\n\n$$precision =\\frac{\\mathbb{P}(\\text{предсказание верное}\\cap\\text{пресказано, что болен})}{\\mathbb{P}(\\text{предсказано, что болен})} = \n$$\n\n$$\\frac{10/10000}{(10+100)/10000}\\approx 0.091.\n$$\n\nНесмотря на высокий `accuracy`, метрики `precision` и `recall` оказались далеки от 1 — это показывает, что модель получилась не очень хорошей.\n\nТеперь вернёмся к формуле условной вероятности и посмотрим, что ещё полезного она нам может дать.\n\n### Формула Байеса и формула полной вероятности\n\nНа практике часто бывает, что величины $\\mathbb{P}(B|A)$ и $\\mathbb{P}(A)$ известны, поэтому формулу оказывается удобнее переписать в виде $\\mathbb{P}(A\\cap B) = \\mathbb{P}(B|A)\\mathbb{P}(A)$. Теперь воспользуемся этой же формулой для того, чтобы переписать числитель в следующей дроби:\n\n$$\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)}=\\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A)}.\n$$\n\nПолученная формула называется `формулой Байеса`. Совсем скоро мы посмотрим на её применение в практических задачах. Но сначала давайте обсудим следующее замечание.\n\nФормулу условной вероятности несложно обобщить на случай нескольких условий. Рассмотрим попарно непересекающиеся события положительной вероятности $H_1, \\ldots , H_n$, для которых $\\bigcup_{i=1}^n H_i=\\Omega$ (их называют `гипотезами`). Тогда для любого события $A$ верна формула:\n\n$$\\mathbb{P}(A) =\\sum \\limits_{k=1}^n \\mathbb{P}(A|H_k)\\mathbb{P}(H_k)\n$$\n\nЭта формула называется `формулой полной вероятности`.\n\nТеперь формулу Байеса можно переписать в следующем виде:\n\n$$\\mathbb{P}(H_k|A) = \\frac{\\mathbb{P}(A|H_k)\\mathbb{P}(H_k)}{\\sum \\limits_{l=1}^{n} \\mathbb{P}(A|H_l)\\mathbb{P}(H_l)}.\n$$\n\n{% cut \"Посчитаем на примере\" %}\n\nПредставьте, что некоторая модель для 90% больных людей правильно предсказывает, что они больны, а для здоровых людей с вероятностью 2% даёт ложноположительный результат — то есть говорит, что они больны. При этом известно, что в мире в настоящий момент болеет всего 1% населения.\n\nПусть теперь вы воспользовались этой моделью и получили результат «болен». С какой вероятностью вы действительно больны? Может показаться, что вероятность по крайней мере 90%. Однако давайте посмотрим более внимательно.\n\nПусть событие $A=\\text{“модель предсказала, что вы больны\"}$, а событие\n\n$B = \\text{“вы больны\"}$. Условие задачи примет вид:\n\n$\\mathbb{P}(B|A)=?,\\quad \\mathbb{P}(A|B)=0.9,\\quad \\mathbb{P}(A|\\overline{B})=0.02, \\quad \\mathbb{P}(B)=0.01$\n\nПо формуле Байеса получаем:\n\n$$\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A|B)\\mathbb{P}(B)+\\mathbb{P}(A|\\overline{B})\\mathbb{P}(\\overline{B})} = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A|B)\\mathbb{P}(B)+\\mathbb{P}(A|\\overline{B})(1-\\mathbb{P}(B))} = \\frac{0.9\\cdot 0.01}{0.9\\cdot 0.01+ 0.02\\cdot 0.99} \\approx 0.3125\n$$\n\nТо есть вероятность быть действительно больным меньше одной трети!\n\n{% endcut %}\n\nТеперь давайте обсудим практическое применение формулы Байеса.\n\n### Байесовский классификатор\n\nПредположим, что мы хотим классифицировать какие-то объекты. Например, хотим рассортировать электронную почту на спам и не спам. Для этого мы находим какие-то важные характеристики этих объектов.\n\nДля электронных писем можно составить список ключевых слов, которые часто употребляются в спаме, и рассмотреть событие $X$, которое будет описывать, какие из этих слов присутствуют в новом письме.\n\nНапример, $X=$ «есть слова ***бесплатно*** и ***срочно***, но нет слова ***акция***». Тогда, чтобы определить, является ли спамом это новое письмо, нужно сравнить\n\n$\\mathbb{P}(\\text{спам}| X) = \\frac{\\mathbb{P}(X | \\text{спам})\\mathbb{P}(\\text{спам})}{\\mathbb{P}(X)}\\quad \\text{и}\\quad  \\mathbb{P}(\\text{не спам}| X) = \\frac{\\mathbb{P}(X | \\text{не спам})\\mathbb{P}(\\text{не спам})}{\\mathbb{P}(X)}$.\n\nКакая из этих условных вероятностей будет больше, к такому классу мы и отнесём это новое письмо.\n\nЗаметим, что знаменатели в правых частях формул одинаковые, то есть достаточно сравнить числители. А оба сомножителя в числителях можно найти по достаточно большой размеченной выборке писем (для которых достоверно известно, какие из них спам). $\\mathbb{P}(\\text{спам / не спам})$ находится как доля писем этого класса в выборке, а $\\mathbb{P}(X| \\text{спам / не спам})$ — как доля писем с характеристиками $X$ среди писем данного класса.\n\nКажется, что мы уже получили работающий спам-фильтр. Однако на практике не всё так просто. Список ключевых слов достаточно обширный, поэтому вероятностная модель, построенная по тренировочной выборке, оказывается слишком неточной — если в выборке не встретилось ни одного письма с характеристиками ${X}$, то мы не сможем достоверно оценить $\\mathbb{P}(X | \\text{спам / не спам})$.\n\nЧтобы разобраться, как можно обойти это ограничение модели, нам потребуется ещё немного теории.\n\n### Независимость событий\n\nРассмотрим два события $A$ и $B$. Мы будем говорить, что $A$ и $B$ независимы, если знание о наступлении одного из них не влияет на вероятность наступления другого, то есть:\n\n$$\\mathbb{P}(A|B) = \\mathbb{P}(A).\n$$\n\nЭто условие можно эквивалентно переписать так:\n\n$$\\mathbb{P}(B|A)= \\mathbb{P}(B)\n$$\n\nИли:\n\n$$\\mathbb{P}(A\\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)\n$$\n\nЭквивалентность означает, что можно использовать любое из этих условий.\n\n\u003e **Важно:** обычно используют условие $\\mathbb{P}(A\\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)$, так как условные вероятности могут быть не определены, если $\\mathbb{P}(A)=0$ или $\\mathbb{P}(B)=0$.\n\nНесмотря на то, что независимость кажется интуитивно понятным условием, хотим предостеречь: интуиция может нас подводить, поэтому проверять независимость стоит строго математически. Рассмотрим следующий пример.\n\nПусть мы кидаем честный кубик. Рассмотрим три события:\n\n$A = \\text{“выпало чётное число очков”}$,\n\n$B_1 = \\text{“выпало больше 2 очков”}$,\n\n$B_2 = \\text{“выпало больше 3 очков”}$.\n\nЯвляются ли события $A$ и $B_1$ независимыми? А события $A$ и $B_2$?\n\nКажется, что события $B_1$ и $B_2$ очень похожи, то есть ответы на эти вопросы должны быть одинаковыми. Однако давайте посмотрим, что нам говорят вычисления.\n\nНаши события: $A =\\{2,4,6\\}$,  $B_1 =\\{3,4,5,6\\}$,  $B_2=\\{4,5,6\\}$.\n\nПолучаем:\n\n$A\\cap B_1 =\\{4,6\\}$ и $A\\cap B_2 =\\{4,6\\}$.\n\nТеперь проверим независимость:\n\n$\\mathbb{P}(A\\cap B_1)=\\frac{2}{6} = \\mathbb{P}(A)\\mathbb{P}(B_1)=\\frac{3}{6}\\cdot \\frac{4}{6}$, то есть $A$ и $B_1$ независимы.\n\n$\\mathbb{P}(A\\cap B_2)=\\frac{2}{6} \\neq \\mathbb{P}(A)\\mathbb{P}(B_2)=\\frac{3}{6}\\cdot \\frac{3}{6}$, то есть $A$ и $B_2$ не независимы!\n\nЭтот пример показывает, что интуицией стоит пользоваться аккуратно, а независимость проверять строго математически.\n\nЕсли у нас не два события, а больше, то говорят либо о `попарной независимости`, либо о `независимости в совокупности`. Разберёмся, что это такое.\n\nПусть у нас есть события $A_1, \\ldots, A_n$. Мы будем говорить, что $A_1, \\ldots, A_n$ `попарно независимы`, если каждая пара событий $\\{A_i, A_j\\}$, где $i \\neq j$, независима. Если же каждое из $A_i$ независимо от всех возможных пересечений остальных, то мы будем говорить, что $A_1, \\ldots, A_n$ `независимы в совокупности`.  Независимость в совокупности эквивалентна выполнению следующих условий:\n\n$$\\mathbb{P}(A_{i_1} \\cap A_{i_2}) = \\mathbb{P}(A_{i_1})\\mathbb{P}(A_{i_2}), \\\\\n\\mathbb{P}(A_{i_1} \\cap A_{i_2} \\cap A_{i_3}) = \\mathbb{P}(A_{i_1})\\mathbb{P}(A_{i_2})\\mathbb{P}(A_{i_3}) \\\\\n\\vdots \\\\\n\\mathbb{P}(A_{i_1} \\cap A_{i_2} \\ldots\\cap A_{i_n}) = \\mathbb{P}(A_{i_1})\\mathbb{P}(A_{i_2})\\ldots \\mathbb{P}(A_{i_n})\n$$\n\nгде $i_j$ — все возможные наборы различных индексов.\n\n\u003e **Важно:** из попарной независимости не следует независимость в совокупности!\n\nМожет звучать сложно — давайте для примера рассмотрим модель классической вероятности на множестве $\\Omega = \\{1,2,3,4\\}$. И рассмотрим следующие события $A=\\{1,2\\}$, $B=\\{1,3\\}$, $C=\\{1,4\\}$. Тогда\n\n$\\mathbb{P}(A) = \\mathbb{P}(B) = \\mathbb{P}(C) = \\frac{2}{4}$.\n\n$A\\cap B =A\\cap C = B\\cap C =\\{1\\}$  и $\\mathbb{P}(A\\cap B) = \\mathbb{P}(A\\cap C) = \\mathbb{P}(B\\cap C) = \\frac{1}{4}$.\n\nНесложно проверить, что события $A, B, C$ попарно независимы. Однако\n\n$\\mathbb{P}(A\\cap B\\cap C) = \\frac{1}{4} \\neq \\frac{1}{8} = \\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C)$, то есть события не независимы в совокупности.\n\nКроме того, обратите внимание, что для проверки независимости в совокупности недостаточно проверять лишь самые длинные равенства из определения.\n\nНа практике оказывается полезным немного более сложное условие — условная независимость. Строго она формулируется следующим образом. События $B$ и $C$ называются `независимыми при условии` $A,$ если $\\mathbb{P}(B\\cap C|A) = \\mathbb{P}(B | A)\\cdot \\mathbb{P}(C|A)$. Несмотря на то, что это свойство кажется похожим на обычную независимость событий, они никак не связаны друг с другом.\n\nАналогично тому, как для нескольких событий мы ввели независимость в совокупности, можно определить также `условную независимость в совокупности`.\n\n### Наивный байесовский классификатор\n\nЭтого объёма теории уже достаточно, чтобы построить спам-фильтр, способный отсекать до 95% спама.\n\nРассмотренное ранее событие $X$ — «есть слова ***бесплатно*** и ***срочно***, но нет слова ***акция***» удобно представить в виде $X=X_1\\cap X_2 \\cap X_3$, где каждое $X_i$ — это отсутствие или наличие соответствующего слова. Если ключевых слов больше, то $X=X_1\\cap X_2 \\cap \\ldots \\cap X_n$.\n\nОсновная проблема Байесовского классификатора заключается в том, что при больших $n$ тренировочного размеченного датасета не хватит, чтобы достоверно оценить $\\mathbb{P}(X| \\text{спам / не спам})$.\n\nТут нам на помощь приходит наивное предположение о том, что все слова появляются как в спаме, так и не в спаме независимо друг от друга. Или, если говорить формально, $X_i$ независимы в совокупности как при условии «спам», так и при условии «не спам».\n\nПосмотрим, что такое предположение даёт нам:\n\n$$\\mathbb{P}(\\text{спам}| X) = \\frac{\\mathbb{P}(X_1\\cap\\ldots\\cap X_n | \\text{спам})\\mathbb{P}(\\text{спам})}{\\mathbb{P}(X)}=\\frac{\\mathbb{P}(X_1| \\text{спам})\\cdot\\ldots\\cdot \\mathbb{P}(X_n| \\text{спам}) \\mathbb{P}(\\text{спам})}{\\mathbb{P}(X)} \n$$\n\n$$\\mathbb{P}(\\text{не спам}| X) = \\frac{\\mathbb{P}(X_1\\cap\\ldots\\cap X_n | \\text{не спам})\\mathbb{P}(\\text{не спам})}{\\mathbb{P}(X)}=\\frac{\\mathbb{P}(X_1| \\text{не спам})\\cdot\\ldots\\cdot \\mathbb{P}(X_n| \\text{не спам}) \\mathbb{P}(\\text{не спам})}{\\mathbb{P}(X)}\n\n$$\n\nЗаметим, что $\\mathbb{P}(X_i | \\text{спам  / не спам})$ уже можно достоверно оценить по не очень большой выборке.\n\nСтоит отметить, что этот же подход без изменений можно применить и к задачам многоклассовой классификации. Например, если нам потребуется рассортировать электронную почту на «спам», «промоакции» и «не спам».\n\n### Байесовский подход к вероятности\n\nВ прошлом параграфе мы рассмотрели частотный подход к вероятности на примере задачи определения вероятности клика по баннеру.\n\nЕдинственный его недостаток — требование, чтобы выборка была достаточно большой. Однако порой людям хочется оценить хотя бы грубо вероятность события, которое у них не было возможности наблюдать. Тут нам опять помогает формула Байеса и её вариации.\n\nРассмотрим вероятность того, что в ближайшую тысячу лет большой метеорит столкнётся с Землей и уничтожит человечество, — событие $A$. Такого раньше не случалось, и частотным подходом воспользоваться не получится. Однако мы можем рассмотреть более простое событие $B$ — в течение следующей тысячи лет астрономы засекут приближающийся к Земле астероид достаточно большого размера.\n\nМы знаем, что\n\n$$\\mathbb{P}(A)=\\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(B|A)}.\n$$\n\nВ этой формуле $\\mathbb{P}(B|A)=1$ — в мире достаточно много обсерваторий, чтобы гарантированно засечь такой большой астероид, а $\\mathbb{P}(A|B)$ можно примерно найти, построив математическую модель подлёта астероида (пролетит ли он мимо Земли, если мы его засекли).\n\nОценка вероятности события $B$ при этом кажется уже гораздо проще оценки вероятности исходного события $A$. Если у нас достаточно данных о наблюдениях астероидов, то можно воспользоваться частотным подходом. Иначе можно придумать некоторое удобное событие $C$ и применить к нахождению $\\mathbb{P}(B)$ тот же приём.\n\nТакой метод нахождения вероятности событий называется `байесовским подходом`. Основным его недостатком является большая неточность, которую, к тому же, нельзя оценить. Однако, когда других вариантов нет, он позволяет получить хотя бы какую-то приблизительную оценку.\n\n***\n\nНу вот, с условной вероятностью и её применениями мы разобрались. Советуем пройти квиз, чтобы закрепить знания. А дальше мы перейдём к новой важной идее — случайным величинам, которые позволяют количественно описывать случайные явления.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13736376.f86f350ba7ea719946813b141be63fa09254ae04/?iframe=1\" frameborder=\"0\" name=\"ya-form-13736376.f86f350ba7ea719946813b141be63fa09254ae04\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"7a:T695a,"])</script><script nonce="">self.__next_f.push([1,"В предыдущих параграфах этой главы мы рассмотрели случайные события и их основные свойства. Однако на практике часто нас интересуют не столько отдельные события, сколько числовые характеристики, связанные с ними.\n\nНапример, количество покупателей, посетивших магазин за день, или число очков, выпавших при броске игрального кубика. Такие величины, которые могут принимать различные значения с определёнными вероятностями, называются `случайными величинами`.\n\nОни позволяют перейти от анализа отдельных событий к количественному описанию процессов и явлений — например, вместо «пользователь кликнул» мы начинаем говорить о среднем числе кликов, вариации, вероятности хотя бы одного клика. Это открывает дорогу к построению прогнозов, оценок рисков, оптимизации бизнес-процессов и разработке моделей для машинного обучения.\n\nВ этом параграфе мы изучим `дискретные случайные величины` и их основные характеристики. Разберёмся, как формально задаются такие величины, как описываются их распределения и как рассчитываются ключевые числовые характеристики, — всё это станет основой для дальнейшего анализа вероятностных моделей.\n\n### Определение дискретных случайных величин\n\n`Дискретные случайные величины` — это величины, которые принимают конечное или счётное число значений. На практике такие величины часто представляют собой количество чего-либо, то есть часто их значениями являются целые числа. Например, количество купленных билетов, количество человек на пароме или количество звонков в колл-центр за день.\n\nВ задачах анализа данных и машинного обучения дискретные случайные величины встречаются постоянно. Например, можно предсказывать количество новых пользователей приложения за день или оценивать вероятность того, что товар не будет куплен или будет куплен, — это можно описать случайной величиной со значениями 0 и 1 (не купят\u0026nbsp;/\u0026nbsp;купят). Также во многих рекомендательных системах подсчитываются клики или просмотры — тоже счётные величины. Всё это — применения дискретных случайных величин в реальных задачах анализа данных.\n\nТеперь дадим формальное определение.\n\n`Случайная величина` $X$ на вероятностном пространстве $(\\Omega, \\mathcal{F}, \\mathbb{P})$ — это измеримая функция $X: \\Omega \\to \\mathbb{R}$, которая каждому элементарному исходу $\\omega \\in\\Omega$ ставит в соответствие число $X(\\omega)$.\n\nУсловие измеримости означает, что для любого промежутка на числовой прямой мы можем определить вероятность того, что случайная величина $X$ примет значение из этого множества. Не стоит пугаться, если это определение кажется сложным: на конкретных примерах всё станет понятно и интуитивно ясно.\n\n{% cut \"Строго про измеримость $X$ относительно $\\sigma$-алгебры $\\mathcal{F}$\" %}\n\nГоворят, что $X: \\Omega \\to \\mathbb{R}$ `измерима относительно $\\sigma$-алгебры` $\\mathcal{F}$, если для любого $t\\in \\mathbb{R}$ выполняется следующее условие:\n\n$$X^{-1}\\big((-\\infty,t]\\big)=\\{\\omega\\in\\Omega: X(\\omega)\\le t\\}\\in\\mathcal{F}\n$$\n\nЭто означает, что мы можем найти $\\mathbb{P}(X\\le t)$, ведь $\\sigma$-алгебра $\\mathcal{F}$ как раз состоит из событий, вероятности которых мы можем найти. На первый взгляд может показаться, что такое определение не полностью соотносится со словесным описанием выше, — можем ли мы найти, например, $\\mathbb{P}\\big(X\\in (0, 1)\\big)$?\n\nОказывается, что можем! Чтобы это показать, нам потребуется всего два шага:\n\n1. Для любых $a, b \\in \\mathbb{R}$, $a\u003cb$ мы можем расписать\n\n$$X^{-1}\\left( \\left(a, b\\right]\\right) = X^{-1}\\left(\\left(-\\infty, b\\right]\\right) \\setminus X^{-1}\\left(\\left(-\\infty, a\\right]\\right) \\in \\mathcal{F}\n$$\n\nТут мы воспользовались тем, что $\\mathcal{F}$ замкнута относительно операции дополнения. А это следует прямо из определения $\\sigma$-алгебры.\n\n$$A\\setminus B= \\Omega \\setminus((\\Omega\\setminus A)\\cup B)\\in \\mathcal{F}\n$$\n\nесли $A, B\\in \\mathcal{F}$.\n\n2. А теперь представим наше искомое множество в виде объединения:\n\n$$X^{-1}\\left( \\left(0, 1\\right)\\right) = \\bigcup_n X^{-1}\\left( \\left(0, 1-\\frac{1}{2^n}\\right]\\right) \\in \\mathcal{F}\n$$\n\nЗдесь мы воспользовались замкнутостью $\\mathcal{F}$ относительно счётных объединений.\n\nНа самом деле это рассуждение даёт нам формулу:\n\n$$\\mathbb{P}(X\\in(0,1)) = \\lim_{n\\to\\infty} \\mathbb{P}\\left(X\\le1- \\frac{1}{2^n}\\right) - \\mathbb{P}(X\\le 0)\n$$\n\nДальше нас будет интересовать не столько вид этой формулы, сколько факт, что $\\mathbb{P}(X\\le t)$ задают распределение $X$, то есть вероятности того, что $X$ принадлежит любому промежутку.\n\n{% endcut %}\n\nСо случайными величинами, заданными на одном вероятностном пространстве, можно делать те же самые операции, что и с числами, — их можно складывать и вычитать, можно перемножать и делить, а также можно умножать на вещественные числа и рассматривать функции от них. Например, для любой случайной величины $X$ можно рассмотреть случайные величины $X^2$ и $\\sin(X)$.\n\nНесмотря на то, что в определении случайной величины фигурирует вероятностное пространство, чаще всего нам будет интересно только её `распределение`, то есть вероятности, с которыми она принимает разные значения.\n\nЕсли этих значений конечное количество, их удобно записывать в виде `таблицы распределения`. Давайте посмотрим, как это выглядит на практике, — разберём два примера.\n\n**Пример №1**\n\nПредположим, что в упрощённой модели пользователи выставляют оценку продукту по целочисленной шкале от 1 до 6. Если дополнительных данных о пользователе и его предпочтениях у нас нет, можно считать каждую из шести оценок `равновероятной`. Пусть случайная величина $X$ — это наблюдаемый рейтинг:\n\n#|\n||\n\nx\n\n|\n\n1\n\n|\n\n2\n\n|\n\n3\n\n|\n\n4\n\n|\n\n5\n\n|\n\n6\n\n||\n||\n\n$\\mathbb{P}(X = x)$\n\n|\n\n$\\tfrac{1}{6}$\n\n|\n\n$\\tfrac{1}{6}$\n\n|\n\n$\\tfrac{1}{6}$\n\n|\n\n$\\tfrac{1}{6}$\n\n|\n\n$\\tfrac{1}{6}$\n\n|\n\n$\\tfrac{1}{6}$\n\n||\n|#\n\nФункцию $\\mathbb{P}(X = x)$, которая по значению $x$ случайной величины $X$ возвращает вероятность $\\mathbb{P}(X=x)$, называют `функцией вероятности`. В данном примере она постоянна и равна $\\tfrac{1}{6}$ для всех возможных значений рейтинга. Такая ситуация может возникать при отсутствии какой-либо априорной информации о склонностях пользователей.\n\nПодобная дискретная модель встречается в задачах анализа данных и машинного обучения, когда для оценки продукта или услуги есть ограниченное целочисленное количество баллов (ранг) и до сбора реальной статистики мы допускаем равномерное распределение этих оценок. Далее, по мере изучения поведения пользователей, можно уточнять распределение, отталкиваясь от реальных данных.\n\n**Пример №2**\n\nДавайте вспомним комбинаторику и рассмотрим следующую ситуацию. Из непрозрачной вазы, в которой лежит $4$ красных и $2$ чёрных шара, случайно вынимают два шара. Пусть $X$ — количество красных шаров среди выбранных.\n\nЭлементарные исходы в такой ситуации описываются парами различных шаров. Всего есть $C^2_{4+2}=C^2_{6}=15$ таких исходов, и они все равновероятны. Посмотрим, какая получится таблица распределения случайной величины $X$.\n\n#|\n||\n\nX\n\n|\n\n0\n\n|\n\n1\n\n|\n\n2\n\n||\n||\n\n$\\mathbb{P}$\n\n|\n\n$\\left(C^0_4\\cdot C^2_2\\right)/{C^2_6}=\\frac{1}{15}$\n\n|\n\n$\\left(C^1_4\\cdot C^1_2\\right)/{C^2_6}=\\frac{8}{15}$\n\n|\n\n$\\left(C^2_4\\cdot C^0_2\\right)/{C^2_6}=\\frac{6}{15}$\n\n||\n|#\n\nНапример, чтобы найти $\\mathbb{P}(X=1)$, мы выбираем $1$ из $4$ красных шаров и $1$ из $2$ чёрных шаров, а дальше делим получившееся количество вариантов на общее количество вариантов.\n\n![picture2.jpg](https://yastatic.net/s3/education-portal/media/5_4_1_b1788ec582.webp)\n\nЗаметим, что сумма чисел в нижней строке таблицы распределения всегда равна $1$. Это даёт ещё один способ проверить правильность её заполнения.\n\nТеперь давайте перейдём от простых примеров к описанию самых важных и часто встречающихся распределений.\n\n### Основные дискретные распределения\n\n**Биномиальное распределение**\n\nРассмотрим подкидывания нечестной монетки с вероятностью выпадения орла $p$. Распределение количества выпавших орлов в серии из $n$ подкидываний называется `биномиальным` и обозначается $Bin(n, p)$. Тут написать одну таблицу уже не получится, ведь мы рассматриваем целое `семейство распределений`. В таких случаях задают функцию вероятности формулой. Для случайной величины $X$, распределённой биномиально с параметрами $n$ и $p$, получается следующая формула:\n\n$P_X(k) = C_n^k p^k q^{n-k}, \\quad k\\ge 0,$\n\nгде $q=1-p$.\n\nБиномиальное распределение описывает количество «успехов» в серии $n$ независимых испытаний. При этом «успехом» может являться и какое-то негативное событие, например неправильная классификация объекта.\n\nТак как часто рассматривается только одно испытание, результатом которого является успех (1) или неудача (0), то для биномиального распределения с $n=1$ используется отдельное название — `распределение Бернулли`.\n\n**Распределение Пуассона**\n\nОдним из самых важных распределений является `распределение Пуассона`. Оно задаёт так называемый `простейший поток`, который используют для описания великого множества величин от количества пришедших за день в магазин покупателей до количества землетрясений за год.\n\nГоворят, что случайная величина $X$ распределена по Пуассону с `интенсивностью` $\\lambda$ (пишут $X\\sim Pois(\\lambda)$), если функция вероятности $X$ задаётся следующей формулой:\n\n$P_X(k) = \\frac{\\lambda^k}{k!}e^{-\\lambda}, \\quad k\\ge 0,$\n\nИнтенсивность потока $\\lambda$ имеет смысл среднего числа рассматриваемых событий за выбранный период.\n\nМожет показаться, что количество покупателей за день всегда меньше миллиона, и поэтому странно использовать для его описания случайную величину, которая принимает любое неотрицательное значение с некоторой положительной вероятностью. Однако на практике эти вероятности настолько малы, что ими пренебрегают.\n\nЧтобы понять причину выбора именно распределения Пуассона для описания потока событий, рассмотрим такую ситуацию. Представим, что есть онлайн-платформа, на которую каждую минуту приходят примерно $10$ новых пользователей, и каждый из них с вероятностью $1\\%$ оформляет платную подписку. Тогда количество оформлений подписки за одну минуту описывается биномиальным распределением $Bin(10, 0.01)$.\n\nА что, если нас интересует общее число платных подписок за час? Кажется естественным умножить $10$ пользователей в минуту на $60$ минут и сказать, что получаем биномиальное распределение $Bin(600, 0.01)$. Однако на практике владелец платформы не будет с полной точностью отслеживать, сколько пользователей вообще зашло в систему, а будет фиксировать лишь количество подписавшихся.\n\nЗдесь и появляется распределение Пуассона: при больших $n$ (число зашедших пользователей) и малом $p$ (вероятность оформленной подписки) биномиальное распределение становится близко к $Pois(\\lambda)$, где $\\lambda = n \\cdot p$.\n\nА если мы знаем только интенсивность $\\lambda$, то гораздо удобнее описать число платных подписок именно пуассоновским распределением. Использованное здесь утверждение строго описывается следующей теоремой.\n\n**Теорема (Пуассон)**\n\nПусть между параметрами $n$ и $p$ биномиальных случайных величин $X_n\\sim Bin(n,p)$ есть такая зависимость, что $np\\longrightarrow\\lambda\\in(0,+\\infty)$ при $n\\longrightarrow \\infty$. Тогда для любого фиксированного $k\\in\\mathbb{N}$ справедливо следующее приближение:\n\n$$\\mathbb{P}(X_n=k)=C_n^k p^k q^{n-k}\\longrightarrow \\frac{\\lambda^k}{k!}e^{-\\lambda}\n$$\n\nпри $n\\longrightarrow\\infty$. В нашем примере интенсивность $\\lambda=600\\cdot0.01=6$, то есть получается $Pois(6)$. Сравним распределения наглядно с помощью следующего графика.\n\n![picture3.png](https://yastatic.net/s3/education-portal/media/5_4_2_3e79487805.webp)\n\nВидим, что никакой заметной разницы нет.\n\nРаспределение Пуассона широко используется при анализе интенсивности событий — например, в системах мониторинга (количество запросов за секунду), в предсказании количества сбоев или отказов в системах, а также в маркетинговой аналитике (число покупок за час). Оно лежит в основе целого класса статистических моделей для данных, в которых искомая величина может принимать только неотрицательные целочисленные значения.\n\nВ частности, в машинном обучении и статистике существует модель `Poisson regression` (пуассоновская регрессия). Её ключевая идея — предположить, что для объекта с признаками $x$ `ожидаемое` значение целевой переменной $Y$, которая чаще всего описывает количество чего-нибудь, задаётся функцией $\\lambda(x)$ вида:\n\n$\\log(\\lambda(x)) \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_1 \\;+\\; \\dots \\;+\\; \\beta_k x_k,$\n\nто есть $\\lambda(x) = \\exp(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k).$\n\nТаким образом, целевая переменная $Y$ при заданных значениях признаков $x$ считается распределённым по Пуассону с параметром $\\lambda(x)$. В задаче обучения ($\\beta_0,\\beta_1,\\dots,\\beta_k$) подбираются так, чтобы максимизировать правдоподобие наблюдаемых данных (или минимизировать соответствующую функцию потерь). Эта модель оказывается особенно полезной, когда мы пытаемся прогнозировать количество кликов, просмотров, обращений, сбоев и прочих счётных характеристик по набору объясняющих факторов.\n\n**Геометрическое распределение**\n\nИ вот ещё один важный пример дискретного распределения. Представим, что у нас снова происходит некоторая серия независимых испытаний с вероятностью успеха $p$, но теперь вместо количества успехов нам интересно $X$ — количество неуспешных испытаний до первого успеха. Давайте найдём функцию распределения такой случайной величины.\n\n$\\mathbb{P}(X=0)=p$ — это вероятность сразу же получить успех. $\\mathbb{P}(X=1)$ — это вероятность в первом испытании получить неудачу, а во втором — успех. То есть $\\mathbb{P}(X=1)=p(1-p)$. Аналогичные рассуждения показывают, что:\n\n$$P_X(k) = \\mathbb{P}(X=k) = p\\cdot (1-p)^k\n$$\n\nПроводя такие рассуждения, удобно держать в уме следующее дерево вариантов:\n\n![picture1.jpg](https://yastatic.net/s3/education-portal/media/5_4_3_793ebd44ce.webp)\n\nТакое распределение называется `геометрическим` и обозначается $Geom(p)$. Обратите внимание, что в некоторых источниках принято считать количество испытаний, включая последний успех, то есть все значения на 1 больше, чем у нас.\n\nПрименение геометрического распределения может встречаться в задачах анализа данных, где нас интересует, сколько раз подряд мы увидим некий неудачный исход, прежде чем случится первый успех. К примеру, в моделировании того, сколько пользователей подряд не совершат целевого действия (покупки, клика), прежде чем случится первый положительный результат.\n\n### Функция распределения\n\nЕщё одним удобным инструментом для работы с распределением любой случайной величины $X$ является `функция распределения` $F_X$. Она по числу $t$ выдаёт вероятность того, что $X$ не больше $t$.\n\n$$F_X(t)=\\mathbb{P}(X\\le t)\n$$\n\nШирокое применение этой функции обусловлено тем, что с её помощью удобно выражать вероятности принадлежности величины $X$ произвольному промежутку. А такие вероятности постоянно приходится находить на практике.\n\nДля примера с выбором шаров из вазы получается следующий график функции распределения.\n\n![picture4.png](https://yastatic.net/s3/education-portal/media/5_4_4_84d6bf0e42.webp)\n\nОтметим отдельно, что, в отличие от функции вероятности, функцию распределения рассматривают для произвольных случайных величин.\n\n---\n\nВ этом параграфе вы познакомились с ключевыми понятиями, связанными с дискретными случайными величинами, а также научились работать с их распределениями. Это создаёт фундамент для изучения более сложных вероятностных моделей и алгоритмов, которые мы рассмотрим в дальнейших главах хендбука.\n\nМногие процессы можно выразить через количество чего-либо: будь то число клиентов, очков или билетов, — для этого подойдут уже знакомые вам дискретные модели. Однако в жизни встречается немало величин, которые не *считаются*, а *измеряются*. Например, расстояние до цели, уровень шума или температура.\n\nТакие характеристики требуют более гибкого подхода к описанию вероятностей. Именно этим и занимаются модели `непрерывных случайных величин`, с которых мы начнём следующий параграф.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13776235.aee75677620a297f1d255e9a2a3308bf20c7fbad?iframe=1\" frameborder=\"0\" name=\"ya-form-13776235.aee75677620a297f1d255e9a2a3308bf20c7fbad\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"7b:T8d3b,"])</script><script nonce="">self.__next_f.push([1,"В предыдущем параграфе вы познакомились с дискретными случайными величинами — величинами, которые принимают конечное или счётное множество значений. Научились описывать их распределения, считать вероятности, а также познакомились с примерами таких распределений: биномиальным, геометрическим и распределением Пуассона.\n\nВ этом параграфе мы рассмотрим `непрерывные случайные величины` — такие, которые могут принимать бесконечно много значений из некоторого интервала. Их важнейшее свойство состоит в том, что вероятность получить любое конкретное значение равна нулю, и вместо перечисления вероятностей мы используем функции плотности.\n\nНепрерывные случайные величины описывают характеристики реального мира, значения которых `измеряются`, а не считаются — например, температуру воздуха, уровень шума, расстояние до объекта, время ожидания автобуса или уровень сахара в крови.\n\nРазобравшись с такими величинами, вы сможете уверенно использовать их в регрессионных моделях, анализе временных рядов, обработке и интерпретации данных, полученных с физических или медицинских приборов.\n\n### Непрерывные величины и плотность вероятности\n\nНа самом деле мы уже встречали непрерывные случайные величины. Рассмотренная ранее модель геометрической вероятности для отрезка $[a,b]$ задаёт распределение, которое мы называем равномерным $Unif([a,b])$.\n\nДавайте посмотрим на него с новой, более общей, точки зрения. Рассмотрим случайную величину $X\\sim Unif([a,b])$ и распишем вероятность события $X\\in A \\subseteq [a,b]$ следующим образом:\n\n$$\\mathbb{P}(X\\in A)= \\frac{\\mu(A)}{\\mu(\\Omega)} = \\int_A\\frac{1}{b-a} dx.\n$$\n\nПолучаем, что у нас есть постоянная функция $\\frac{1}{b-a}$ и вероятность того, что значение $X$ попадёт в $A$, равна интегралу от этой функции по $A$. Чтобы не ограничиваться $A\\subseteq [a,b]$, $p_X(x)$ — `плотность вероятности` $X$ задают следующей формулой:\n\n$$p_X(x) = \\begin{cases}\\frac{1}{b-a}, \u0026 x \\in [a,b]\\\\ 0, \u0026   \\text{иначе} \\end{cases}\n$$\n\nФормально для произвольной случайной величины $X$ `плотность вероятности` $p_X(x)$  — это просто функция, такая что для любого (измеримого) множества $A$ выполняется:\n\n$$\\mathbb{P}(X\\in A)= \\int_A p_X(x) dx\n$$\n\nНепрерывные случайные величины определяют как случайные величины, у которых есть плотность вероятности. Это согласовано со свойством выше, — если у $X$ есть плотность вероятности, то:\n\n$$\\mathbb{P}(X=t)=\\int_{\\{t\\}}p_X(x)dx = 0\n$$\n\nДавайте посмотрим на примеры. Попробуем построить распределение случайной величины $X$ со значениями на $[0, 1]$, у которой плотность будет линейной. То есть:\n\n$$p_X(x) = \\begin{cases}C\\cdot x, \u0026 x \\in [0,1]\\\\ 0, \u0026   \\text{иначе} \\end{cases}\n$$\n\nТак как всегда $\\mathbb{P}(X\\in (-\\infty, \\infty))=1$, плотность $p_X(x)$ должна удовлетворять следующему условию:\n\n$$\n\\int_{-\\infty}^{+\\infty} p_X(x) \\, dx = C \\cdot \\left. \\frac{x^2}{2} \\right|_0^1 = \\frac{C}{2} = 1\n$$\n\n\nТо есть в нашей плотности $C=2$. Мы не будем это обосновывать, но для любой неотрицательной функции $p(x)$, для которой выполняется условие ниже, на самом деле существует вероятностное пространство и случайная величина на нём с такой плотностью:\n\n$$\n\\int_{-\\infty}^{+\\infty} p(x) \\, dx = 1\n$$\n\n\nЭто означает, что любой желающий может построить сколько угодно непрерывных распределений.\n\nНо главный вопрос — насколько они полезны и для каких прикладных задач применимы. Поэтому давайте рассмотрим их подробнее и начнём с  экспоненциального распределения.\n\n### Экспоненциальное распределение\n\nРаньше мы уже обсуждали, что распределение Пуассона описывает количество событий в простейшем потоке. Теперь, познакомившись с непрерывными распределениями, мы готовы описать, как распределено $Y$ — время между любыми последовательными событиями потока. Например, между приездом двух машин на заправку или между двумя звонками в колл-центр.\n\nЭту случайную величину $Y$ описывают `экспоненциальным` распределением, у которого следующая плотность вероятности:\n\n$$p_Y(x) = \\begin{cases}\\lambda \\cdot e^{-\\lambda x}, \u0026 x\\ge 0 \\\\ 0, \u0026 x\u003c0 \\end{cases}.\n$$\n\nЭто распределение обозначается $Exp(\\lambda)$. Таким образом, в простейшем потоке интенсивности $\\lambda$ среднее время между событиями описывается $Y\\sim Exp(\\lambda)$, а количество событий за единичный промежуток времени описывается $X\\sim Pois(\\lambda)$.\n\nДовольно естественно спросить, сколько в среднем проходит времени между событиями в простейшем потоке. Чтобы ответить на этот и аналогичные вопросы о средних значениях других случайных величин, ввели понятие `математического ожидания`.\n\n### Математическое ожидание\n\n`Математическим ожиданием` (или коротко — `матожиданием`) непрерывной случайной величины $X$ (или, точнее, её распределения) называется следующее число:\n\n$$\\mathbb{E}X= \\int_{-\\infty}^{\\infty}x\\cdot p_X(x)dx\n$$\n\nЕго можно интерпретировать как среднее значение $X$, где для усреднения используется плотность вероятности.\n\nДавайте посмотрим, какое получится среднее время между событиями в простейшем потоке. Пусть $Y\\sim Exp(\\lambda)$, тогда по формуле интегрирования по частям имеем:\n\n$\\mathbb{E}Y= \\int_{-\\infty}^{\\infty}x\\cdot p_Y(x)dx = \\int_{0}^{\\infty}x\\lambda e^{-\\lambda x} dx = -x e^{-\\lambda x}\\Big |_0^{\\infty}+\\int_0^{\\infty}e^{-\\lambda x}dx= -\\frac{1}{\\lambda}e^{-\\lambda x}\\Big |_0^{\\infty} = \\frac{1}{\\lambda}$\n\nТо есть абсолютно логично, что $\\lambda$ -событиям за единицу времени соответствует среднее время между событиями $\\frac{1}{\\lambda}$.\n\nОпределение, данное выше, пригодно только для непрерывных случайных величин. Для дискретной случайной величины $X$ `матожидание` определяется похоже:\n\n$$\\mathbb{E}X = \\sum_x x\\cdot P_X(x)\n$$\n\nЗдесь вместо интеграла стоит сумма, которая всегда конечна или счётна, а вместо плотности распределения стоит функция вероятности.\n\nДавайте посмотрим, сколько в среднем раз подряд выпадает решка при подкидывании нечестной монетки. То есть найдём $\\mathbb{E}X$ для $X\\sim Geom(p)$. Посчитать эту сумму можно несколькими способами.\n\nЧтобы не привлекать слишком много математического анализа, давайте выразим её через саму себя.\n\n$$\\mathbb{E}X = \\sum_{k\\ge 0} k\\cdot p \\cdot(1-p)^k = \\sum_{k\\ge 1} \\cdot p \\cdot(1-p)^k + \\sum_{k\\ge 1} (k-1)\\cdot p \\cdot(1-p)^k  =\n$$\n\n$$=(1-p) + \\sum_{l\\ge 0} l \\cdot p \\cdot(1-p)^{l+1}=(1-p)+(1-p)\\mathbb{E}X,\n$$\n\nоткуда находим $\\mathbb{E}X = \\frac{1-p}{p}$. В преобразованиях выше мы воспользовались суммой геометрической прогрессии и сделали замену $[l=k-1]$.\n\nОсновным свойством матожидания является его линейность. Это означает, что для `любых` случайных величин $X$ и $Y$ и любых чисел $a$ и $b$:\n\n$$\\mathbb{E}(aX+bY)=a\\cdot \\mathbb{E}X + b\\cdot\\mathbb{E}Y\n$$\n\n{% cut \"Доказательство для дискретного случая\" %}\n\n$\\mathbb{E}(aX+bY)=\\sum_{t}t\\cdot  \\mathbb{P}(aX+bY=t)= \\sum_x\\sum_y (ax+by)\\mathbb{P}(X=x, Y=y)=\\sum_x\\sum_y ax\\mathbb{P}(X=x, Y=y)+ \\sum_x\\sum_y by\\mathbb{P}(X=x, Y=y)=\\sum_x ax\\mathbb{P}(X=x)+\\sum_y by\\mathbb{P}(Y=y)= a\\cdot  \\mathbb{E}X + b\\cdot\\mathbb{E}Y$\n\nЗдесь мы в начале разбили событие $aX+bY=t$ на более мелкие, а в конце, наоборот, собрали события $X=x$ и $Y=y$ из частей.\n\n{% endcut %}\n\nКроме матожидания самой случайной величины $X$, часто рассматривают матожидания $\\mathbb{E}f(X)$, где $f:\\mathbb{R}\\to \\mathbb{R}$ — некоторая функция. Мы уже говорили, что $f(X)$ — это новая случайная величина и $\\mathbb{E}f(X)$ — это её математическое ожидание. Важным и очень полезным фактом является то, что для нахождения $\\mathbb{E}f(X)$ достаточно знать только распределение $X$. Для непрерывной случайной величины $X$ можно использовать следующую формулу:\n\n$$\\mathbb{E}f(X)=\\int_{-\\infty}^{+\\infty}f(x)p_X(x)dx.\n$$\n\nВ дискретном случае формула имеет следующий вид:\n\n$$ \\mathbb{E}X = \\sum_x f(x)P_X(x).\n$$\n\nИногда функция $f$ не просто задаёт преобразование данных, а описывает некоторую важную характеристику самой случайной величины. Давайте посмотрим, как это бывает, на примере следующего важного понятия  — `дисперсии`.\n\n{% cut \"Матожидание не всегда существует. Распределение Коши.\" %}\n\nНа самом деле не у всякой случайной величины есть матожидание. Это связано с тем, что интеграл/сумма в определении матожидания могут расходиться. Стандартным примером здесь является распределение Коши $C(x_0,\\gamma)$, которое имеет плотность\n\n$$p_{C(x_0,\\gamma)}(x) = \\frac{1}{\\pi}\\cdot \\frac{\\gamma}{(x-x_0)^2+\\gamma^2},\n$$\n\nгде $x_0$ — центр распределения, который может принимать любьое значение, а $\\gamma\u003e0$ — параметр масштаба.\n\nНесмотря на то, что такое распределение имеет центр симмутрии — точку $x_0$, у него нет матожидания, так как интеграл\n\n$$\\int_{-\\infty}^{+\\infty}\\frac{1}{\\pi}\\cdot \\frac{x\\cdot \\gamma}{(x-x_0)^2+\\gamma^2}dx\n$$\n\nрасходится.\n\n{% endcut %}\n\n### Дисперсия и среднеквадратичное отклонение\n\nКроме среднего значения случайной величины $X$ часто интересно, насколько сильно и насколько часто она отклоняется от своего среднего значения. Именно это измеряет `дисперсия`:\n\n$$\\mathbb{D}X:=\\mathbb{E}(X-\\mathbb{E}X)^2,\n$$\n\nТо есть это матожидание квадрата отклонения от матожидания. Обратите внимание, что матожидание в правой части берётся от квадрата выражения в скобках.\n\nДля дисперсии есть следующая формула, которая часто оказывается удобной.\n\n$$\\mathbb{D}X:=\\mathbb{E}(X-\\mathbb{E}X)^2= \\mathbb{E}(X^2-2X\\cdot\\mathbb{E}X+(\\mathbb{E}X)^2)=\\mathbb{E}X^2 - 2 (\\mathbb{E}X)(\\mathbb{E}X)+(\\mathbb{E}X)^2= \\mathbb{E}X^2 - (\\mathbb{E}X)^2\n$$\n\nЗдесь мы воспользовались только линейностью матожидания. Давайте посмотрим, как эта формула применяется на примере.\n\nПусть $X$ — количество выпавших очков на игральному кубике. Тогда:\n\n$$\\mathbb{E}X = \\frac{1+2+3+4+5+6}{6}=\\frac{7}{2} \\\\\n\n\\mathbb{E}X^2 = \\frac{1^2+2^2+3^2+4^2+5^2+6^2}{6}=\\frac{91}{6} \\\\\n\n\\mathbb{D}X = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 = \\frac{91}{6} - \\frac{49}{4}=\\frac{35}{12}\n$$\n\nВидим, что среднее количество выпавших очков равно $3.5$, что довольно естественно. А как проинтерпретировать полученное значение дисперсии? Если бы мы сравнивали два разных распределения, то после сравнения значений их дисперсий можно было бы сказать, в каком из них значения чаще лежат ближе к среднему значению, а в каком из них — дальше от него.\n\nОднако для одного распределения дисперсия сама по себе не очень информативна. Это вызвано отчасти тем, что из-за присутствия квадрата в формуле дисперсия $\\mathbb{D}X$ измеряется не в тех же величинах, что сама случайная величина $X$. В примере выше $\\mathbb{D}X=\\frac{35}{12}$ очков в квадрате.\n\nПоэтому вместо дисперсии часто рассматривают `среднеквадратичное отклонение`:\n\n$$\\sigma(X) := \\sqrt{\\mathbb{D}X},\n$$\n\nОно измеряется в тех же величинах, что и $X$.\n\nИ вот теперь мы готовы рассказать про главного героя этого параграфа и, возможно, всего урока.\n\n### Нормальное распределение\n\nХотя формула плотности нормального распределения может показаться довольно причудливой и сложной, это распределение на практике встречается чаще всего. Потому что сумма большого числа случайных величин имеет примерно нормальное распределение.\n\nСтрогую формулировку этого утверждения мы дадим в параграфе про предельные теоремы, а сейчас давайте опишем нормальное распределение.\n\nГоворят, что случайная величина $X$ распределена нормально с матожиданием $\\mu$ и дисперсией $\\sigma^2$, если её плотность имеет следующий вид:\n\n$$p_X(x) = \\frac{1}{\\sqrt{2\\pi}\\cdot \\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\nЭто распределение обозначается $N(\\mu, \\sigma^2)$. Давайте на его примере посмотрим, какой смысл имеет среднеквадратичное отклонение  $\\sigma$.\n\n![image](https://yastatic.net/s3/education-portal/media/5_5_1_e6078b52cf.webp)\n\nТо есть вероятность, что нормальная случайная величина отклонится от матожидания не больше чем на $\\sigma$, примерно равна $68\\%$, на $2\\sigma$ — $95\\%$, а не более $3\\sigma$ — уже больше $99\\%$. Этот статистический факт называют ещё «правило трёх сигм», он позволяет в уме выполнять прикидки в математической статистике.\n\nДля нормального распределения уже нетривиально проверить даже, что интеграл $p_X(x)$ по всей числовой прямой равен 1, то есть что указанная плотность действительно задаёт распределение.\n\n{% cut \"Коротко про вычисление интегралов с $e^{-x^2/2}$.\" %}\n\nПри вычислении интегралов с плотностью нормального распределения стандартным первым шагом является замена $\\left[x'=\\frac{x-\\mu}{\\sigma}\\right]$. После неё остаётся только $e^{-x^2/2}$ с некоторой константой перед ней.\n\nДля начала найдём интеграл от этой функции по всей числовой прямой. Чтобы это сделать, обозначим его как $I$, перемножим два таких интеграла по разным переменным $x$ и $y$, а затем перейдём к полярным координатам.\n\n$\\int_{-\\infty}^ {+\\infty}e^{-x^2/2}dx = I$\n\n$I^2 = \\int_{-\\infty}^ {+\\infty}e^{-x^2/2}dx \\int_{-\\infty}^ {+\\infty}e^{-y^2/2}dy = \\int_{-\\infty}^ {+\\infty}\\int_{-\\infty}^ {+\\infty}e^{-(x^2+y^2)/2}dxdy=\\int_{0}^ {2\\pi}\\int_{0}^ {+\\infty}e^{-r^2/2} r dr d\\varphi = [t=r^2/2]= 2\\pi \\int_0^{+\\infty}e^{-t}dt = 2\\pi$\n\nТо есть $I=\\sqrt{2\\pi}$ , и становится понятно, откуда появился такой коэффициент в плотности нормального распределения.\n\n**Упражнение:** с помощью интегрирования по частям и замен переменной найдите следующие интегралы:\n\n$\\int_{-\\infty}^ {+\\infty}x\\cdot e^{-x^2/2}dx, \\quad \\int_{-\\infty}^ {+\\infty}x^2\\cdot e^{-x^2/2}dx$\n\nи проверьте, что математическое ожидание и дисперсия $X\\sim N(\\mu, \\sigma^2)$ действительно равны $\\mu$ и $\\sigma^2$.\n\n{% cut \"Ответы к упражнению\" %}\n\n$\\int_{-\\infty}^ {+\\infty}x\\cdot e^{-x^2/2}dx=0, \\quad \\int_{-\\infty}^ {+\\infty}x^2\\cdot e^{-x^2/2}dx=\\sqrt{2\\pi}$\n\n{% endcut %}\n\n{% endcut %}\n\nС помощью нормального распределения описывается великое множество абсолютно разных величин. От роста взрослых людей и артериального давления до количества осадков и ошибок предсказаний регрессионных моделей. Нормально распределённые характеристики постоянно встречаются в датасетах.\n\nКроме этого, нормальное распределение встречается не только в данных, но и в самих моделях. Например, веса в нейронных сетях инициализируют малыми значениями либо из равномерного, либо из нормального распределения. Вариационные автоэнкодеры — одни из лучших способов генерации изображений — пытаются свести описание изображения к набору нормально распределённых характеристик. Причём делают это таким образом, что любые другие значения этих характеристик будут давать новые разумные изображения.\n\nРассмотренные выше примеры — это только вершина айсберга. Но для описания всех применений нормального распределения пришлось бы выделить отдельный параграф.\n\nЧтобы работать с непрерывными распределениями на практике, особенно при анализе данных и прогнозировании, нам нужно уметь находить не только плотности, но и вероятности попадания случайной величины в заданный интервал. Для этого удобно использовать функцию распределения, которую мы уже рассматривали ранее для дискретных случайных величин.\n\n### Функция распределения непрерывных случайных величин\n\nДля непрерывных величин функция распределения — это гораздо более важный объект, чем для дискретных. Потому что для проведения любых статистических тестов необходимо вычислять значения функции распределения и обратной к ней функции.\n\nДля нормального и нескольких других распределений раньше использовали большие таблицы этих значений. Сейчас достаточно двух строчек кода, чтобы найти их с гораздо большей точностью.\n\nЕщё одна особенность непрерывных случайных величин — это наличие следующей простой связи между функцией распределения и плотностью распределения:\n\n$$p_X(x) = F_X'(x)\n$$\n\nЭта связь на самом деле не что иное, как формула дифференцирования интеграла по верхнему пределу.\n\nНа практике часто возникает необходимость изменить распределение случайной величины. Это может потребоваться, чтобы сделать данные более *удобными* для анализа, привести их к нормальному виду или, например, устранить смещения и асимметрии. Такие преобразования особенно актуальны в задачах, где нужно предсказывать одну величину по другим, — и именно с этим мы сейчас разберёмся.\n\n### Трансформации случайных величин\n\nДавайте представим, что мы хотим предсказывать цены на недвижимость $Y$ по уровню преступности, доле жилой застройки, уровню загрязнения воздуха и другим характеристикам $X_i$.\n\nПускай мы построили гистограммы, чтобы прикинуть, какие распределения описывают каждую из них, и получили совсем разные результаты. Например, цена на недвижимость распределена нормально, уровень преступности — экспоненциально, а доля жилой застройки — равномерно.\n\nКажется, что самые простые модели, которые будут пытаться выразить цену в виде линейной комбинации остальных характеристик, тут неприменимы. Ведь вряд ли связь нормально распределённого $Y$ с экспоненциальным распределением линейна. Однако данные можно предобработать — заменить каждую характеристику $X_i$ на $f(X_i)$ так, чтобы такая связь могла быть линейной. Обсудим, как это делать.\n\nРассмотрим непрерывную случайную величину $X$, монотонную функцию $f:\\mathbb{R}\\to\\mathbb{R}$ и попробуем найти плотность распределения $f(X)$. Для этого распишем:\n\n$$p_{X}(x)=F_{X}'(x)=\\mathbb{P}(X \\le x)'=\\mathbb{P}(f(X)\\le f(x))'=F_{f(X)}(f(x))' = f'(x)\\cdot F_{f(x)}'(f(x)) = f'(x)\\cdot p_{f(X)}(f(x))\n$$\n\nПолучившееся равенство проще запомнить, если написать его с помощью дифференциалов:\n\n$$p_X(x)dx = p_{f(X)}(f(X))\\cdot df(x)= p_{f(X)}(f(X))\\cdot f'(x) dx\n$$\n\nДля того чтобы превратить распределение $X$ в какое-то другое $X'$, можно взять $f(x) = F_{X'}^{-1}(F_X(x))$. Но на практике часто используют более простые $f$, которые выполняют примерно такое же преобразование.\n\nМы увидели, что с помощью преобразований можно привести случайную величину к более удобному виду — например, чтобы использовать простые линейные модели или сравнивать данные с теоретическим распределением.\n\nНо даже такие преобразования не всегда позволяют описать данные одной функцией плотности: иногда в выборке скрываются разные типы наблюдений, которые подчиняются различным законам. В таких случаях используют `смесь распределений` — подход, который объединяет несколько распределений в единую модель.\n\n### Смеси распределений\n\nНа практике часто оказывается, что распределение некоторой характеристики не получается описать ни одним из рассмотренных распределений. Например, рассмотрим [датасет](https://www.kaggle.com/datasets/satvicoder/call-center-data/data) звонков в кол-центр, в котором собрана статистика звонков по дням.\n\nДавайте посмотрим, как распределено количество входящих звонков.\n\n![image](https://yastatic.net/s3/education-portal/media/5_5_2_c404ada8d6.webp)\n\nТакой вид гистограммы может свидетельствовать о разнородности объектов — объекты (то есть дни), скорее всего, можно разделить на некоторые группы так, что внутри каждой группы значения выбранной характеристики будут описываться «чистыми» распределениями.\n\nС интуитивной точки зрения, смесь распределений возникает, когда данные формируются под действием нескольких различных причин. Например, количество звонков в колл-центр может по-разному распределяться в зависимости от дня недели: в будние дни много обращений, в выходные — значительно меньше.\n\nКроме того, бывают исключительные дни, когда резко возрастает число звонков — например, из-за технической аварии, рекламной кампании или неполадок в системе. Такие выбросы тоже можно считать результатом «включения» другого распределения с высокой интенсивностью. Таким образом, каждое наблюдение — это результат работы одного из нескольких сценариев, и модель смеси позволяет это учесть.\n\nВ примере выше подавляющее большинство дней с количеством звонков меньше 80 приходится на один и тот же день недели. То есть данные логично описать как «смесь» двух распределений Пуассона (о них мы говорили в предыдущем параграфе) с разными интенсивностями в пропорции $1:6$ — шесть обычных будних дней и один более «спокойный» выходной.\n\nДавайте опишем это формально. Пусть у нас есть случайные величины $X_1, X_2, \\ldots, X_k$ и вероятности выбора каждой из них $q_1, q_2, \\ldots ,q_k$, причём  $q_1+q_2+\\ldots +q_k=1$. Чтобы получить значение `смеси` этих случайных величин $X$, мы случайно с вероятностями $q_i$ выбираем одну из $X_i$ и берём её значение.\n\nНесложно проверить, что плотность распределения $X$ можно найти как:\n\n$$p_X(x) = q_1 \\cdot p_{X_1}(x) + q_2\\cdot  p_{X_2}(x) +\\ldots + q_k \\cdot p_{X_k}(x)\n$$\n\n![image](https://yastatic.net/s3/education-portal/media/5_5_3_50f29e476d.webp)\n\nНа рисунке выше показано, как выглядит плотность смеси трёх нормальных распределений. Ещё раз видим, что количество «горбов» в общем и целом совпадает с количеством «чистых» распределений в смеси.\n\nНапоследок стоит сказать, что в функциональном анализе есть [понятие](https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D0%BE%D0%B1%D1%89%D1%91%D0%BD%D0%BD%D0%B0%D1%8F_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F) обобщённых функций, которое позволяет единообразно описывать как непрерывные, так и дискретные случайные величины. Но оно, к сожалению, далеко выходит за рамки нашего хендбука.\n\n---\n\nВ этом параграфе мы изучили непрерывные случайные величины — математические модели для измеримых характеристик, таких как время, расстояние и температура. Мы узнали, как с помощью плотности вероятности вычислять вероятности, как находить математическое ожидание, дисперсию и другие важные числовые характеристики.\n\nТакже мы рассмотрели ключевые непрерывные распределения, такие как равномерное, экспоненциальное и нормальное, и научились работать с функцией распределения, выполнять преобразования случайных величин и описывать сложные явления через смеси распределений.\n\nЭти идеи лежат в основе современных статистических моделей и алгоритмов машинного обучения. Умение работать с непрерывными величинами позволяет не просто анализировать данные, а по-настоящему понимать закономерности, которые за ними скрываются, — будь то поведение пользователей, параметры физических процессов или структура шума в изображениях.\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13776229.deb648892a72c08cecb56d3f70f05efe18de1af0?iframe=1\" frameborder=\"0\" name=\"ya-form-13776229.deb648892a72c08cecb56d3f70f05efe18de1af0\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"7c:T91fb,"])</script><script nonce="">self.__next_f.push([1,"В предыдущих параграфах мы познакомились со случайными величинами и научились описывать их поведение через распределения, математическое ожидание и дисперсию. Но в реальных прикладных задачах мы редко работаем с одной случайной величиной.\n\nОбычно мы имеем дело с несколькими характеристиками, которые могут быть зависимыми. Например:\n\n* Количество заказов и выручка за день.\n* Время, проведённое пользователем на сайте, и вероятность клика.\n* Возраст клиента и средний чек.\n\nТакие случайные величины необходимо анализировать *совместно*, чтобы улавливать и количественно описывать связи между ними. Для этого мы рассмотрим совместное распределение случайных величин.\n\n## Дискретный случай\n\nЕсли $X$ и $Y$ — дискретные случайные величины, то их совместное распределение задаётся `таблицей совместного распределения`:\n\n$$P_{X,Y}(x,y)= \\mathbb{P}(X = x, Y = y)\n$$\n\nНапример, пусть $X$ — количество просмотренных пользователем страниц ($1$, $2$ или $3$), а $Y$ показывает совершил ли пользователь покупку: $0$, если не совершил, и $1$, если совершил. Рассмотрим `таблицу совместного распределения` $X$ и $Y$:\n\n#|\n||\n\n\n|\n\n**Y=0**\n\n|\n\n**Y=1**\n\n||\n||\n\nX=1\n\n|\n\n0\\.4\n\n|\n\n0\\.1\n\n||\n||\n\nX=2\n\n|\n\n0\\.15\n\n|\n\n0\\.15\n\n||\n||\n\nX=3\n\n|\n\n0\\.05\n\n|\n\n0\\.15\n\n||\n|#\n\nИз этой таблицы видно, как связаны значения $X$ и $Y$. Например, мы можем вычислить следующие вероятности:\n\n$$\\mathbb{P}(Y=1|X=1)=\\frac{0.1}{0.4+0.1}=0.2,\n$$\n\n$$\\mathbb{P}(Y=1|X=2)=\\frac{0.15}{0.15+0.15}=0.5,\n$$\n\n$$\\mathbb{P}(Y=1|X=3)=\\frac{0.15}{0.05+0.15}=0.75.\n$$\n\nОтсюда мы можем сделать вывод, что пользователи, посетившие большее число страниц, гораздо более склонны сделать покупку.\n\nС помощью этой таблицы также несложно найти `маргинальные распределения`, то есть распределения каждой случайной величины по отдельности — для этого надо просуммировать значения по строкам или по столбцам:\n\n$$P_X(x) = \\sum_y P_{X,Y}(x, y), \\quad P_Y(y) = \\sum_x P_{X,Y}(x, y).\n$$\n\nНапример, вероятность того, что $X$ принимает значение $1$, равна:\n\n$$P_X(1) = P_{X, Y}(1,0) +P_{X, Y}(1,1) = 0.4+0.1 = 0.5. \n$$\n\n## Непрерывный случай\n\nПусть мы снова анализируем пользовательское поведение, но теперь нас интересуют следующие случайные величины:\n\n* $X$ — время, проведенное пользователем на сайте.\n* $Y$ — сумма покупок.\n\nДля простоты рассмотрим только покупателей, которые совершили покупку. Тогда обе случайные величины являются непрерывными. Как и в дискретном случае, нас будет интересовать их совместное распределение. Теперь давайте более формально.\n\nЕсли $X$ и $Y$ — непрерывные случайные величины, то их совместное распределение описывается `совместной плотностью распределения` $p_{X,Y}(x, y)$, а вероятности событий можно вычислить по следующей формуле:\n\n$$\\mathbb{P}((X,Y) \\in A) = \\int\\limits_A p_{X,Y}(x,y)\\,dxdy. \n$$\n\nДля получения `маргинальной плотности` $X$ используется формула:\n\n$$p_X(x) = \\int_{-\\infty}^{\\infty} p_{X,Y}(x,y)\\,dy \n$$\n\nИ аналогично для $p_Y(y)$. Эти формулы — непрерывный аналог суммирования по строкам или столбцам. В нашем примере они позволяют определить, как распределены время пребывания на сайте и сумма покупок для всех пользователей.\n\n#### Пример\n\nДавайте разберём всё на конкретных числах. Предположим, что совместная плотность времени $X$ (в часах) и суммы покупки $Y$ (в сотнях долларов) для нового пользователя задаётся формулой:\n\n$$p_{X,Y}(x,y) = \n\\begin{cases} \nx + y, \u0026 \\text{если } 0 \\leq x \\leq 1, 0 \\leq y \\leq 1 \\\\ \n0, \u0026 \\text{в ином случае} \n\\end{cases} $$\n\n\u003e 💡Примечание: константа здесь уже подобрана так, чтобы интеграл от плотности был равен $1$:\n\u003e\n\u003e $$ \\int_0^1\\!\\!\\int_0^1 (x+y)\\,dx\\,dy\n\u003e =\\int_0^1\\!\\Bigl(\\tfrac{x^2}{2}+\\tfrac{x}{2}\\Bigr)\\Big|_{0}^{1}dy\n\u003e =\\int_0^1 1\\,dy=1.\n\u003e $$\n\u003e \n\u003e Это обязательное условие для любой функции плотности вероятности.\n\n**Шаг 1.** Найдём маргинальную плотность для времени $X$.\n\nЧтобы понять, как распределено только время на сайте (без учёта суммы покупки), проинтегрируем совместную плотность по всем возможным значениям $y$:\n\n$$ p_X(x) = \\int_0^1 (x+y) \\, dy = \\left[ xy + \\frac{y^2}{2} \\right]_{y=0}^{y=1} = x + \\frac{1}{2}, \\quad \\text{для } x \\in [0,1]. \n$$\n\nПолученная плотность $p_X(x) = x + \\tfrac{1}{2}$ показывает, что вероятность провести на сайте больше времени выше, чем вероятность провести на нём меньше времени (так как функция линейно возрастает). Другими словами, плотность $p_X(x)$ линейно возрастает, значит, большие значения $X$ встречаются чаще малых.\n\n**Шаг 2.** Вычислим конкретную вероятность.\n\nКакова вероятность того, что пользователь проведёт на сайте меньше получаса ($X \u003c 0.5$) и потратит меньше 50 долларов ($Y \u003c 0.5$)? Для этого нужно взять двойной интеграл по соответствующей области:\n\n$$\\mathbb{P}(X \u003c 0.5, Y \u003c 0.5) = \\int_0^{0.5} \\int_0^{0.5} (x+y) \\, dx \\, dy \n$$\n\nСначала интегрируем по $x$:\n\n$$\\int_0^{0.5} (x+y) \\, dx = \\left[ \\frac{x^2}{2} + yx \\right]_{x=0}^{x=0.5} = \\frac{0.5^2}{2} + 0.5y = 0.125 + 0.5y \n$$\n\nТеперь результат интегрируем по $y$:\n\n$$\\int_0^{0.5} (0.125 + 0.5y) \\, dy = \\left[ 0.125y + 0.5 \\frac{y^2}{2} \\right]_{y=0}^{y=0.5} = \n$$\n\n$$= (0.125 \\cdot 0.5) + (0.25 \\cdot 0.5^2) = 0.0625 + 0.0625=0.125 \n$$\n\nТаким образом, вероятность такого события равна 12.5%. Этот пример показывает, что, зная совместное распределение, мы можем анализировать поведение двух величин одновременно.\n\n## Независимость случайных величин\n\nСовместное распределение позволяет описывать, как ведут себя две случайные величины одновременно. Но во многих случаях нам важно понять: *влияют ли они друг на друга вообще*? Или их поведение полностью независимо?\n\nДля контраста возьмём два примера.\n\n1. $X$ — количество очков, выпавшее на игральном кубике, а $Y$ — температура воздуха за окном. Интуитивно связи нет, знание $X$ ничего не говорит о $Y$.\n2. $X$ — цвет автомобиля; $Y$ — число ДТП за год. Кажется, что связь должна отсутствовать, но не исключено, что есть косвенные факторы и их стоит проверить: вдруг более рискованные водители предпочитают автомобили красного цвета или зимой больше аварий с авто белого цвета?\n\nСейчас сформулируем, как именно проверяют независимость.\n\nДве случайные величины $X$ и $Y$ называются `независимыми`, если для любых $A, B\\in\\mathbb{R}$ события $X\\in A$ и $Y\\in B$ независимы.\n\nТо есть:\n\n$$\\mathbb{P}(X\\in A,Y\\in B)=\\mathbb{P}(X\\in A)\\cdot \\mathbb{P}(Y\\in B) \n$$\n\nОднако на практике такое условие оказывается не очень удобным, поэтому используют следующее эквивалентное условие.\n\n**Для дискретных случайных величин:**\n\n$$P_{X,Y}(x,y) = P_X(x)\\cdot P_Y(y) \n$$\n\n**Для непрерывных случайных величин:**\n\n$$p_{X,Y}(x,y) = p_X(x)\\cdot p_Y(y) \n$$\n\nОдним из важных свойств независимых случайных величин $X$ и $Y$ является то, что\n\n$$\\mathbb{E}(XY)=\\mathbb{E}(X)\\cdot \\mathbb{E}(Y) \n$$\n\nЭто свойство означает, что для независимых величин среднее значение их произведения равно произведению их средних значений: при независимости «высокие» или «низкие» значения одной величины не сдвигают среднее другой, поэтому среднее от произведения распадается в произведение средних.\n\nЭто очень мощный инструмент, который сильно упрощает многие вычисления в теории вероятностей и статистике. Чтобы понять, почему эта формула верна, нужно взглянуть на общие правила вычисления матожидания от функции двух случайных величин.\n\n{% cut \"Матожидание функций от нескольких случайных величин\" %}\n\nКак и для функций от одной случайной величины, можно не находить распределение случайной величины $XY$ для того, чтобы найти её матожидание. В общем случае матожидание $f(X,Y)$ можно найти как\n\n$$\\mathbb{E}[f(X,Y)] = \\sum_{x,y} f(x,y)\\cdot P_{X,Y}(x,y) \n$$\n\nдля дискретного совместного распределения и\n\n$$\\mathbb{E}[f(X,Y)] = \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty} f(x,y)\\cdot p_{X,Y}(x,y)dxdy \n$$\n\nдля непрерывных случайных величин. Этими простыми формулами пользуются повсеместно, даже не задумываясь об этом.\n\nКстати, с их помощью просто доказывается указанное выше свойство матожидания — сумма и интеграл распадаются в произведение отдельных сумм и интегралов по $x$ и по $y$.\n\n{% endcut %}\n\nТеперь, когда мы разобрались с теорией, посмотрим, как это свойство работает на практике.\n\n#### Пример\n\nПусть каждый месяц стоимость акций меняется случайным образом, причём в среднем увеличивается на 5%. На сколько процентов в среднем будет меняться стоимость акций за два месяца, если изменения за месяц не зависят друг от друга?\n\nПусть случайные величины $X_1$ и $X_2$ описывают, во сколько раз изменится стоимость акций за первый и второй месяц соответственно. Тогда за два месяца она изменится в $X_1X_2$ раз. Так как $X_1$ и $X_2$ независимы:\n\n$$\\mathbb{E}(X_1 X_2)=\\mathbb{E}(X_1)\\mathbb{E}(X_2)=1.05\\cdot 1.05 = 1.1025 \n$$\n\nТо есть ожидаемое изменение за два месяца $+10.25\\%$.\n\nТаким образом, независимость случайных величин означает полное отсутствие взаимного влияния: знание значения одной случайной величины не даёт никакой информации о другой. Это свойство упрощает вычисления и позволяет разложить сложные задачи на более простые, как в примере выше.\n\n## Ковариация\n\nМы только что обсудили, что означает полная независимость случайных величин. Однако на практике такие ситуации редки. Чаще всего между величинами существует какая-то зависимость — и нам важно не только установить её наличие, но и измерить её силу и направление.\n\n`Ковариация` — это один из способов количественно описать, как две случайные величины соотносятся между собой. В частности, она позволяет понять, растут ли значения одной величины вместе с другой, убывают или вовсе не связаны.\n\nНапример, если мы рассмотрим случайные величины:\n\n* Положительная ковариация ($Cov\u003e0$): Рост температуры воздуха ($X$) и объём продаж мороженого ($Y$). Когда одно растёт, другое тоже имеет тенденцию к росту.\n* Отрицательная ковариация ($Cov\u003c0$): Количество пройденных километров на автомобиле ($X$) и остаток бензина в баке ($Y$). Когда одно растёт, другое закономерно уменьшается.\n* Нулевая ковариация ($Cov\\approx0$): Размер обуви человека ($X$) и его словарный запас ($Y$). Между этими величинами нет очевидной линейной связи.\n\n\u003e 💡**Важно**: в реальных данных возможны скрытые факторы (сезонность, состав выборки и т. п.).\n\u003e\n\u003e Например, температура и продажи мороженого растут летом; если смешать взрослую и детскую выборки, «размер обуви — словарный запас» может ложно коррелировать через возраст.\n\u003e\n\u003e И помним: $Cov⁡=0$ не означает независимость, это лишь отсутствие линейной связи.\n\nФормально ковариацию двух случайных величин $X$ и $Y$ определяют следующим образом:\n\n$$Cov(X,Y)=\\mathbb{E}[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)] \n$$\n\nЧтобы разобраться с этим понятием, вначале давайте поймём, какими свойствами оно обладает. Несложно проверить, что ковариация линейна по каждому из аргументов и не меняется от увеличения их на число\n\n$$Cov(aX_1+bX_2+c, Y)=a\\cdot Cov(X_1,Y)+b\\cdot Cov(X_2,Y) . \n$$\n\nТакже можно заметить, что:\n\n$$Cov(X,X)=\\mathbb{D}X. \n$$\n\nТо есть ковариация является некоторым билинейным обобщением дисперсии на случай нескольких случайных величин.\n\n\u003e 💡Можно думать про ковариацию как про аналог обычного скалярного произведения в трёхмерном пространстве для случайных величин с конечной дисперсией вместо векторов. Аналогично дисперсия — аналог квадрата длины вектора.\n\nРовно как для векторов, чем больше $Cov(X,Y)$, тем больше похожи друг на друга случайные величины $X$ и $Y$. А если наоборот, $Cov(X,Y)$ принимает большое отрицательное значение, это означает, что $X$ похожа на $-Y$. Дальше мы обсудим подробнее точный смысл слова «похожа» в этом контексте.\n\nНо прежде чем двигаться дальше, рассмотрим один важный частный случай. Оказывается, если случайные величины `независимы`, то их ковариация всегда равна нулю. Это легко следует из определения ковариации и свойства математического ожидания произведения независимых величин:\n\n$$Cov(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\cdot \\mathbb{E}[Y] = 0. \n$$\n\nОднако обратное утверждение неверно: нулевая ковариация не гарантирует независимость. Она лишь указывает, что между $X$ и $Y$ нет линейной связи, но они всё ещё могут быть связаны *нелинейно*.\n\nДопустим, $X$ — среднесуточная температура, $Y$ — число визитов в шиномонтаж для смены резины. Практика показывает: когда температура переходит через $\\sim+7^{\\circ}C$, многие «переобуваются», и $Y$ резко растёт. Когда же долго жарко или долго холодно, $Y$ низкое.\n\nИтого получается нелинейная зависимость $Y$ от $X$ с пиком около $+7^{\\circ}C$ (примерно ∩-образная кривая, то есть наблюдается параболическая зависимость). Если распределение температур за год симметрично вокруг этого уровня, то положительные и отрицательные отклонения $(X-\\mathbb{E}X)$ дают в среднем компенсирующиеся вклады, и $Cov(X,Y)\\approx 0$, хотя зависимость очевидно есть. Просто она нелинейная. Зная температуру, мы можем предсказать высокий спрос именно в периоды перехода через $\\sim+7^{\\circ}C$, а не в устойчивую жару/стужу.\n\n\u003e 💡Та же идея работает и для других колоколообразных зависимостей (например, возраст — зарплата/производительность): сильная, но нелинейная связь может давать ковариацию, близкую к нулю. Независимости при этом нет.\n\n#### Строгий пример\n\nРассмотрим дискретную случайную величину $X$, принимающую значения $-1$, $0$ и $1$ с равными вероятностями:\n\n$$\\mathbb{P}(X = -1) = \\mathbb{P}(X = 0) = \\mathbb{P}(X = 1) = \\frac{1}{3} \n$$\n\nЗададим другую величину $Y$ как $Y = X^2$. Тогда $Y$ также дискретна и принимает значения $0$ и $1$ с вероятностями $\\frac{1}{3}$ и $\\frac{2}{3}$ соответственно.\n\nНесложно заметить, что $Y$ полностью определяется через $X$, то есть величины явно *зависимы*. Теперь вычислим ковариацию.\nПосчитаем:\n\n* $\\mathbb{E}[X] = (-1)\\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + 1\\cdot \\frac{1}{3} = 0$,\n* $\\mathbb{E}[Y] = 0 \\cdot \\frac{1}{3} + 1\\cdot \\frac{2}{3} = \\frac{2}{3}$,\n* $\\mathbb{E}[XY] = (-1)\\cdot1\\cdot \\frac{1}{3} + 0\\cdot0\\cdot \\frac{1}{3} + 1\\cdot1\\cdot \\frac{1}{3} = -\\frac{1}{3} + 0 + \\frac{1}{3} = 0$.\n\nТогда:\n\n$$Cov(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] = 0 - 0 \\cdot \\frac{1}{3} = 0. \n$$\n\nТо есть несмотря на явную зависимость случайных величин, ковариация оказалась равна нулю. Это значит, что между ними отсутствует `линейная связь`, хотя зависимость есть — `нелинейная`.\n\nКовариация возникает не только при изучении связи между переменными, но и в более прикладном контексте — при вычислении `дисперсии суммы` двух случайных величин:\n\n$$\\mathbb{D}(X + Y) = \\mathbb{D}(X) + \\mathbb{D}(Y) + 2 \\cdot Cov(X, Y). \n$$\n\nЭто выражение показывает, как взаимосвязь между $X$ и $Y$ влияет на общую изменчивость их суммы. Если ковариация положительная, то величины «колеблются в одну сторону», и суммарная дисперсия растёт. Если отрицательная — колебания компенсируют друг друга, и итоговая дисперсия может быть даже меньше суммы дисперсий.\n\n#### Пример\n\nРассмотрим, например, обратный перевод. Одна модель переводит с русского на китайский, а вторая модель переводит обратно с китайского на русский. Пускай у нас дополнительно есть способ измерять лаконичность текста в любом языке, а $X$ и $Y$ — то, насколько первая и вторая модели меняют лаконичность текста. При этом предположим, что модели аккуратные и в среднем лаконичность текста не меняют. Тогда естественной мерой качества является $\\mathbb{D}(X+Y)$. Она показывает, насколько сильно в среднем меняется лаконичность.\n\nЕсли вторая модель будет пытаться предсказать и скорректировать ошибки первой, то $Cov(X,Y)\u003c0$ и $\\mathbb{D}(X+Y) \u003c \\mathbb{D}(X)+\\mathbb{D}(Y)$. А если, наоборот, вторая модель будет только сильнее путаться на переводах с изменённой лаконичностью, то $Cov(X,Y) \u003e0$ и $\\mathbb{D}(X+Y) \u003e \\mathbb{D}(X)+\\mathbb{D}(Y)$.\n\n## Коэффициент корреляции\n\nОсновной проблемой ковариации является то, что у неё нет фиксированного масштаба: её численные значения зависят от единиц измерения величин и могут быть сложны для интерпретации.\n\nЧтобы избавиться от этой проблемы и получить универсальную шкалу оценки линейной связи, вводится коэффициент корреляции — нормированная версия ковариации, всегда лежащая в интервале от $-1$ до $1$.\n\n$$\\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{\\mathbb{D}(X)}\\sqrt{\\mathbb{D}(Y)}} \n$$\n\n{% cut \"**Доказательство** $-1\\le \\rho(X,Y)\\le 1$\" %}\n\nДля векторов справедливо неравенство Коши — Буняковского, которое утверждает, что:\n\n$$|(v, w)|\\le ||v||\\cdot ||w||. \n$$\n\nДоказательство этого неравенства после замены скалярного произведения на ковариацию даёт:\n\n$$\\left| Cov(X,Y)\\right|\\le \\sqrt{\\mathbb{D}(X)}\\cdot \\sqrt{\\mathbb{D}(Y)} \n$$\n\n{% endcut %}\n\nПосмотрим теперь, как интерпретировать коэффициент корреляции.\n\n* Если $\\rho(X, Y) = 1$, то между $X$ и $Y$ существует `положительная линейная связь`. То есть $Y=aX+b$, где $a\u003e0$.\n* Если $\\rho(X, Y) = -1$ — связь `отрицательная`. То есть $Y=aX+b$, где $a\u003c0$.\n* Если $\\rho(X, Y) = 0$ — `линейной зависимости нет` (но может быть нелинейная, как мы видели выше).\n\nКоэффициент корреляции можно интерпретировать как аналог косинуса угла между векторами для случайных величин.\n\nМы определили коэффициент корреляции через математическое ожидание и дисперсию случайных величин. В реальной работе с данными используют `коэффициент корреляции Пирсона` — это приближённая оценка теоретической корреляции, вычисляемая по данным. Он широко применяется для анализа связей между числовыми признаками и реализован в большинстве библиотек (`pandas`, `numpy`, `scipy` и др.).\n\nФормально выборочный коэффициент корреляции Пирсона определяется так:\n\n$$r = \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^n (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum\\limits_{i=1}^n (y_i - \\bar{y})^2}}, \n$$\n\nгде:\n\n* $x_i$ и $y_i$ — значения двух переменных\n* $\\bar{x}$ и $\\bar{y}$ — их выборочные средние.\n\nЭта формула — аналог $\\rho(X, Y)$, применённый к конечной выборке, и именно её чаще всего используют на практике. В числителе стоит оценка ковариации, умноженная на $n$, а в знаменателе стоят корни из оценок дисперсий, умноженные на $n$. Добавление этих множителей упрощает формулу и делает её более удобной для вычислений.\n\nПри построении регрессионной модели коэффициент корреляции используется на стадии первичного анализа данных. Высокая (по модулю) корреляция между признаком и целевой переменной может говорить о его `предсказательной силе`. Высокая корреляция между признаками может говорить об их избыточности и приводить к нестабильной оценке коэффициентов, например, в линейных моделях.\n\nДля удобства восприятия коэффициенты корреляции всех признаков собирают в так называемую `корреляционную матрицу`. Её удобно вывести в виде тепловой карты, на которой красным отмечены значения, близкие к 1, и синим отмечены значения, близкие к -1.\n\nНапример, в [задаче](https://www.kaggle.com/datasets/rabieelkharoua/alzheimers-disease-dataset) диагностирования болезни Альцгеймера часть признаков и целевая переменная дают следующую таблицу:\n\n![image](https://yastatic.net/s3/education-portal/media/6_6_1_e8fc5c1150.webp)\n\nПо ней видно, что характеристики “FunctionalAssessment” и “ADL” заметно связаны с целевой переменной “Diagnosis”, причем относительно небольшие по модулю значения корреляции от 0.3 до 0.4 на самом деле гораздо более значимы, если учесть, что целевая переменная принимает только значения $0$ и $1$, а перечисленные признаки числовые.\n\nПомимо корреляции Пирсона, полезны также и другие коэффициенты корреляции, измеряющие степень взаимосвязи по набору значений двух случайных величин. Например, `коэффициент корреляции Спирмена`, основанный на `рангах` значений. Он позволяет выявлять `монотонные` (в том числе и нелинейные) связи между переменными и является более устойчивым к выбросам, чем коэффициент корреляции Пирсона.\n\n\u003e 💡Связь между $X$ и $Y$ называется монотонной, если при увеличении $X$ значения $Y$ в целом не убывают (неубывающая) или не возрастают (невозрастающая).\n\n**Важно**: монотонность не требует линейности. Кривая может быть изогнутой, ступенчатой и с плато, главное — порядок сохраняется. Именно поэтому ранговая корреляция Спирмена видит такие зависимости.\n\n![image](https://yastatic.net/s3/education-portal/media/6_6_2_233b4c495e.webp)\n\n{% cut \"**Подробнее про коэффициент корреляции Спирмена**\" %}\n\nВместо самих значений переменных $x_i$ и $y_i$ в этой корреляции используются `ранги` — то есть позиции значений в отсортированном списке. Например, если $x = [7,\\ 2,\\ 9]$, то ранги будут $[2,\\ 1,\\ 3]$: $2$ — первое, $7$ — второе, $9$ — третье.\n\nЕсли все значения различны (нет повторов), коэффициент Спирмена можно вычислить по формуле:\n\n$$r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)} \n$$\n\nгде:\n\n* $d_i$ — разность рангов $x_i$ и $y_i$;\n* $n$ — число наблюдений.\n\nВ более общем случае (при наличии одинаковых значений) используется следующий подход:\n\n1. Каждое значение заменяется на `средний ранг` (например, если три значения совпадают и занимают места 4, 5 и 6, им присваивается ранг $5$).\n2. После этого считается `обычный коэффициент Пирсона по этим рангам`.\n\nИменно так реализован `method=\"spearman\"` в библиотеках `pandas` и `scipy`.\n\n**Пример: монотонная, но не линейная связь**\n\nПусть\n\n$$X = [0,\\ 1,\\ 2,\\ 3,\\ 4], \\quad Y = [0,\\ 0.1,\\ 0.2,\\ 10,\\ 1000]. \n$$\n\nМы видим, что зависимость монотонная, но она не линейна. Вычислим коэффициенты корреляции:\n\n* $r \\approx 0.71$ — корреляция Пирсона обнаруживает связь,\n* $r_s = 1$ — корреляция Спирмена фиксирует идеальную монотонную зависимость.\n\n{% endcut %}\n\n***\n\nВ этом параграфе мы научились описывать, как случайные величины взаимодействуют друг с другом через\n\n* совместные распределения;\n* условные вероятности;\n* ковариацию;\n* корреляцию.\n\nЭти инструменты позволяют не только обнаруживать зависимость, но и количественно её оценить. Они лежат в основе анализа связей между переменными и помогают построить более точные и интерпретируемые модели.\n\nВо многом мы говорили именно про форму связи между величинами: совместные/условные распределения, ковариация и корреляция показывают, двигаются ли $X$ и $Y$ вместе и насколько линейно. Но часто нам важнее другое: сколько неопределённости есть в самой величине и насколько различаются целые распределения, например предсказания модели и реальность.\n\nДля таких вопросов удобен язык теории информации. В следующем параграфе мы введём энтропию, то есть меру неопределённости, её интуитивную шкалу — перплексию — и дивергенцию Кульбака-Лейблера как способ сравнивать распределения.\n\nУвидим, почему эти величины естественно стыкуются с совместными/условными распределениями и почему они так популярны в машинном обучении: от решающих деревьев и языковых моделей до методов снижения размерности и визуализации данных.\n\n\n\u003cbr/\u003e\n\u003cbr/\u003e\n\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13796448.60039af550a92a0a490fd58b05154cb54a46351f?iframe=1\" frameborder=\"0\" name=\"ya-form-13796448.60039af550a92a0a490fd58b05154cb54a46351f\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"7d:Tc7b1,"])</script><script nonce="">self.__next_f.push([1,"В предыдущих разделах мы изучали `характеристики случайных величин` — математическое ожидание, дисперсию, ковариацию. Теперь обратимся к понятиям, которые традиционно относятся к `теории информации`, но мы рассмотрим их здесь, поскольку они идейно близки к уже изученным характеристикам и широко применяются в машинном обучении.\n\nЭти новые величины помогают отвечать на вопросы:\n\n* Сколько неопределённости содержится в случайной величине?\n* Сколько информации мы получаем, когда узнаём её значение?\n* Насколько «далеки» друг от друга две случайные величины или распределения?\n\nЧтобы разобраться в этом, мы изучим такие понятия, как энтропия, перплексия и KL-дивергенция. А затем увидим, как эти, на первый взгляд, абстрактные идеи становятся рабочими инструментами в таких мощных и популярных алгоритмах визуализации и понижения размерности, как t-SNE и UMAP.\n\n## Энтропия Шеннона\n\nЧтобы количественно измерять неопределённость случайной величины $X$, вводят понятие `энтропии Шеннона` $H(X)$. Эта величина описывает, сколько «информации» заключено в случайной величине или, другими словами, насколько непредсказуем её исход.\n\nИнтуитивно:\n\n* Чем *равномернее* распределены вероятности значений $X$, тем *выше* энтропия.\n* Если исход можно предсказать наверняка (вероятность одного исхода равна 1), то энтропия равна нулю.\n\nНапример, бросок честной монеты даёт максимальную энтропию среди всех случайных величин с двумя исходами: результат полностью непредсказуем, и каждое наблюдение несёт ровно 1 бит информации.\n\nЭнтропия Шеннона используется в машинном обучении, криптографии и других областях, где важно оценивать меру неопределённости.\n\nПерейдем к строгому определению. Для дискретной случайной величины энтропию определяют как:\n\n$$H(X)=-\\sum_x P_X(x)\\cdot\\log_2 \\left(P_X(x)\\right)=\\mathbb{E}_X[\\log_2(P_X(X))]. \n$$\n\nЗаметим сразу, что эта величина зависит от вероятности принятия значений и не зависит от самих принимаемых значений. Давайте подробнее разберём последний член формулы.\n\nНа самом деле матожидание по $X$ можно брать не только от обычных функций, таких как $X^2$ или $\\log_2 X$, но и от функций, зависящих от самой $X$, то есть, например, использующих $P_X(x)$. Именно это и происходит в этой формуле.\n\nТеперь перейдём к примерам.\n\n### Равномерное дискретное распределение\n\nПусть $X$ принимает $n$ разных значений с одинаковыми вероятностями $\\frac{1}{n}$.\n\n$$H(X) = -\\sum_{i=1}^n \\frac{1}{n}\\cdot\\log_2 \\left(\\frac{1}{n}\\right) = \\log_2(n). \n$$\n\nОказывается, что для любой случайной величины, принимающей ровно $n$ значений, её энтропия не может превышать $\\log_2(n)$. Причём максимум достигается *только* в случае равномерного распределения, когда неопределённость $X$ наибольшая.\n\n{% cut \"Неравенство Йенсена и наибольшее значение энтропии дискретной случайной величины\" %}\n\nЗнаменитое неравенство Йенсена можно сформулировать следующим образом в терминах случайных величин. Для выпуклой функции $f(x)$:\n\n$$\\mathbb{E}[f(X)] \\ge f(\\mathbb{E}[X]). \n$$\n\nВыглядит очень просто, к тому же выполняется как для дискретных случайных величин, так и для непрерывных при условии, что матожидания существуют.\n\nНа самом деле Неравенство Йенсена имеет и немного более сложную формулировку:\n\n$$\\mathbb{E}_X[f(g(X))] \\ge f(\\mathbb{E}_X[g(X)]), \n$$\n\nгде $f(x)$ выпуклая функция, а $g(x)$ — любая. Причем равенство достигается только если $g(X)$ постоянная функция c вероятностью $1$.\n\nТеперь применим его к $f(x)=-\\log_2 x$, $g(x)=1/P_X(x)$:\n\n$$H(X)= - \\mathbb{E}_X\\left[-\\log_2\\left(\\frac{1}{P_X(X)}\\right)\\right]\\le \\log_2\\left(\\mathbb{E}_X\\left[\\frac{1}{P_X(X)}\\right]\\right)= \\log_2(n), \n$$\n\nтак как\n\n$$\\mathbb{E}X\\left[\\frac{1}{P_X(X)}\\right]=\\sum{x:P_X(x)\\ne 0} \\frac{P_X(x)}{P_X(x)}=\\sum_{x:P_X(x)\\ne 0}1=n, \n$$\n\nгде $n$ — количество значений $X$. Причём равенство достигается только когда $g(x)=1/P_X(x)$ постоянна на значениях $X$, то есть только для равномерных $X$.\n\n{% endcut %}\n\nМинимальное значение энтропии дискретной случайной величины равно $0$, поскольку все слагаемые в сумме $-P_X(x) \\log_2 P_X(x)$ неотрицательны (так как логарифмы вероятностей не положительны). Причём $H(X) = 0$ только в том случае, если случайная величина принимает одно значение с вероятностью 1, то есть является полностью детерминированной.\n\nТаким образом, для случайной величины $X$, принимающей $n$ различных значений, выполняется неравенство:\n\n$$0 \\le H(X) \\le \\log_2 n. \n$$\n\nЭнтропию в этом случае можно рассматривать как меру «равномерности» распределения: чем ближе распределение к равномерному, тем выше энтропия, и наоборот.\n\nОпределение энтропии для непрерывных случайных величин во многом похоже на дискретный случай, только сумма заменяется интегралом:\n\n$$H(X)=-\\int_x p_X(x)\\cdot \\log_2\\left( p_X(x) \\right)dx. \n$$\n\nЧасто в литературе используют ещё следующую запись:\n\n$$H(X)=-\\mathbb{E}[\\log_2 p_X(X)]. \n$$\n\nЧем полезна такая запись? Она подчёркивает единую идею: энтропия — это отрицательное математическое ожидание логарифма плотности или вероятности. Эта концепция универсальна, и в дискретном случае формула выглядит аналогично:\n\n$$H(X) = -\\sum_x P_X(x)\\,\\log_2 P_X(x) = -\\mathbb{E}\\big[\\log_2 P_X(X)\\big]. \n$$\n\nЗдесь меняется лишь способ вычисления ожидания (сумма вместо интеграла), а для исходов с нулевой вероятностью по соглашению принимают, что $0 \\cdot \\log 0 = 0$.\n\nВажно помнить, что в отличие от дискретной энтропии, которая всегда неотрицательна, её непрерывный аналог (дифференциальная энтропия) зависит от масштаба данных и может принимать отрицательные значения.\nДля начала рассмотрим равномерное распределение на отрезке $[a,b]$.\n\n### Равномерное распределение на отрезке $[a,b]$\n\nПусть $X\\sim Unif([a,b])$. Тогда плотность $p_X(x) = \\frac{1}{b - a}$ на отрезке $[a,b]$, и, подставляя её в формулу для энтропии, получаем:\n\n$$H(X) =-\\int_a^b \\frac{1}{b-a}\\cdot \\log_2\\left(\\frac{1}{b-a}\\right)dx = \\log_2 (b-a) \n$$\n\nЗаметим важное отличие: энтропия непрерывной случайной величины может быть отрицательной. Это происходит, например, если $(b - a) \u003c 1$ для равномерного распределения — тогда $\\log_2 (b - a) \u003c 0$.\n\nКак и в дискретном случае, распределение с максимальной энтропией на фиксированном отрезке $[a,b]$ — равномерное. Значение $\\log_2 (b - a)$ является верхней границей для всех распределений, поддержка которых находится на этом отрезке. Поэтому и здесь энтропию можно понимать как меру «равномерности» распределения.\n\n### Применения энтропии\n\nВ машинном обучении энтропия используется как мера «неопределённости» данных. Один из наглядных примеров — алгоритмы построения [решающих деревьев](https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya).\n\nПредставьте, что вы — врач, к которому пришёл пациент. Чтобы поставить диагноз, вы задаёте вопросы с ответами «да» или «нет»: «*Есть ли температура?*», «*Болит ли горло?*», «*Есть ли сыпь?*»\n\nНо в каком порядке задавать вопросы, чтобы как можно быстрее определить болезнь?\n\nКаждый вопрос делит всех возможных пациентов на две группы — тех, кто ответил «да», и тех, кто ответил «нет». После этого разбиения мы хотим, чтобы внутри каждой группы неопределённость была минимальна — чтобы ответы пациентов в каждой подгруппе максимально указывали на какую-то конкретную болезнь.\n\nДля оценки «качества» вопроса используется информационный выигрыш (information gain):\n\n$$IG = H_{\\text{родителя}} - \\sum_{\\text{ответ}\\in\\{\\text{да}, \\text{нет}\\}} \\frac{n_{\\text{ответ}}}{n} H_{\\text{ответ}}, \n$$\n\nгде:\n\n* $H_{\\text{родителя}}$ — энтропия до разбиения (насколько разнообразны болезни изначально)\n* $H_{\\text{ответ}}$ — энтропия в подгруппе с таким ответом.\n\nМы выбираем вопрос, у которого $IG$ максимален, — это эквивалентно минимизации энтропии в дочерних узлах.\n\n{% cut \"**Пример расчётом** $IG$\" %}\n\nДопустим, у нас есть 100 пациентов:\n\n* 50 с гриппом,\n* 30 с ангиной,\n* 20 с аллергией.\n\nНачальная энтропия:\n\n$$H = -\\frac{50}{100}\\log_2 \\frac{50}{100} - \\frac{30}{100}\\log_2 \\frac{30}{100} - \\frac{20}{100}\\log_2 \\frac{20}{100} \\approx 1.485. \n$$\n\nРассмотрим вопрос: «Есть ли температура?» Среди тех, кто ответил «да» (60 человек), болезни распределены так: 50 грипп \\+ 10 ангина. Энтропия этой группы:\n\n$$H_{\\text{да}} = -\\frac{50}{60}\\log_2 \\frac{50}{60} - \\frac{10}{60}\\log_2 \\frac{10}{60} \\approx 0.650. \n$$\n\nСреди тех, кто ответил «нет» (40 человек), болезни распределены так: 20 ангина \\+ 20 аллергия. Энтропия этой группы:\n\n$$H_{\\text{нет}} = -\\frac{20}{40}\\log_2 \\frac{20}{40} - \\frac{20}{40}\\log_2 \\frac{20}{40} = 1. \n$$\n\nИнформационный выигрыш:\n\n$$IG = 1.485 - \\left( \\frac{60}{100} \\cdot 0.650 + \\frac{40}{100} \\cdot 1 \\right) = 1.485 - 0.79 = 0.695. \n$$\n\n{% endcut %}\n\nЕсли другой вопрос даёт больший $IG$, дерево выберет его первым.\n\n\u003e 💡Именно так решающие деревья шаг за шагом «задают вопросы», постепенно минимизируя энтропию в листьях, пока классы не станут максимально однородными.\n\nТаким образом, энтропия помогает измерять «неопределённость» и выбирать оптимальные разбиения данных. Однако иногда удобнее работать не напрямую с энтропией, а с другой связанной величиной — `перплексией`.\n\n## Перплексия\n\nКак мы видели, значения энтропии содержат логарифм, что не всегда интуитивно. Поэтому часто используют связанную величину — `перплексию`, которую можно воспринимать как «эффективное число вариантов», которые предлагает случайная величина.\n\nИными словами, перплексия отвечает на вопрос: «Насколько сильно распределение размазано по возможным исходам?». Если случайная величина похожа на равномерный выбор из $k$ вариантов, её перплексия будет близка к $k$.\n\nФормально перплексия вычисляется через энтропию:\n\n$$Perp(X)=2^{H(X)} . \n$$\n\n#### Примеры\n\n* Для дискретной равномерной случайной величины с $n$ значениями:\n\n$$Perp(X)=n. \n$$\n\n* Для равномерного распределения $X \\sim Unif([a,b])$:\n\n$$Perp(X)=b−a. \n$$\n\nПерплексия особенно наглядна, когда речь идёт о предсказательных моделях. Она используется как метрика качества в языковых моделях (LLM). Смысл здесь простой: если модель «уверена» в следующем слове, то её предсказания концентрируются на нескольких вариантах, энтропия распределения низка, и перплексия мала. Если же модель «теряется» и распределяет вероятности почти равномерно по большому количеству слов, перплексия становится большой.\n\n**Пример**. Пусть языковая модель прогнозирует следующее слово в предложении и выдаёт следующие вероятности для 4 кандидатов:\n\n* «кошка» — 0.7\n* «собака» — 0.1\n* «мышь» — 0.1\n* «рыба» — 0.1\n\nЭнтропия:\n\n$$H = - \\sum p_i \\log_2 p_i = - (0.7\\log_2 0.7 + 3\\cdot 0.1 \\log_2 0.1) \\approx 1.356. \n$$\n\nПерплексия:\n\n$$Perp = 2^{H} \\approx 2^{1.356} \\approx 2.56. \n$$\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_1_da92fccbc1.webp)\n\nЭто значит, что «по степени неопределённости» распределение модели похоже на равномерный выбор из 2.56 вариантов. Если же распределение было бы равномерным (0.25 на каждое слово), энтропия была бы $H = 2$, а перплексия — $Perp = 4$. То есть модель была бы полностью «неуверенной».\n\nКак мы заметили, энтропия в некотором смысле измеряет, насколько распределение «далеко» от равномерного — то есть насколько в нём выражена неопределённость.\n\nНо иногда нам важно сравнивать два произвольных распределения между собой — например, насколько текущее поведение модели отличается от ожидаемого или насколько аппроксимация близка к реальному распределению.\n\nИ тут возникает естественный вопрос: как измерять расстояние между двумя распределениями? На этот вопрос отвечает следующая важная концепция — дивергенция Кульбака — Лейблера (KL-дивергенция).\n\n## Дивиргенция Кульбака — Лейблера\n\nСамый естественный способ ввести расстояние между распределениями $X$ и $Y$ — сравнить их функции распределения. Например, можно рассмотреть величину\n\n$$\\sup_x \\left|F_X(x) -F_Y(x)\\right|, \n$$\n\nкоторая называется расстоянием Колмогорова и обладает многими интересными свойствами.\n\nВ частности, она используется в тесте Колмогорова, проверяющем, пришёл ли набор данных из некоторого распределения. Однако из-за использования супремума (максимального отклонения) расстояние Колмогорова плохо работает в задачах, где нужно учитывать всю структуру различий между распределениями, а не только наибольшее.\n\nДругой способ ввести расстояние — это `дивергенция Кульбака — Лейблера`. Для двух дискретных величин $X$ и $Y$ она определяется как\n\n$$D_{KL}(X||Y) = \\sum_x P_X(x)\\cdot \\log_2\\left(\\frac{P_X(x)}{P_Y(x)}\\right) = \\mathbb{E}_X\\left[\\log_2\\left( \\frac{P_X(X)}{P_Y(X)}\\right)\\right]. \n$$\n\nИнтуитивно KL-дивергенция измеряет, насколько «удивительным» кажется распределение $X$, если мы считаем, что мир устроен по распределению $Y$. То есть она отвечает на вопрос: сколько дополнительной информации (в среднем) нам понадобится, если мы будем кодировать данные из $X$, думая, что они приходят из $Y$?\n\nСразу заметим, что это «расстояние» не симметрично, то есть часто\n\n$$D_{KL}(X||Y)\\ne D_{KL}(Y||X). \n$$\n\nДавайте посмотрим на простом примере с подбрасыванием монеты, как проявляется эта асимметрия. Сравним два распределения: одно для честной монеты ($P$) и другое для нечестной ($Q$), а затем посчитаем дивергенцию $D_{KL}(P||Q)$ и $D_{KL}(Q||P)$.\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_2_579291a693.webp)\n\nКак видно из расчётов, результаты получаются разными, что подтверждает асимметричность KL-дивергенции. Однако другие свойства расстояния выполняются. В частности,\n\n$$D_{KL}(X||Y)\\ge 0, \n$$\n\nпричём равенство достигается тогда и только, когда их распределения совпадают, то есть $P_X(x)=P_Y(x)$ для всех $x$.\n\n{% cut \"Неравенство Йенсена и неотрицательность $D_{KL}(X||Y)$\" %}\n\nПрименим неравенство Йенсена к $f(x)=-\\log_2 x$, $g(x)=P_Y(x)/P_X(x)$:\n\n$$D_{KL}(X||Y)=\\mathbb{E}_X\\left[-\\log_2\\left(\\frac{P_Y(X)}{P_X(X)}\\right)\\right]\\ge -\\log_2\\left(\\mathbb{E}_X\\left[\\frac{P_Y(X)}{P_X(X)}\\right]\\right)= 0, \n$$\n\nтак как\n\n$$\\mathbb{E}_X\\left[\\frac{P_Y(X)}{P_X(X)}\\right]=\\sum_x P_X(x)\\frac{P_Y(x)}{P_X(x)}=\\sum_xP_Y(x)=1. \n$$\n\nПричём равенство достигается только когда $g(x)=P_Y(x)/P_X(x)$ постоянная функция, то есть когда распределения совпадают.\n\n{% endcut %}\n\nДивергенция Кульбака — Лейблера широко используется в машинном обучении.\n\n**Пример: [дистилляция моделей](https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij)**\n\nПорой хочется сохранить точность модели и при этом уменьшить её вес и параллельно ускорить инференс. В таких случаях применяется дистилляция — процесс переноса знаний от большой, мощной модели (teacher) к более компактной (student) путём обучения последней воспроизводить поведение первой.\n\nДля задачи классификации компактную модель обычно учат предсказывать распределения, выдаваемые мощной моделью, вместо исходных меток. При этом в функции потерь используется именно KL-дивергенция.\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_3_13aad30811.webp)\n\nДля непрерывных случайных величин дивергенция Кульбака — Лейблера определяется похожим образом, но с использованием интеграла вместо суммы:\n\n$$D_{\\mathrm{KL}}(X \\,\\|\\, Y) = \\int p_X(x) \\cdot \\log_2 \\left( \\frac{p_X(x)}{p_Y(x)} \\right) dx. \n$$\n\nЗдесь $p_X(x)$ и $p_Y(x)$ — плотности распределения $X$ и $Y$ соответственно. Как и в дискретном случае, дивергенция измеряет, насколько распределение $X$ отличается от $Y$, и также может быть несимметричной.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/6_7_4_9de4ff05cd.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Визуализация плотностей P(x) и Q(x)\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВизуализация плотностей $P(x)$ и $Q(x)$ и вклада $p(x)\\log\\frac{p(x)}{q(x)}$ в KL-дивергенцию $D_{\\mathrm{KL}}(P \\parallel Q)$\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nНа графике видно, как формируется KL-дивергенция $D_{\\mathrm{KL}}(P \\parallel Q)$. Слева: два близких нормальных распределения $\\mathcal{N}(0,1)$ и $\\mathcal{N}(0.2,1)$, KL-дивергенция мала. Справа: распределения $\\mathcal{N}(0,1)$ и $\\mathcal{N}(1,1)$, KL-дивергенция значительно больше. Зелёная пунктирная кривая показывает локальный вклад в расхождение между распределениями в конкретной точке $x$. Чем сильнее отклоняется $Q(x)$ от $P(x)$ в областях с высокой плотностью $P(x)$, тем больше вклад в $D_{\\mathrm{KL}}(P \\parallel Q)$.\n\nЕще один пример, где можно встретить KL-дивергенцию, — это генеративные модели, которые учатся приближать распределение данных с помощью параметрических моделей.\n\n**Пример: [вариационный автоэнкодер (VAE)](https://education.yandex.ru/handbook/ml/article/variational-autoencoder-\\(vae\\))**\n\nVAE — это нейросетевая архитектура, которая обучается восстанавливать объекты (например, изображения) из сжатого представления — скрытого вектора. При этом предполагается, что координаты скрытого вектора распределены по стандартному нормальному закону.\n\nЕстественная и теоретически обоснованная функция потерь для этой модели раскладывается в сумму двух слагаемых. Первое слагаемое отвечает за реконструкцию — насколько похоже восстановленное изображение на исходное. А второе слагаемое является KL-дивергенцией.\n\n$$D_{\\mathrm{KL}}(q(z|x) \\,\\|\\, p(z)). \n$$\n\nОно стремится сделать распределение скрытых переменных $q(z∣x)$ похожим на заданное априорное распределение $p(z)$. Обычно оно стандартное нормальное.\n\nПонятия, с которыми мы только что познакомились, — `перплексия` и `дивергенция Кульбака — Лейблера` — кажутся абстрактными, но на деле удивительным образом встречаются также и в одном из самых популярных методов визуализации данных.\n\nРечь идёт о методе t-SNE (t-Distributed Stochastic Neighbour Embedding) — нелинейном методе понижения размерности, который позволяет отображать многомерные данные в двумерное или трёхмерное пространство, сохраняя локальные структуры.\n\n## t-SNE\n\nt-SNE — это метод, построенный на вероятностной модели: он пытается построить двумерную карту, на которой близкие объекты (в высоком пространстве) отображаются как близкие точки, а далёкие — как далёкие.\n\nПри этом t-SNE не сохраняет точные расстояния между точками, а стремится, чтобы вероятности близости объектов в исходном и проекционном пространствах были максимально схожи. Алгоритм оперирует не расстояниями напрямую, а распределениями: он пытается сопоставить распределение в исходном пространстве с распределением в пространстве пониженной размерности, минимизируя их различие.\n\nСначала коротко, что делает алгоритм:\n\n* строит в исходном пространстве распределение соседства точек $p_{j|i}$ (ширину $\\sigma_i$ подбирает так, чтобы перплексия $\\approx$ заданному числу соседей);\n* симметризует и нормирует его в матрицу $P=(p_{ij})$;\n* строит в низкой размерности аналогичное распределение $Q=(q_{ij})$ на базе t-распределения с одной степенью свободы (Коши);\n* находит координаты проекции $\\{y_i\\}$, минимизируя $D_{\\mathrm{KL}}(P\\parallel Q)$ градиентным спуском.\n\nТеперь давайте рассмотрим каждый этап подробнее.\n\n**Этап 1.** В исходном пространстве (например, $\\mathbb{R}^{100}$) считается распределение вероятностей $p_{j|i}$, которое отражает, с какой вероятностью $x_j$ будет соседом $x_i$. Эта вероятность основана на гауссовом распределении расстояний между точками:\n\n$$p_{j|i}=\\frac{\\exp\\left(\\frac{-\\|x_i-x_j\\|^2}{2\\sigma^2_i}\\right)}{\\sum\\limits_{k\\neq i}\\exp\\left(\\frac{-\\|x_i-x_k\\|^2}{2\\sigma^2_i}\\right)} \n$$\n\nЗдесь:\n\n* $x_i$ — фиксированная точка, для которой мы хотим посчитать распределение соседей;\n* $x_j$ — одна из других точек, для которой мы хотим узнать, насколько она «соседняя» к $x_i$;\n* $x_k$ — переменная суммирования, пробегающая по всем точкам, кроме $x_i$ (используется для нормировки, чтобы сумма вероятностей по $j$ давала 1);\n* $\\sigma_i$ — параметр, определяющий, насколько «широко» точка $x_i$ считает других своими соседями.\n\nЧем больше $\\sigma_i$, тем больше объектов вокруг считаются похожими на $x_i$.\n\nАлгоритм t-SNE подбирает значение $\\sigma_i$ автоматически для каждой точки — так, чтобы `перплексия` распределения соседей была одинаковой для всех точек. Перплексия в данном случае задаёт желаемое `эффективное число соседей` и является важным гиперпараметром метода.\n\n**Этап 2.** Чтобы устранить направление зависимости между точками и получить общее распределение соседства, t-SNE симметризует и нормирует условные вероятности:\n\n$$p_{ij}=\\frac{p_{j|i}+p_{i|j}}{2n}, \n$$\n\nгде $n$ — общее число точек. Симметризация делает $p_{ij}$ одинаковым при обмене $i\\leftrightarrow j$ , а деление на $2n$ нормирует сумму всех вероятностей до 1. Это превращает локальные вероятности в глобальную вероятностную матрицу $P$, отражающую взаимные близости между всеми парами точек.\n\n**Этап 3.** Затем в двумерном пространстве $\\mathbb{R}^{2}$ рассчитывается аналогичное распределение $q_{ij}$, но не по Гауссу, а по t-распределению Стьюдента с одной степенью свободы (мы знаем его как распределение Коши):\n\n$$q_{ij}=\\frac{\\left(1+\\|y_i-y_j\\|^2\\right)^{-1}}{\\sum\\limits_{k\\neq l}\\left(1+\\|y_k-y_l\\|^2\\right)^{-1}}, \n$$\n\nгде $y_i$ — координаты проекции исходной точки $x_i$; сумма в знаменателе — нормирующий множитель, гарантирующий, что сумма $q_{ij}$ по всем парам $i\\ne j$ равна $1$.\n\nПочему используется именно это распределение? Оно обладает тяжёлыми хвостами, что позволяет точкам, находившимся недалеко друг от друга, иметь более удалённые проекции. Это необходимо из-за проблемы тесноты — в низкой размерности не хватает «места», чтобы правильно отразить все расстояния между точками многомерного пространства. Название этого распределения дало букву t в названии t-SNE, его использование отличает этот метод от более раннего метода SNE.\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://yastatic.net/s3/education-portal/media/6_7_5_3a6c1c62ac.webp\" loading=\"lazy\" decoding=\"async\" alt=\"\"\u003e\n  \u003cfigcaption\u003e\n    Сравнение плотностей распределений Гаусса и Коши. Видно, что распределение Коши имеет «тяжёлые хвосты» по бокам, что используется в t-SNE\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**Этап 4.** Оптимизация. Цель t-SNE — найти такие координаты точек в проекции, чтобы распределение $q_{ij}$ было как можно ближе к $p_{ij}$. Это достигается путём минимизации дивергенции Кульбака-Лейблера:\n\n$${\\displaystyle D_{KL}(P\\parallel Q)=\\sum \\limits_{i\\neq j}p_{ij}\\log {\\frac {p_{ij}}{q_{ij}}}}. \n$$\n\nЭтот функционал штрафует ситуации, в которых две точки были близки в исходном пространстве (высокое $q_{ij}$), но в проекции оказались далеко друг от друга (малое $q_{ij}$). Таким образом, t-SNE фокусируется именно на сохранении локальных соседей, а не глобальной геометрии.\n\nОптимизация проводится итеративно с помощью градиентного спуска, так как аналитического решения нет. Алгоритм пошагово передвигает точки $y_i$ так, чтобы максимально сохранить локальные отношения.\n\nОсобенности t-SNE:\n\n* t-SNE не сохраняет глобальные расстояния — кластеры могут располагаться произвольно.\n* Не существует явного отображения $f(x)$, то есть нельзя напрямую проецировать новые данные.\n* Выход сильно зависит от значения перплексии, скорости обучения и начальной инициализации.\n\nТем не менее, t-SNE остаётся мощным инструментом для анализа скрытых представлений (эмбеддингов), так как он может визуально раскрывать кластеры, группы и даже фазовые переходы в структуре данных.\n\n{% cut \"Реализация в Python\" %}\n\n```\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, perplexity=30, random_state=0)\nX_tsne = tsne.fit_transform(X)\n```\n\nПри этом на самом деле t-SNE проецирует не обязательно на двумерное пространство, за размерность проекции отвечает параметр n_components.\n\n{% endcut %}\n\nБольше подробностей про этот метод, например красивое физическое описание градиента функции потерь, желающие могут найти в исходной [статье](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_6_3b1d5b931f.webp)\n\n## Ещё про нелинейные методы понижения размерности: UMAP\n\nДругим популярным методом визуализации, альтернативным t-SNE, является UMAP (англ. [Uniform Manifold Approximation and Projection](https://arxiv.org/pdf/1802.03426)).\n\nЭто метод нелинейного снижения размерности, разработанный на основе теории категорий и топологии многообразий. Он исходит из предположения, что данные лежат на гладком многообразии, и стремится сохранить локальную топологию этого многообразия. В отличие от t-SNE, он работает быстрее, а также допускает построение проекций для новых точек.\n\nUMAP состоит из трех этапов:\n\n1. Построение взвешенного графа в исходном пространстве\n\n2. Начальное вложение графа в маломерное пространство.\n\n3. Оптимизация.\n\nРазберём их подробнее.\n\n**Этап 1. Построение взвешенного графа в исходном пространстве.** Пусть у нас есть данные в $\\mathbb{R}^n$: точки $x_1, x_2, \\dots, x_m$. Для каждой точки $x_i$ мы ищем её $k$ ближайших соседей по расстоянию (обычно $k = 15$). Это делается, чтобы рассматривать только локальные связи, которые близки к поверхности нашего многообразия.\n\nЗатем мы строим ориентированный взвешенный граф: для каждой точки $x_i$ и $k$ ближайших к ней точек $x_j$ мы хотим назначить вес $w_{ij}$ — он показывает, насколько «плотно» точки связаны. Этот вес определяется по формуле:\n\n$$w_{ij}=\\exp\\left(-\\frac{\\|x_i-x_j\\|-\\rho_i}{\\sigma_i}\\right)\\le 1, \n$$\n\nгде:\n\n* $\\|x_i - x_j\\|$ — евклидово расстояние между точками;\n* $\\rho_i$ — минимальное расстояние от $x_i$ до ближайшего соседа (чтобы гарантировать $w_{ij} = 1$ для ближайшего);\n* $\\sigma_i$ — настраиваемый масштаб, подбираемый так, чтобы $\\sum_j w_{ij}= \\log_2 k$.\n\nДля всех остальных точек $x_k$ положим $w_{ik}=0$. Далее веса симметризуются по формуле:\n\n$$B_{ij}=w_{ij}+w_{ji}-w_{ij}\\cdot w_{ji}=1-(1-w_{ij})(1-w_{ji}) \\in [0, 1] \n$$\n\nЕсли $w_{ij}$ задавала вероятность проведения ориентированного ребра из $x_i$ в $x_j$, то $B_{ij}$ задаёт вероятность того, что между этими вершинами будет проведено хотя бы одно ребро. UMAP-граф — это неориентированный граф с весами $B_{ij}$ на ребрах. Именно его структуру мы хотим повторить в маломерном пространстве.\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_7_b8a1fc4df9.webp)\n\n**Этап 2. Начальное вложение графа в маломерное пространство.** Первоначально можно расположить все точки в маломерном пространстве случайно. Однако авторы статьи предлагают использовать для этого спектральное разложение лапласиана $L$, построенного по матрице $B$.\n\n{% cut \"Почему спектральное разложение дает хорошее вложение?\" %}\n\nДавайте вначале рассмотрим задачу построения вложения $\\{y_i\\}_i$, минимизирующего функционал энергии:\n\n$$\\mathcal{E}(Y)=\\sum_{i,j}B_{ij}\\cdot ||y_i-y_j||^2 \\longrightarrow \\min . \n$$\n\nЧем больше $B_{ij}$, тем ближе мы хотим поставить соответствующие точки. А для $B_{ij}=0$ нам не важно, насколько близко будут $y_i$ и $y_j$. Это ровно то, что нам нужно! Однако, конечно, такая задача имеет тривиальное решение — поставить все $y_i$ в одну точку. Для устранения этой проблемы вводится дополнительное нормировочное условие, но про него позже.\n\nЕсли использовать равенство $||y_i-y_j||^2=||y_i||^2+||y_j||^2-2y_i^Ty_j$ и собрать все $y_i$ в одну матрицу $Y$ по строчкам, то функционал перепишется как\n\n$$\\mathcal{E}(Y)=\\sum_i ||y_i||\\sum_j B_{ij}+\\sum_j ||y_j||\\sum_i B_{ij}-2\\sum_{i,j} B_{ij} y_i^T y_j= \n$$\n\n$$=2\\left[\\sum_i d_i \\| y_i \\|^2 - \\sum_{i,j} B_{ij} y_i^T y_j\\right]=2\\cdot Y^T(D-B)Y \\longrightarrow \\min, \n$$\n\nгде $d_i=\\sum_i B_{ij}=\\sum_j B_{ij}$ — взвешенная степень вершины $i$, а $D$ — диагональная матрица взвешенных степеней вершин.\n\nОсталось добавить нормировочное условие $Y^TY=1$, которое по сути говорит, что у каждой из координат должен быть фиксированный разброс и координаты должны быть никак не связаны друг с другом. Полученная задача решается, например, после перехода к ортонормированному базису, в котором матрица $L=D-B$ диагональна. И получается, что столбцами матрицы $Y$ должны быть ортонормированные собственные векторы $L$, соответствующие наименьшим собственным значениям.\n\n**Примечания:**\n\n* Матрица $L$ всегда имеет собственное значение $0$. У соответствующего собственного вектора все координаты одинаковые. Поэтому берут собственные вектора, соответствующие наименьшим собственным значениям, кроме $0$.\n\n* В исходной статье использовано немного другое нормировочное условие, которое после замены переменных приводит к другому виду лапласиана\n\n$$L=D^{\\frac{1}{2}}(D-B)D^{\\frac{1}{2}} . \n$$\n\n{% endcut %}\n\n**Этап 3. Оптимизация.** Теперь у нас есть граф в исходном пространстве и проекции его вершин в маломерное пространство. Дальше происходит процесс, очень похожий на градиентный спуск в t-SNE. На каждом шаге к проекции некоторой точки $y_i$ применяется сила притяжения\n\n$$\\frac{-2ab||y_i-y_j||^{2(b-1)}}{1+ ||y_i-y_j||^{2}}\\cdot w_{ij}\\cdot (y_i-y_j) \n$$\n\nсо стороны точки $y_j$ и силы отталкивания\n\n$$\\frac{2b}{(\\varepsilon + ||y_i-y_k||^{2})(1+a||y_i-y_k||^{2b})}\\cdot(1-w_{ij})\\cdot (y_i-y_k) \n$$\n\nсо стороны некоторых других точек $y_k$. Здесь $a$ и $b$ — параметры алгоритма, а $\\varepsilon$ — маленькое положительное число, добавленное для вычислительной стойкости. Как и в t-SNE, эти формулы не случайны, а приходят из минимизации функции потерь, которая называется кросс-энтропией нечётких множеств.\n\n### Особенности UMAP:\n\n* Поддерживает сохранение как локальной, так и глобальной структуры.\n\n* Быстрее t-SNE и масштабируется на большие объёмы.\n\n* Позволяет проецировать новые данные (в отличие от t-SNE).\n\nUMAP широко используется в задачах анализа данных, где необходимо визуализировать, структурировать или упростить сложные признаки. Он особенно полезен при работе с эмбеддингами — векторными представлениями слов, изображений, документов, пользователей и т. д. В таких случаях UMAP помогает выявить кластеры, скрытую структуру, переходы между классами и аномалии. Его высокая скорость и масштабируемость делают его удобным выбором для интерактивного анализа больших наборов данных, включая тексты, геномные данные, пользовательское поведение и скрытые слои нейросетей.\n\n{% cut \"Реализация в Python\" %}\n\nUMAP в `Python` реализован в [одноимённой библиотеке](https://umap-learn.readthedocs.io/en/latest/):\n\n```\nmport umap\n\numap_model = umap.UMAP(n_components=2, n_neighbors=15)\nX_umap = umap_model.fit_transform(X)\n```\n\nОн поддерживает метод .transform(X_new) для новых данных.\n\n{% endcut %}\n\n![6.7](https://yastatic.net/s3/education-portal/media/6_7_8_1_c6c420cf91.webp)\n\n***\n\nВ этом параграфе главы мы вышли за рамки классических характеристик случайных величин и познакомились с инструментами теории информации.\n\nМы научились измерять неопределённость с помощью энтропии, переводить её в интуитивную шкалу через перплексию и оценивать различие между распределениями с помощью дивергенции Кульбака — Лейблера. Мы также увидели, что эти идеи лежат в основе современных методов визуализации и снижения размерности данных — от t-SNE до UMAP.\n\nЭти идеи завершают наше погружение в мир случайных величин. Освоив классические методы анализа связей, такие как корреляция, и более современные подходы из теории информации, вы получили комплексный набор инструментов для глубокого анализа данных.\n\nЭти знания являются фундаментом для понимания, создания и оценки самых разнообразных моделей машинного обучения: от простых регрессий до сложных нейросетевых архитектур.\nА в следующем параграфе мы подведём итоги по всей главе — коротко вспомним, чему научились, и обсудим, что будет дальше.\n\n\u003c/br\u003e\n\u003c/br\u003e\n\u003cscript src=\"https://forms.yandex.ru/_static/embed.js\"\u003e\u003c/script\u003e\u003ciframe src=\"https://forms.yandex.ru/surveys/13796591.0f9460738ef1a61c4f231b7646dbd8d2b5ef1a82?iframe=1\" frameborder=\"0\" name=\"ya-form-13796591.0f9460738ef1a61c4f231b7646dbd8d2b5ef1a82\" width=\"650\"\u003e\u003c/iframe\u003e\n"])</script><script nonce="">self.__next_f.push([1,"7e:Tffa,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы собрали цельную картину вероятностного подхода — от базовых определений до инструментов теории информации и визуализации.\n\nДавайте коротко вспомним, что вы уже знаете и умеете.\n* **Вероятностные основы.** Строить вероятностные пространства ($\\Omega,\\mathcal{F},\\mathbb{P})$, работать с условными вероятностями, законом полной вероятности и формулой Байеса. Корректно трактовать независимость событий и величин.\n* **Случайные величины и распределения.** Работать с дискретными и непрерывными величинами, применять ключевые распределения (биномиальное, Пуассона, экспоненциальное, нормальное) по назначению.\n* **Характеристики распределений.** Использовать матожидание, дисперсию, функции распределения. Понимать трансформации величин и смеси распределений.\n* **Связи между величинами.** Анализировать совместные и условные распределения, измерять связь с помощью ковариации и корреляции (Пирсона и Спирмена), а также отличать независимость от нулевой корреляции и видеть роль монотонных (нелинейных) связей.\n* **Качество моделей.** Интерпретировать метрики вроде `precision` и `recall` с учётом дисбаланса классов и бизнес-контекста.\n* **Теория информации в ML.** Измерять неопределённость через энтропию и её интуитивную шкалу — перплексию — и сравнивать распределения с помощью KL-дивергенции. Понимать связь с кросс-энтропией и функциями потерь (дистилляция, VAE).\n* **Снижение размерности и визуализация.** Видеть, как перплексия и KL-дивергенция используются в t-SNE, и знать UMAP как быструю альтернативу с поддержкой трансформации новых точек.\n\nЭтот набор инструментов позволит вам не просто применять готовые модели, но и глубоко понимать их внутреннюю работу, грамотно интерпретировать результаты и принимать обоснованные решения в условиях неопределённости.\n\nТеперь, когда вы освоили язык теории вероятностей для описания случайности, мы готовы сделать следующий шаг и научиться принимать решения в условиях неопределённости с помощью математической статистики.\nВ следующей главе мы погрузимся в мир АБ-тестирования и проверки гипотез, разберём фундаментальные понятия: закон больших чисел (ЗБЧ) и центральную предельную теорему (ЦПТ).\n\nЭто даст нам чёткий пайплайн для работы с данными, чтобы вооружиться всей мощью статистических критериев."])</script><script nonce="">self.__next_f.push([1,"7f:Te06,"])</script><script nonce="">self.__next_f.push([1,"Это вторая часть про графы: мы будем смотреть на граф как на пространство сигналов, изучать его спектр и динамику, а затем строить меры сходства и векторные представления для практических задач.\n\nС помощью этих методов вы сможете:\n\n- Анализировать глобальную структуру сетей — выявлять сообщества, оценивать связность и находить узкие места.\n- Моделировать динамические процессы — понимать, как распространяется информация (или влияние) в социальных сетях, интернете или биологических системах.\n- Представлять графы для ML — превращать сложные графовые структуры в векторы (эмбеддинги), понятные для классических алгоритмов, или сравнивать графы напрямую с помощью ядер.\n- Понимать основу современных GNN — увидеть, как спектральный анализ и графовые фильтры лежат в основе графовых нейронных сетей.\n\nМы специально вынесли эту главу в конец, поскольку для комфортного чтения вам потребуются знания из глав по [математическому анализу](https://education.yandex.ru/handbook/math/article/predeli-i-neprerivnost-funktsii), [линейной алгебре](https://education.yandex.ru/handbook/math/article/vektori) и [теории вероятностей](https://education.yandex.ru/handbook/math/article/veroiatnostnoe-prostranstvo).\n\nЭта глава состоит из двух параграфов:\n\n- В первом поговорим о спектральных методах и диффузии на графах. Мы введём понятия графового лапласиана и его спектра, разберёмся, как его собственные значения и векторы описывают структуру графа. Изучим графовое Фурье-преобразование, диффузионные процессы (включая PageRank) и графовые фильтры — основу GCN.\n- Второй будет посвящён ядрам и эмбеддингам графов. Мы рассмотрим методы сравнения графов (ядра) и способы их векторного представления (эмбеддинги, Node2Vec и Graph2Vec). Затем покажем, как эти представления используются для кластеризации, извлечения признаков для табличных моделей и быстрого поиска похожих объектов.\n\nВ итоге вы получите рабочий инструментарий: от анализа структуры графа и моделирования процессов до построения векторных представлений, кластеризации и быстрого поиска похожих объектов.\n\nДавайте приступим!"])</script><script nonce="">self.__next_f.push([1,"80:T10714,"])</script><script nonce="">self.__next_f.push([1,"В этом параграфе мы научимся смотреть на граф не просто как на набор вершин и рёбер, а как на пространство, по которому могут распространяться сигналы подобно тому, как звук распространяется в физической среде. Этот подход, известный как обработка сигналов на графах (Graph Signal Processing), позволяет применять идеи из физики для анализа сложных сетей.\n\nМы разберём:\n\n- **Спектр графа.** Введём понятие графового лапласиана и разберёмся, почему его собственные значения можно трактовать как некие «частоты» графа, а собственные векторы — как его основные «моды» колебаний.\n- **Графовое Фурье-преобразование.** По аналогии с классическим анализом сигналов мы научимся раскладывать данные на графе по базису из собственных векторов.\n- **Диффузионные процессы и случайные блуждания.** Мы изучим, как информация растекается по графу со временем и как это связано с такими алгоритмами, как PageRank.\n- **Графовые фильтры.** Мы рассмотрим, как можно обрабатывать сигналы на графах, усиливая или ослабляя определённые частотные компоненты для выделения нужных структур.\n\nПрочитав этот параграф, вы сможете анализировать глобальную структуру графа с помощью его спектра. Например, для оценки связности сети или поиска сообществ. Вы поймёте, как моделируются динамические процессы на сетях: от распространения мнений в соцсетях до ранжирования веб-страниц. Всё это даст вам прочную математическую базу для понимания работы графовых свёрточных сетей (GCN) и других современных методов графового машинного обучения.\n\nТеперь, когда цели ясны, самое время начать!\n\n## Спектр графа и диффузионные процессы\n\nКак устроен граф? Насколько он связен? Есть ли в нём сообщества, или, напротив, он весь почти распадается? Представьте, что граф — это музыкальный инструмент, например гитара.\n\nКогда вы играете на инструменте, дёргая струны, он звучит на своем уникальном наборе резонансных частот. Вот и граф, если его представить как колеблющуюся систему, имеет свои частоты вибрации. Эти частоты и есть собственные значения. А вместе они дают спектр графа — как спектр излучения или звука.\n\nИ эта аналогия с частотами не просто красивая метафора: на ней построено целое научное направление, которое позволяет анализировать данные на графах так же строго, как инженеры анализируют звуковые или радиосигналы.\n\nВ теории графов это направление называется `обработкой сигналов на графах` (англ. Graph Signal Processing, GSP). Оно расширяет традиционную обработку сигналов на случай, когда данные представлены в виде графов.\n\nПрежде чем погружаться в математику, давайте посмотрим на GSP в действии.\n\nПредставьте умный дом с десятками датчиков температуры. Один датчик установлен неудачно и стабильно «шумит» на пару градусов. Если принять его значение за истину, система климат-контроля начнёт ошибаться.\n\nИдея GSP проста: мы доверяем не одному числу, а сети в целом. Значение в узле корректируется с оглядкой на соседей по графу (план дома), поэтому наш выбивающийся датчик мягко подтягивается к согласованной картине. В итоге мы получаем устойчивые оценки и корректное управление.\n\nКроме того, GSP применяется и в других областях:\n\n- Нейробиология (ЭЭГ/МРТ). Мозг моделируют здесь как граф функциональных/структурных связей. GSP помогает фильтровать шумы в сигналах, выделять сообщества нейронов, образующие паттерны активности, и локализовать источники сигналов.\n- Социальные сети. Анализ и моделирование диффузии новостей или мнений, выявление источников и влиятельных пользователей, фильтрация бот-активности, улучшение рекомендаций.\n- Транспортные системы. Трафик рассматривается как сигнал на узлах/рёбрах дорожного графа. GSP используют для краткосрочного прогноза пробок, обнаружения аномалий и оптимизации маршрутов или светофорных планов.\n\nВсе эти задачи объединяет одна простая идея: чтобы понять сигнал, нужно учитывать структуру сети, по которой он распространяется. Давайте теперь разберём ключевые математические инструменты и принципы, которые позволяют это делать.\n\nВ общем случае сигналом на графе будем называть вектор значений на вершинах $f:V\\to\\mathbb{R}$. Чтобы фильтровать, сглаживать или извлекать структуру из таких сигналов, нужно ввести понятие частот на графе — аналогов синусоид в классическом [Фурье-анализе](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%80%D0%BC%D0%BE%D0%BD%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7).\n\nНо откуда берутся сами синусоиды в классическом анализе? Они являются собственными функциями `оператора Лапласа $\\Delta$` — то есть функциями, которые этот оператор лишь растягивает, не меняя их формы. Эта идея даёт нам ключ к решению: чтобы найти частоты на графе, нам нужно сначала определить графовый аналог оператора Лапласа, а затем найти его собственные векторы.\n\nДля привычных нам функций оператор Лапласа $\\Delta f$ интуитивно можно воспринимать как меру отклонения значения функции в точке от её среднего значения в малой окрестности этой точки.\n\n\u003e 💡Для функции двух переменных $f(x, y)$, оператор Лапласа (или лапласиан) определяется как сумма её вторых частных производных:\n\u003e\n\u003e $$\\Delta f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\n\u003e $$\n\u003e \n\u003e В общем случае для функции $n$ переменных он является дивергенцией её градиента и равен сумме всех чистых вторых производных:\n\u003e\n\u003e $$\\Delta f = \\nabla\\cdot(\\nabla f) = \\sum_{i=1}^{n} \\frac{\\partial^2 f}{\\partial x_i^2}\n\u003e $$\n\nЗнак лапласиана показывает, как значение функции в точке соотносится со средним по её соседям:\n\n- Если лапласиан в точке большой и положительный, то функция в этой окрестности вогнута, похожа на яму, и её значение в центре меньше среднего по окрестности.\n- Если отрицательный — функция выпукла, словно похожа на холм, и значение в центре больше среднего.\n\nГраница между этими случаями — нулевое отклонение. Уравнение Лапласа $\\Delta f = 0$ определяет гармонические функции: в каждой точке их значение совпадает со средним по малой окрестности. В физике это соответствует стационарному режиму — например, из уравнения теплопроводности $\\partial_t u=\\kappa\\Delta u$ при $\\partial_t u=0$ остаётся $\\Delta u=0$; аналогично для электростатического потенциала в областях без зарядов.\n\nОт этой непрерывной картины перейдем к дискретной. Для неориентированного взвешенного графа с матрицей смежности $A=(w_{ij})$, где $w_{ij}=w_{ji} \\ge 0$, графовый лапласиан, или `матрица Кирхгофа`, задаётся как:\n\n$$L = D - A,\n$$\n\nздесь $D = \\mathrm{diag}(d_1, \\dots, d_n)$ — диагональная матрица взвешенных степеней вершин, где $d_i = \\sum_j w_{ij}$.\n\nДействие графового лапласиана на сигнал $f \\in \\mathbb{R}^n$ (вектор значений на вершинах) раскрывает суть самой матрицы:\n\n$$(Lf)i = \\sum{j \\in N(i)} w_{ij} (f_i - f_j)\n$$\n\nЭта формула наглядно показывает, что лапласиан измеряет несогласованность значения в вершине со значениями её соседей, взвешенную по силе связей. Это и есть прямой дискретный аналог оператора Лапласа.\n\nЧасто в алгоритмах используют и *нормализованные варианты* лапласиана, которые понадобятся нам в дальнейшем:\n\n$$L_{\\mathrm{rw}} = I - D^{-1}A \\quad (\\text{random-walk})\n$$\n\n$$L_{\\mathrm{sym}} = I - D^{-1/2}AD^{-1/2} \\quad (\\text{симметричный})\n$$\n\nКстати, второе название — «матрица Кирхгофа» — не случайно. Если представить взвешенный граф как электрическую сеть, где вершины — это узлы, а вес ребра $w_{ij}$ — это проводимость (величина, обратная сопротивлению) между ними, то система уравнений, описывающая токи в сети на основе законов Кирхгофа, примет в точности ту же матричную форму, что и лапласиан.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_2_1_78f5740889.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Слева — схема электрической цепи (резистивной сети), справа — её эквивалентное представление в виде графа\"\n  \u003e\n  \u003cfigcaption\u003e\n\nСлева — схема электрической цепи (резистивной сети), справа — её эквивалентное представление в виде графа.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nЭта глубокая связь позволяет применять аппарат линейной алгебры к физическим задачам и наоборот.\n\n{% cut \"Связь с физикой и комбинаторикой\" %}\n\n**Электрические сети.** Если веса рёбер — это проводимости, то $L$ — узловая матрица проводимостей. Эффективное сопротивление между вершинами $i$ и $j$ выражается через псевдообратную матрицу $L^{+}$: $R_{\\mathrm{eff}}(i,j)=(e_i-e_j)^{\\top}L^{+}(e_i-e_j)$.\n\n**Теорема о деревьях.** В комбинаторике, по теореме Кирхгофа, любой алгебраический кофактор матрицы $L$ — минор с правильным знаком — равен числу остовных деревьев в графе.\n\n{% endcut %}\n\nДальше воспользуемся этим и изучим собственные векторы и значения получившейся матрицы:\n\n$$Lu_i = \\lambda_i u_i\n$$\n\nЗдесь $u_i$ назовем `модами` графа, а $\\lambda_i$ — `частотами`. Именно этот набор собственных значений и составляет то, что формально называют `спектром графа`.\n\n\u003e 💡Спектром графа называется мультимножество, или упорядоченный набор, собственных значений $\\{\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\}$ его графового лапласиана. Этот набор чисел является своего рода визитной карточкой, которая несёт в себе информацию о ключевых структурных свойствах графа.\n\nТеперь, когда у нас есть формальное определение, давайте разберём ключевые свойства спектра. Для этого упорядочим собственные значения лапласиана по возрастанию:\n\n$$0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n \n$$\n\nВот что они нам говорят о структуре графа:\n\n1. Число нулевых собственных значений равно числу компонент связности графа. Если граф связен, то $\\lambda_1$ — единственный нулевой элемент спектра.\n2. Второе собственное значение ($\\lambda_2$) — это алгебраическая связность, или число Фидлера. Оно показывает, насколько хорошо граф связан в целом: $\\lambda_2\u003e0$ тогда и только тогда, когда граф связен. Чем больше $\\lambda_2$, тем труднее разрезать граф на две части. Соответствующий собственный вектор $u_2$ (вектор Фидлера) используется для спектральной кластеризации.\n\nДавайте посмотрим, как эти теоретические свойства работают на практике. Рассмотрим на примере [Karate Club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) — классического графа из социологического исследования Уэйна Захари (1977), описывающего взаимодействия членов университетского клуба карате. Вершины представляют участников клуба, рёбра — дружеские связи.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_2_2_015ca8866f.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Визуализация спектрального разбиения графа «Клуба карате».\"\n  \u003e\n  \u003cfigcaption\u003e\n\nВизуализация спектрального разбиения графа «Клуба карате». Цвета соответствуют двум сообществам, определённым по знаку компонент вектора Фидлера.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ примере:\n\n- Граф связен, поэтому в спектре лапласиана ровно один нуль ($\\lambda_1=0$ кратности 1).\n- Второе собственное значение $\\lambda_2 \u003e 0$ указывает на общую связность и «хрупкость» разреза: чем оно больше, тем труднее разделить клуб на две большие части.\n- Вектор Фидлера $u_2$ даёт одномерную координату для каждой вершины: разделение по знаку (или порогу) компонент $u_2$ разбивает граф на две группы, что хорошо согласуется с реальным расколом клуба, который произошёл в результате конфликта между инструктором и администрацией.\n\nПрактически это выглядит так: строим $L=D-A$ (или $L_{\\text{sym}}$), считаем собственные пары, сортируем $\\lambda_1 \\le \\lambda_2 \\le \\cdots$, берём $u_2$ и делим вершины по знаку его компонент — получаем разбиение, которое визуально совпадает с историческим разделением в клубе.\n\nМы только что увидели, как всего один собственный вектор ($u_2$) помог нам понять глобальную структуру графа. Это подводит к логичному вопросу: а что, если использовать не один, а все собственные векторы сразу?\n\nОказывается, они обладают фундаментальным свойством: собственные векторы лапласиана образуют ортонормированный базис, по которому можно разложить любой сигнал на графе.\n\n## Графовое Фурье-преобразование (GFT)\n\nПусть $L = U \\Lambda U^\\top$ — спектральное разложение графового лапласиана. Тогда `графовое Фурье-преобразование` задаётся парой операций:\n\n$$\\hat f = U^\\top f \\quad \\text{(анализ)}, \n\\qquad \nf = U\\hat f \\quad \\text{(синтез)}.\n$$\n\nЗдесь $\\hat f_i$ — спектральные коэффициенты на частотах $\\lambda_i$. Малые $\\lambda_i$ соответствуют гладким (низкочастотным) компонентам (значения в соседних вершинах близки), большие $\\lambda_i$ — осциллирующим (высокочастотным).\n\nГладкость сигнала удобно измерять квадратичной формой лапласиана:\n\n$$f^\\top L f = \\sum_{i=1}^n \\lambda_i \\hat f_i^{2}.\n$$\n\nИз формулы видно, что, подавляя компоненты с большими $\\lambda_i$, мы выполняем низкочастотную (low-pass) фильтрацию и тем самым сглаживаем сигнал.\n\nЗамечания:\n\n- Для связного графа $\\lambda_1=0$. DC-компонента: $u_1 \\propto \\mathbf 1$ для невзвешенного $L=D-A$; для симметрически нормализованного $L_{\\mathrm{sym}}$ — $u_1 \\propto D^{1/2}\\mathbf 1$.\n- Далее, говоря о GFT и фильтрах, будем использовать базис собственных векторов $L_{\\mathrm{sym}}$ — это удобно для ортонормировки.\n- Полная спектральная декомпозиция дорога ($\\mathcal O(n^3)$), поэтому на практике применяют полиномиальные приближения (например, Чебышёв), к которым мы вернёмся в разделе про фильтры.\n\nЭтот аппарат и есть отправная точка для свёрток, фильтров и графовых нейросетей (GNN).\n\n\u003e 💡`GNN` (англ. Graph Neural Networks, графовые нейронные сети) — это архитектура глубокого обучения, которая обрабатывает данные на графах, учитывая как признаки вершин и рёбер, так и их структуру.\n\u003e\n\u003e Подробно о GNN можно прочитать в соответствующей главе хендбука по машинному обучению.\n\nКроме того, спектральное представление напрямую связывает структуру графа с динамическими процессами на нём. Один из важнейших примеров таких процессов — это `диффузия`: постепенное сглаживание сигнала за счёт обмена значениями между соседними вершинами.\n\n## Диффузия\n\nВ классической диффузии (например, теплопроводности) скорость изменения температуры в точке пропорциональна разности между её значением и значениями соседей. На примере с сетью IoT-датчиков можно думать про это так, что в одной комнате включили обогреватель и теперь тепло растекается по нашему графу.\n\n![gif](https://yastatic.net/s3/education-portal/media/HB_Algoritm07_1_optimized_bef6a44f72.gif)\n\nФормализуем эту идею через дифференциальное уравнение:\n\n$$\\frac{d f_i}{d t}=\\sum_{j \\in N(i)} w_{i j}\\left(f_j-f_i\\right),\n$$\n\nгде:\n\n- $f_i(t)$ — значение вектор-столбца сигнала (температуры) в вершине $i$;\n- $N(i)$ — соседи вершины $i$;\n- $w_{ij}$ — вес ребра.\n\nВ матричной форме это уравнение элегантно записывается через графовый лапласиан:\n\n$$\\frac{d f}{d t} = -Lf.\n$$\n\nРешением этого уравнения будет:\n\n$$f(t) = e^{-tL} f(0),\n$$\n\nа матрицу $e^{-tL}$ называют `оператором теплопроводности` (англ. heat kernel). Эта матрица является некоторой машиной времени: она берёт начальный сигнал $f(0)$ и показывает, как он будет выглядеть после растекания по графу в течение времени $t$.\nПолезно взглянуть на решение в спектральной базе. Пусть $L=U\\Lambda U^\\top$. Тогда\n\n$$f(t)=U\\,e^{-t\\Lambda}U^\\top f(0)\\quad\\Rightarrow\\quad \\hat f_i(t)=e^{-t\\lambda_i}\\,\\hat f_i(0). \n$$\n\nЭта формула наглядно показывает, что низкочастотные компоненты (где $\\lambda_i$ малы) затухают медленно, а высокочастотные (где $\\lambda_i$ велики) — очень быстро. Это и есть суть low-pass-сглаживания, которое неразрывно связывает диффузию со спектром графа.\nПосмотрим на диффузию с другой стороны — как на усреднение по соседям. Тогда мы хотим, чтобы сигнал в вершине $i$ на следующем шаге был средним значением сигналов её соседей, нормированным на их количество. Это приводит к матрице \n\n$$P=D^{−1}A,$$\n\nгде каждая строка $i$ нормируется так, что $\\sum_j P_{ij} = 1$. Эта матрица называется `матрицей перехода`, и она описывает `случайное блуждание` по графу: $P_{ij}$ — вероятность перейти из вершины $i$ в вершину $j$.\n\n{% cut \"Как случайное блуждание может быть полезным\" %}\n\nНа самом деле случайное блуждание в машинном обучении встречается сплошь и рядом. Во временных рядах ситуацию, когда значение временного ряда предсказывают как значение на прошлом шагу плюс некий случайный шум, тоже иногда называют случайным блужданием.\n\nВ графах мы будем использовать случайные блуждания дальше для получения признакового описания графа. В путешествиях вы часто гуляете по городу, чтобы потом описать его друзьям. А чем графы хуже и почему бы по ним тоже не «погулять»?\n\n{% endcut %}\n\nА как матрица перехода связана с диффузией? Если мы хотим моделировать диффузию дискретными шагами времени, то новое состояние $f^{(t+1)}$ можно получить как:\n\n$$f^{(t+1)} =Pf^{(t)}.$$\n\nЭто дискретный аналог уравнения $\\frac{d f}{d t} = -Lf$. Давайте теперь перепишем в ином виде:\n\n$$f^{(t+1)} - f^{(t)} = (P - I)f^{(t)},$$\n\nгде $I$ — единичная матрица. Теперь можно ввести понятие нормализованного несимметричного, или `лапласиана случайного блуждания` (англ. random walk):\n\n$$L_{rw} = I - P = D^{-1} (D-A).$$\n\nНаш процесс теперь можно переписать в виде:\n\n$$f^{(t+1)} - f^{(t)} = -L_{rw}f^{(t)} $$\n\nА при малых шагах $dt$ и вовсе получим уже известное уравнение:\n\n$$\\frac{d f}{d t} = -L_{rw}f $$\n\nОстановимся поподробнее на `случайном блуждании`. В случае достаточно удачного графа, а именно:\n\n1. Из каждой вершины можно добраться до любой другой.\n1. Для дискретных графов блуждание положительно возвратное (вероятность вернуться в это же состояние когда-нибудь в будущем больше нуля) и апериодическое.\n1. Вероятности переходов между узлами положительные.\n\nЧто произойдёт, если мы будем совершать очень много шагов по графу? Если граф связен и не имеет «патологических» периодических структур (является апериодическим), то после множества шагов вероятность оказаться в любой вершине перестанет меняться и сойдётся к стационарному распределению $\\pi$.\n\nСтационарное распределение — это такое распределение вероятности, которое не меняется с течением времени. Такое «вектор-строка»-распределение $\\pi$ удовлетворяет уравнению\n\n$$ \\pi = \\pi P, \\quad \\text{при условии} \\quad \\sum_i \\pi_i = 1.$$\n\nИными словами, стационарное распределение — нормированный на 1 собственный вектор матрицы $P$ при собственном значении 1.\n\nВ случае ненаправленного связного графа с положительными весами получаем\n\n$$\\pi_i = \\frac{d_i}{\\sum_j d_j},$$\n\nгде $d_i$ — степень вершины $i$. Это означает, что в долгосрочной перспективе вероятность оказаться в конкретной вершине пропорциональна её степени. Это естественный результат: если у вас в какой-то узел приходит 5 труб, а в какой-то всего 2, то вероятность оказаться в узле с 5 трубами выше.\n\nТеперь мы можем перейти к одной из самых известных задач на графах — `PageRank`.\n\n## PageRank\n\nЭто алгоритм, изначально созданный для ранжирования веб-страниц. Он моделирует поведение случайного пользователя, который случайным образом переходит по ссылкам на страницах.\n\nПредставьте себе огромную библиотеку, где миллиарды сайтов ссылаются друг на друга. Как в интернете найти действительно полезную и авторитетную информацию? В 1998 году два аспиранта Стэнфордского университета — Ларри Пейдж и Сергей Брин — предложили простую идею: давайте измерять «важность» страницы по тому, сколько раз на неё ссылаются другие страницы.\n\nИ не просто по количеству ссылок, а с учётом того, насколько сами эти страницы авторитетны. Так появился PageRank — алгоритм, положивший начало поисковой системе Google.\n\nГрубо говоря, они применили принцип, распространённый в научном сообществе: у учёных и по сей день одной из важных метрик считается цитируемость — индекс Хирша. Если вы пишете крутые статьи — значит, на вас часто будут ссылаться.\n\nРассматриваем интернет как ориентированный граф: вершины — страницы сайтов, рёбра — ссылки между ними. Дальше вводим вероятность перехода по ссылкам с текущей страницы $\\alpha$. То есть с вероятностью $\\alpha$ человек перейдёт по ссылкам, а с вероятностью $1-\\alpha$ перейдёт на другую случайную страницу. Это сделано для того, чтобы избежать тупиков и периодичности. Как же это связано с диффузией?\n\n![gif](https://yastatic.net/s3/education-portal/media/HB_Algoritm06_1_optimized_23c78b72ab.gif)\n\nВыше мы получали $f^{(t+1)} = Pf^{(t)}$ для вектор-столбца сигнала в задаче диффузии. Давайте применим это к вектор-строке вероятности: $p^{(t+1)} = p^{(t)}P$.\nВспомним, что в PageRank у нас есть $\\alpha$. Можно ли это учесть? Вполне!\n\n$$p^{(t+1)} = \\alpha p^{(t)}P + (1-\\alpha)v,$$\n\nгде $v$ — вектор перехода на случайную страницу; $0 \u003c \\alpha \u003c 1$. Если он равномерный (то есть все его значения равны), то получим обычный PageRank.\nНо это не обязательно, мы можем сделать его и персонализированным (какие-то значения больше других). Например, пользователь не перейдёт на случайный сайт, а предпочтёт вернуться на сайты с красивым дизайном. Такая модификация называется `Personalized PageRank`.\n\nРешим задачу PageRank в пределе стационарного распределения $p^{(t+1)} = p^{(t)} = p$:\n\n$p = \\alpha pP + (1-\\alpha)v$\n\n$p - \\alpha pP = (1-\\alpha)v$\n\n$p (I- \\alpha P) = (1-\\alpha)v$\n\n$p = (1-\\alpha)v(I- \\alpha P)^{-1}$\n\nВ последнем действии есть важный нюанс: мы считаем, что обратная матрица существует. Что для этого надо?\n\n1. $0 \u003c \\alpha \u003c 1$\n1. Если у вершины $i$ нет исходящих рёбер, то строка $i$ в $P$ не определена. Надо это исправить, например заменив пустую строку на $v$.\n\nПерепишем задачу PageRank через лапласиан, чтобы увидеть её связь с процессами сглаживания на графе.\n\nКлюч — связь матрицы переходов $P$ и нормализованного лапласиана случайного блуждания $L_{rw}=I-P$. Подставим $P=I-L_{rw}$ в стационарное уравнение:\n\n1. Исходное уравнение:\n\n$$p=\\alpha\\,pP+(1-\\alpha)\\,v$$\n\n2. Подстановка и группировка:\n\n$$p=\\alpha\\,p(I-L_{rw})+(1-\\alpha)\\,v\n\\quad\\Longrightarrow\\quad\np-\\alpha p+\\alpha pL_{rw}=(1-\\alpha)\\,v$$\n\n3. Итоговая линейная система:\n\n$$p\\bigl((1-\\alpha)I+\\alpha L_{rw}\\bigr)=(1-\\alpha)\\,v$$\n\nЭта запись раскрывает PageRank как задачу сглаживания с «якорем»:\n\n- член $\\alpha\\,pL_{rw}$ отвечает за сглаживание (делает значение узла ближе к среднему по соседям);\n- член $(1-\\alpha)I$ — это «якорь», который тянет решение к вектору телепортации $v$.\n\n{% block padding=s border=dashed %}\n\nЧем выше $\\alpha$, тем сильнее влияние структуры графа (сглаживание) и слабее влияние «якоря». Формула с обратной матрицей из предыдущего вывода — это явное решение этой системы.\n\nУравнение PageRank можно переписать в ещё более компактной форме. Если ввести параметр $\\beta = \\alpha / (1-\\alpha)$, то линейная система примет вид:\n\n$$p (I + \\beta L_{rw}) = v$$\n\nЭта запись подчёркивает роль $\\beta$ как коэффициента гладкостной регуляризации на графе. Она показывает, что итоговый ранг $p$ — это некий компромисс между близостью к исходному вектору $v$ (за это отвечает $I$) и гладкостью решения на графе (за это отвечает $L_rw$).\n\n{% endblock %}\n\n{% cut \"Столбцовая запись и симметричная форма\" %}\n\nВ этом параграфе мы использовали строчную запись: $p$ — строковый вектор, $p^{(t+1)}=p^{(t)}P$, а $P=D^{-1}A$ — построчно-стохастическая матрица переходов.\n\n**Столбцовая формулировка**\n\nЭквивалентная запись со столбцовыми векторами (часто встречается в литературе):\n\n$$\\pi=\\alpha\\,P^{\\top}\\pi+(1-\\alpha)\\,v,\\qquad \\sum_i \\pi_i=1.$$\n\nЭто даёт линейную систему:\n\n$$\\bigl((1-\\alpha)I+\\alpha L_{rw}^{\\top}\\bigr)\\,\\pi=(1-\\alpha)\\,v,\\quad L_{rw}=I-P.$$\n\nДля узлов без исходящих рёбер («висячие» узлы) соответствующие строки $P$ нужно корректировать (например, заменять вектором $v$), чтобы система была корректно определена.\n\n**Симметричная СЛАУ для неориентированных графов**\n\nЕсли граф неориентированный ($A=A^\\top$), то систему можно сделать симметричной. Подстановкой $y=D^{-1/2}\\pi$ получаем симметричную положительно определённую систему:\n\n$$\\bigl((1-\\alpha)I+\\alpha L_{\\mathrm{sym}}\\bigr)\\,y=(1-\\alpha)\\,D^{-1/2}v,$$\n\nчто удобно для численных решателей (метод сопряжённых градиентов, разложение Холецкого и другие).\n\n{% endcut %}\n\nМы увидели, как PageRank использует диффузионный процесс для распространения «важности» по графу.\n\nОказывается, этот же мощный механизм можно применить и для совершенно другой задачи — полунадзорной классификации. Вместо того чтобы распространять ранг, мы будем распространять известные метки классов от немногих размеченных вершин ко всем остальным. Этот алгоритм называется Label Propagation.\n\n## Label Propagation\n\nЭто один из базовых алгоритмов _полунадзорного обучения_ на графах — подхода, при котором для обучения модели используется смесь размеченных и неразмеченных данных.\n\nВ нашем случае известные метки распространяются по рёбрам на соседние вершины — как краска, падающая в воду и постепенно окрашивающая всё вокруг. Со временем распределения меток в узлах выравниваются, и вершины с изначально неизвестной категорией получают метку в зависимости от того, какие классы преобладают среди их соседей.\n\nДопустим, что есть граф, в котором у части вершин мы знаем их метки, а у части — нет. Обозначим матрицу исходных меток $Y$. Для каждой размеченной вершины соответствующая ей строка в $Y$ будет вектором из нулей, где на позиции, соответствующей номеру класса, стоит единица (такие векторы называются one-hot). Строки для неразмеченных вершин будут состоять из одних нулей.\n\nСделаем ещё одну матрицу $F$, в строках которой будет лежать распределение вероятности меток для каждой вершины. Её и будем искать. Запишем задачу через процесс диффузии:\n\n$$F^{(t+1)} = \\alpha PF^{(t)} + (1-\\alpha)Y$$\n\nМатрицу $F$ инициализируем $F^{(0)} = Y.$ Не будем подробно останавливаться на расписывании решения, поскольку оно аналогично предыдущему. Отметим, что здесь также нужно перейти к пределу, и тогда можно получить:\n\n$$F=(1-\\alpha)(I-\\alpha P)^{-1} Y .$$\n\nОбратите внимание на поразительное сходство этой формулы с решением для PageRank. В обоих случаях ядром является одна и та же матричная структура $(I-\\alpha P)^{-1}$, что подчёркивает единство лежащего в их основе диффузионного процесса.\n\n## Геометрия через диффузию\n\nМы уже увидели, что и диффузия, и PageRank используют одну и ту же динамику по графу:\n\n- непрерывную, через тепловой оператор $e^{-tL_{\\mathrm{sym}}}$ (heat kernel);\n- дискретную — через матрицу переходов $P=I-L_{\\mathrm{rw}}$ (где $L_{\\mathrm{rw}}$ связан с $L_{\\mathrm{sym}})$.\n\nНо на практике часто нужны не только сглаживание и фильтрация сигнала, а _компактные координаты вершин_, в которых обычное евклидово расстояние отражает близость по сети.\n\nДиффузионные карты (англ. `Diffusion Maps`) как раз это и делают: берут оператор диффузии и строят эмбеддинг низкой размерности, где расстояния между точками аппроксимируют вероятность добраться друг до друга за $t$ шагов случайного блуждания (так называемую `диффузионную дистанцию`). Это удобно для визуализации, кластеризации и поиска скрытой геометрии данных — особенно когда прямые расстояния плохо передают структуру графа/многообразия.\n\nПредставьте себе парк с дорожками, которые извиваются между деревьев. Люди могут ходить только по этим дорожкам. Если смотреть на парк сверху, то расстояния между людьми можно измерить «по прямой» — так, как летел бы самолет. Но на самом деле люди могут добраться друг до друга только вдоль дорожек, и прямое расстояние не отражает их истинную «близость».\n\n![imag](https://yastatic.net/s3/education-portal/media/8_2_4_82bae48320.webp)\n\nИдею диффузионных карт можно визуализировать так: представьте, что мы запускаем маленького муравья, который бежит по дорожкам (графу) случайным образом. Мы смотрим, насколько быстро он может добраться из одной точки в другую, и эти «времена добегания» формируют новую меру расстояния. Затем мы «сворачиваем» карту парка так, чтобы эти времена (а не прямые линейные расстояния) сохранялись. Ещё одна аналогия: как будто муравей ползет по коктейльной трубочке, а мы по спирали разворачиваем её в линию.\n\nВ спектральных терминах это эквивалентно применению низкочастотного фильтра, который подавляет резкие изменения между соседними вершинами и усиливает плавные, согласованные паттерны.\n\nБолее формально это делается через спектральное разложение графового лапласиана. Пусть\n\n$$L_{\\mathrm{sym}} = U\\Lambda U^\\top,\\qquad \n\\Lambda=\\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots,\\lambda_n), 0=\\lambda_1\\le\\lambda_2\\le\\cdots.$$\n\nТогда координаты Diffusion Maps масштаба $t$ задаются как\n\n$$\\Phi_t(i)=\\bigl(e^{-t\\lambda_2}\\,u_2(i), e^{-t\\lambda_3}\\,u_3(i),\\ldots, e^{-t\\lambda_m}\\,u_m(i)\\bigr),$$\n\nи евклидово расстояние $\\|\\Phi_t(i)-\\Phi_t(j)\\|$ аппроксимирует диффузионную дистанцию между вершинами $i$ и $j$. Параметр $t$ — это «масштаб обзора»: малые $t$ подчеркивают локальную структуру, большие $t$ — более глобальные паттерны. Заметим, что множители $e^{-t\\lambda_k}$ играют роль низкочастотного фильтра: чем больше $\\lambda_k$, тем сильнее подавляется соответствующая компонентa.\n\nЭта связь между динамикой диффузии и частотными компонентами приводит нас к более общей идее — `графовых фильтров`, которые позволяют избирательно усиливать или подавлять определённые частоты сигнала на графе. Именно через такие фильтры строятся многие современные алгоритмы, включая графовые свёрточные сети.\n\n## Фильтры\n\nВ классическом машинном обучении и обработке сигналов фильтры помогают выделять важные частоты и подавлять шум. Но как фильтровать данные, заданные не на регулярной решётке (например, изображении), а на графе — структуре с произвольной связностью?\n\nПредставьте социальную сеть, где у каждого пользователя есть мнение по какому-то вопросу (сигнал на вершинах). Часть мнений шумная или экстремальная. Задача — получить сглаженную картину по сообществам, подавив шум, но не размыв границы между группами.\n\n`Графовый фильтр` — это как раз такой инструмент: он **усредняет сигнал по соседям в графе** (друзьям в сети) с регулируемой интенсивностью и локальностью. Слабое сглаживание подавляет одиночные выбросы, а более сильное выявляет устойчивые тенденции внутри сообществ, сохраняя макроструктуру сети.\n\nПара слов о `локальности`. Она означает, что влияние фильтра должно распространяться не на весь граф, а только на окрестность вершины (ближайшие соседи, соседи через одного). Это важно для вычислительной эффективности, и кажется логичным, что изменение сигнала в одной вершине должно влиять только на близлежащие вершины, а не на весь граф.\n\nДля построения эффективных фильтров нам снова понадобится `нормализованный симметричный лапласиан`, который мы уже вводили ранее:\n\n$$L_{\\mathrm{sym}} = I - D^{-\\frac12} A D^{-\\frac12}.$$\n\nОн обладает парой удобных свойств:\n\n1. Он симметричен.\n1. Все его собственные значения лежат в диапазоне $[0, 2]$, а максимум достигается тогда и только тогда, когда граф `двудольный`, — как мы уже знаем, это граф, вершины которого можно разбить на две доли так, что рёбра соединяют только вершины из разных долей.\n\n### Спектральные графовые фильтры\n\nОбщая идея графового фильтра — это операция, которая изменяет спектр сигнала на графе. Мы можем усилить или ослабить определённые частоты (собственные значения $\\lambda_i$), чтобы, например, сгладить сигнал (подавить высокие частоты) или, наоборот, выделить резкие изменения (усилить высокие частоты).\n\nФормально это реализуется через спектральное разложение лапласиана $L_{\\mathrm{sym}} = U \\Lambda U^\\top$. Графовый фильтр — это оператор $g_\\theta(L_{\\mathrm{sym}})$, который в спектральном представлении действует как функция $g_\\theta$ на собственные значения:\n\n$$g_\\theta(L_{\\mathrm{sym}}) = U g_\\theta(\\Lambda) U^\\top$$\n\nЗдесь $g_\\theta(\\Lambda)$ — это диагональная матрица, где каждый элемент — это результат применения функции $g$ к соответствующему собственному значению, а $\\theta$ — параметры, определяющие форму фильтра (например, коэффициенты полинома).\nПрименение такого фильтра к сигналу $f$ выглядит как:\n\n$$y = g_\\theta(L_{\\mathrm{sym}}) f = U g_\\theta(\\Lambda) U^\\top f$$\n\nЭтот подход красив и понятен:\n\n1. **Анализ.** Переводим сигнал в спектральное представление ($U^\\top f$).\n1. **Фильтрация.** Умножаем на коэффициенты фильтра ($g_\\theta(\\Lambda)$).\n1. **Синтез.** Возвращаем сигнал обратно ($U \\dots$).\n\nТо есть для применения фильтра требуется привести матрицу к диагональному виду. Для больших графов этот процесс будет долгим: в общем случае приведение матрицы к диагональному виду имеет асимптотику $O(n^3)$ для матрицы размером $n \\cdot n$. Можно ли как-нибудь обойтись без этого?\n\n### Полиномиальные фильтры\n\nОказывается, что да. Рассмотрим `полиномиальный фильтр`. То есть такой, где $g_\\theta(\\lambda)$ — полином степени $K$:\n\n$$g_\\theta(\\lambda) = \\sum_{k=0}^K \\theta_k \\lambda^k.$$\n\nЭто можно переписать в матричном виде:\n\n$$g_\\theta(L) = \\sum_{k=0}^K \\theta_k L^k,$$\n\nа применение фильтра сводится к:\n\n$$y = \\sum_{k=0}^K \\theta_k L^k x.$$\n\nЭто гораздо проще применять, поскольку нам не требуется приводить лапласиан к диагональному виду. Степень полинома $K$ при этом определяет количество соседей, которые участвуют в формировании нового пространства. Из-за этого полиномиальный фильтр получается локальным.\n\n### Фильтры Чебышёва и GCN\n\nЧтобы сделать полиномиальные фильтры более гибкими и численно устойчивыми, вместо обычных степеней $L^k$ часто используют базис из `многочленов Чебышёва` $T_k(x)$. Их главное преимущество — эффективное вычисление через рекуррентную формулу:\n\n$$T_{k+1}(x) = 2xT_k(x) - T_{k-1}(x), \\quad \\text{где} \\quad T_0(x) = 1, \\ T_1(x) = x. \n$$\n\n{% cut \"Подробнее о многочленах Чебышёва\" %}\n\nМногочлены Чебышёва первого рода обладают рядом уникальных свойств, которые делают их полезными для аппроксимации функций:\n\n- **Минимальное отклонение от нуля.** Многочлен $T_k$ степени $k$ со старшим коэффициентом $2^{k-1}$ меньше всего отклоняется от нуля на отрезке $[-1, 1]$.\n- **Чётность и нечётность.** Многочлены чётных степеней являются чётными функциями (содержат только чётные степени $x$), а нечётных — нечётными. Для лучшего понимания приведем здесь $T_7$ и $T_8:$\n    - $T_7(x) = 64x^7-112x^5+56x^3-7x$\n    - $T_8(x) = 128x^8-256x^6+160x^4-32x^2+1$\n    Легко заметить, что $2^{7-1=6}=64$ и $2^{8-1=7}=128$.\n- **Ортогональность.** Они образуют систему ортогональных многочленов, что важно для численной стабильности.\n\n{% endcut %}\n\nЧтобы применить их к графу, спектр лапласиана сначала нужно уместить в отрезок $[-1, 1]$, на котором эти многочлены обладают наилучшими свойствами:\n\n$$\\tilde{L}{\\mathrm{sym}} = \\frac{2}{\\lambda{\\max}} L_{\\mathrm{sym}} - I, \n$$\n\nгде $\\lambda_{max}$ — максимальное собственное значение $L$.\nТогда фильтр Чебышёва степени $K$ определяется как:\n\n$$g_\\theta(\\tilde{L}{\\mathrm{sym}}) = \\sum{k=0}^K \\theta_k T_k(\\tilde{L}_{\\mathrm{sym}}). \n$$\n\nПри этом мы можем воспользоваться рекуррентным соотношением выше, чтобы посчитать результат применения такого фильтра без необходимости приведения матрицы $L$ к диагональному виду.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_2_5_e4ec375e7e.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Иллюстрация графовой свёрточной сети.\"\n  \u003e\n  \u003cfigcaption\u003e\n\nИллюстрация графовой свёрточной сети.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nИменно этот подход лежит в основе `графовых свёрточных сетей` (англ. Graph Convolutional Network, GCN). Идея GCN — взять самый простой, но нетривиальный локальный фильтр Чебышёва ($K=1$) и сделать его обучаемым.\n\nВсё, что требуется, — взять $K=1$ и приблизить $\\lambda_{\\max}\\approx 2$: действие такого фильтра на сигнал $f$ можно свести к выражению с двумя параметрами, $\\theta_0$ и $\\theta_1$. Чтобы сократить их до одного и получить каноническую формулу GCN, используется трюк с перепараметризацией.\n\nПоложим, $\\theta=\\theta_0=-\\theta_1$. Это ограничение позволяет связать два параметра в один и приводит к очень изящному результату. Подставим это в одну из промежуточных форм фильтра:\n\n$$y \\;=\\; \\theta_0 f + \\theta_1\\,L_{\\mathrm{sym}}f \n\\;\\;\\Longrightarrow\\;\\;\ny \\;=\\; \\theta f \\;-\\; \\theta\\,(I - D^{-\\frac12}AD^{-\\frac12})\\,f.$$\n\nРаскрыв скобки, получаем уже чистую агрегацию соседей:\n\n$$\\tilde A = A + I,\\qquad \n\\tilde D_{ii}=\\sum_j \\tilde A_{ij},\\qquad\n\\hat A=\\tilde D^{-\\frac12}\\,\\tilde A\\,\\tilde D^{-\\frac12}.$$\n\nЗдесь возникает важный нюанс. В такой форме оператор $D^{-\\frac12}AD^{-\\frac12}$ при применении к вектору $f$ усредняет значения только по соседям вершины, полностью игнорируя исходное значение в самой вершине. На практике это часто приводит к потере информации.\n\nЧтобы решить эту проблему, в GCN используется простой, но мощный трюк `ренормализации`: сначала добавляют «петли» (англ. self-loops), а уже потом нормализуют:\n\n$$y \\;=\\; \\theta\\,\\hat A\\,f.$$\n\nЭто приводит к финальной, канонической форме слоя GCN:\n\n$$Y \\;=\\; \\hat A\\,F\\,\\Theta.$$\n\nВот как теперь работает эта операция:\n\n- $\\tilde A = A+I$ добавляет вклад самой вершины (self-loop),\n- $\\hat A=\\tilde D^{-\\frac12}\\tilde A\\tilde D^{-\\frac12}$ агрегирует информацию от соседей и самой вершины в устойчивом масштабе,\n- умножение на $\\theta$ делает фильтр обучаемым.\n\nТеперь у нас в скалярной постановке остался всего один обучаемый параметр $\\theta$ на слой, а сама операция интуитивно понятна: она усредняет вектор признаков вершины с векторами её соседей, не теряя собственный сигнал.\n\nНа практике GCN работают не с одним сигналом, а с матрицей признаков $F$, где каждая строка — это вектор признаков для одной вершины. Тогда формула обобщается:\n\n$$Y = \\hat A\\,F\\Theta.$$\n\nГде:\n\n- $F$ — матрица входных признаков;\n- $Y$ — выходные признаки;\n- $\\Theta$ — _обучаемая матрица_ весов слоя нейросети (в многоканальном случае она заменяет скаляр $\\theta$).\n\nЭта операция, по сути, для каждой вершины усредняет её вектор признаков с признаками соседей (и собственной вершины), создавая новое, обогащённое представление.\n\n### ARMA-фильтры\n\nПолиномиальные фильтры, включая фильтры Чебышёва, отлично подходят для создания гладких, в основном низкочастотных (сглаживающих) фильтров. Но что делать, если нам нужна более сложная частотная характеристика — например, выделить определённую полосу частот, подавив и низкие, и высокие?\n\nДля таких задач используют `ARMA-фильтры` (англ. AutoRegressive Moving Average, авторегрессионный фильтр скользящей средней), пришедшие из анализа временных рядов. Их ключевое отличие в том, что функция фильтра задаётся не полиномом, а _рациональной функцией_ (отношением двух полиномов). В спектральной области (для $L_{\\mathrm{sym}}$, где $\\lambda\\in[0,2]$) функция задаётся как:\n\n$$g_{a, b}(\\lambda)=\\frac{\\sum_{q=0}^{Q}b_{q}\\lambda^{q}}{1+\\sum_{p=1}^{P}a_{p}\\lambda^{p}}$$\n\nВ операторной форме это:\n\n$$g_{a,b}(L_{\\mathrm{sym}})\\;=\\;\\bigl(I+\\textstyle\\sum_{p=1}^{P} a_p L_{\\mathrm{sym}}^{\\,p}\\bigr)^{-1}\\,\\Bigl(\\sum_{q=0}^{Q} b_q L_{\\mathrm{sym}}^{\\,q}\\Bigr).$$\n\nРациональная форма позволяет строить избирательные, в том числе полосовые и режекторные, фильтры с меньшим порядком, чем у чисто полиномиальных аппроксимаций. Это полезно, когда требуется выделить средние масштабы структуры и подавить и слишком локальные всплески, и слишком глобальные тренды.\n\nНапример, в транспортной сети такой фильтр может помочь подчеркнуть районные паттерны трафика, приглушив как шум отдельных перекрёстков (высокие частоты), так и общефоновую нагрузку города (низкие частоты).\n\nЧто следует учитывать?\n\n- **Вычисления.** Реализация сводится к решению линейной системы или итерационным схемам, а это дороже, чем простая полиномиальная свёртка, например.\n- **Устойчивость.** Для численной устойчивости коэффициенты выбирают так, чтобы знаменатель не обращался в ноль на $[0,2]$.\n- **Выбор.** Если важны скорость и простота, берут полиномиальные/чебышёвские фильтры. Если нужна острая частотная селективность — выигрывает ARMA.\n\n***\n\nВсё, что мы обсуждали до этого, — спектр, диффузию и графовые фильтры — можно рассматривать как способы преобразования исходных сигналов на графе в более удобное представление, подчёркивающее нужные свойства структуры.\n\nОднако фильтры в первую очередь работают _с сигналами на фиксированном графе_.\n\nЕсли же задача — сравнивать разные графы между собой или искать похожие вершины или структуры в большом наборе графов, нам нужны другие инструменты.\n\nЗдесь на сцену выходят два подхода:\n\n- `Графовые ядра` — позволяют ввести меру сходства между графами или их элементами, что особенно полезно для алгоритмов, основанных на матрицах сходства (например, SVM).\n- `Графовые эмбеддинги` — представляют вершины, подграфы или целые графы в виде векторов фиксированной размерности, чтобы применять к ним стандартные методы машинного обучения.\n\nМы рассмотрим эти подходы в следующем параграфе, а пока советуем пройти квиз, чтобы закрепить прочитанный материал."])</script><script nonce="">self.__next_f.push([1,"81:Ta08f,"])</script><script nonce="">self.__next_f.push([1,"Когда мы работаем с графами, часто нужно понимать, насколько похожи два графа друг на друга. Например:\n\n- Похожи ли две молекулы по структуре?\n- Похожи ли два человека в соцсети по их связям?\n- Можно ли предсказать, появится ли новое соединение в молекуле, если у нас есть похожие?\n\nЧтобы это делать, нужно уметь сравнивать графы. Для обычных векторов достаточно просто сравнить похожесть. Можно, например, посчитать косинус угла между ними, тем самым оценив, направлены они в одну сторону или в разные.\n\nА как быть с графами? Тут дела обстоят сложнее, но существует два мощных подхода, которые мы последовательно рассмотрим: ядра (англ. kernels) и эмбеддинги (англ. embeddings).\n\nЗатем покажем, как на их основе решать практические задачи: кластеризацию (алгоритм Ng-Jordan-Weiss), выделение признаков для табличных ML-моделей, а также быстрый поиск похожих объектов (NSW/HNSW).\n\nНачнём с ядер.\n\n## Графовые ядра\n\nЭто функции, которые измеряют сходство между графами, не переводя их в векторы явно. В этом разделе мы поговорим о трёх ключевых типах ядер:\n\n- Ядро на случайных блужданиях (англ. Random Walk kernel) — оно сравнивает графы по набору случайных путей в них.\n- Ядро на кратчайших путях (англ. Shortest-Path kernel) — оно использует расстояния между парами вершин.\n- Ядро Weisfeiler-Lehman — это мощный мощный метод, основанный на итеративном сравнении локальных окрестностей вершин.\n\n### Ядро на случайных блужданиях\n\nВозьмем два графа. В каждом из них запустим случайное блуждание — представьте пьяницу, который случайно гуляет по улицам-рёбрам города — и посмотрим, как часто встречаются одинаковые «пути».\n\nКак ядро пьяницы записывается формулой? Для блужданий максимальной длины $l$:\n\n$$K(G_{1},G_{2})=\\sum_{i,j}\\sum_{k=0}^{l}\\lambda^{k}\\cdot P_{G_{1}}^{k}(i)\\cdot P_{G_{2}}^{k}(j),$$\n\nгде $\\lambda$ — коэффициент затухания (чем длиннее путь, тем менее он для нас важен), $P_{G_{1}}^{k}(i)$— вероятность пройти путь длиной $k$ в графе 1, начиная из вершины $i$. Этот показатель косвенно отражает структуру окрестности вершины.\n\nЕсли графы помечены (то есть вершины и/или рёбра имеют метки/атрибуты — тип атома, цвет, роль пользователя, тип связи и т. п.), совпадение путей обычно считают по последовательностям этих меток или через небольшое «ядро на метках». Если нет — по самим последовательностям вершин. Для сопоставимости значений часто используют нормировку:\n\n$$ \\tilde K(G_1,G_2)=\\dfrac{K(G_1,G_2)}{\\sqrt{K(G_1,G_1),K(G_2,G_2)}}. $$\n\nУ данного ядра есть два важных недостатка:\n\n1. Оно долго считается на больших графах, поскольку требуется перебрать очень много путей (эквивалентная матричная форма приводит к работе с произведением графов размера $|V_1|!\\cdot!|V_2|$).\n1. Случайные рёбра и топтание на месте (то есть частые возвраты туда-обратно по одному ребру), будут сильно портить результат. На практике это смягчают малым $l$ и $\\lambda$— запретом мгновенного возврата или переходом к более устойчивым ядрам.\n\nИз-за этих ограничений случайные блуждания в чистом виде используются редко. Однако сама идея исследовать граф через пути легла в основу более современных и эффективных методов, таких как эмбеддинги DeepWalk и Node2Vec, которые мы рассмотрим далее. А пока перейдём к более простому и устойчивому подходу — ядру на кратчайших путях.\n\n### Ядро на кратчайших путях\n\nДавайте вместо случайных путей рассматривать кратчайшие. Интуиция такая: если в двух графах много пар узлов с одинаковыми расстояниями, то они похожи.\n\nПусть $d(u,v)$ — кратчайшее расстояние между вершинами $u$ и $v$. Тогда ядро можно записать так:\n\n$$ K(G_{1},G_{2})=\\sum_{(u,v)\\in{G}{1}\\,(x,y)\\in{G}{2}}[d_{G_{1}}(u,v)=d_{G_{2}}(x,y)], $$\n\nгде $[\\cdots]$ обозначают индикатор (также известный как скобка Айверсона), то есть функцию, которая принимает $1$, если условие внутри выполнено, и $0$ — если нет. Суммы берутся по неупорядоченным парам ${u,v}$, чтобы не считать каждую пару дважды.\n\nЭту же идею удобно переписать через гистограммы расстояний. Для каждого графа $G$ мы можем составить функцию $h_G(\\ell)$, которая подсчитывает, сколько пар вершин находится на расстоянии $\\ell$ друг от друга:\n\n$$ h_G(\\ell)=\\bigl|\\{\\{u,v\\}\\subset V(G): d_G(u,v)=\\ell\\}\\bigr|. $$\n\nЗдесь символ $\\lvert S\\rvert$ означает мощность множества $S$ (число его элементов). Для чисел $\\lvert x\\rvert$ — это модуль, но здесь аргументом является множество.\n\nТогда формула ядра превращается в простое скалярное произведение этих гистограмм, в некое ядро на кратчайших путях:\n\n$$ K(G_{1},G_{2})=\\sum_{\\ell\\ge 0} h_{G_{1}}(\\ell) \\cdot h_{G_{2}}(\\ell).$$\n\nТакой взгляд показывает, что мы, по сути, превращаем каждый граф в вектор (гистограмму расстояний), а затем измеряем их сходство через скалярное произведение.\n\n\u003e 💡Если графы помеченные (вершины/рёбра имеют некие атрибуты), то сравнение можно ужесточить: учитывать только пары с совпадающими метками или добавить «ядро на метках» $k_{\\text{label}}(\\cdot,\\cdot)$ внутри индикатора.\n\u003e\n\u003e Для взвешенных графов часто сравнивают не точное равенство расстояний, а близость — например, через $\\exp\\{-\\gamma,(d_{G_1}-d_{G_2})^2\\}$ или бининг расстояний.\n\nТакое ядро работает быстрее, но содержит меньше информации о структуре, скажем, игнорируем узор окрестностей (сколько и какие соседи, их метки и т. д.). Для более тонкого учёта окружения вершин применяют подход `Weisfeiler-Lehman`.\n\n### Ядро Weisfeiler-Lehman\n\nЭто самое сложное ядро, которое мы разберём. Вот его алгоритм:\n\n1. Даём метку каждой вершине (по типу атома, по цвету и т. д.).\n1. Повторяем N раз (обычно 2–5): \n   - Для каждой вершины собираем метки всех соседей.\n   - Склеиваем метки $[своя, sorted(соседи)]$.\n   - Хешируем полученную строку (присваиваем новую метку).\n1. Считаем, сколько раз одинаковые метки появились в обоих графах.\n\nФормулой это можно записать так:\n\n$$ K_{WL}(G_{1},G_{2})=\\sum_{h=0}^{H}\\langle\\phi_{h}(G_{1}),\\phi_{h}(G_{2})\\rangle,$$\n\nгде $\\phi_{h}(G)\\,$— гистограмма меток на шаге $h$, $\\langle\\cdot,\\cdot\\rangle$ — скалярное произведение.\n\nИнтуиция за этим алгоритмом такая: если два узла находятся в похожем окружении (такие же соседи и структура), они получат одинаковые метки. А чем больше совпадений в метках и их истории, тем более похожи графы.\n\nВ математике существует тест на `изоморфизм`. Изоморфизм — логико-математическое понятие, выражающее одинаковость строения (структуры) систем (процессов, конструкций). Графы считаются изоморфными в том случае, если они имеют одинаковую структуру, но различный внешний вид.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_3_1_a7ee5fa666.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Пример изоморфных графов\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПример изоморфных графов.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nWeisfeiler-Lehman основан на тесте изоморфизма графов: если два графа неразличимы этим методом, то, возможно, они изоморфны (структурно одинаковы). Из этого вытекает, что это ядро ещё и удобный инструмент проверки структурной эквивалентности.\n\nТаким образом, ядра позволяют измерять степень сходства графов, неявно задавая пространство признаков.\n\nОднако иногда полезнее явно построить это пространство — представить вершины или целые графы в виде числовых векторов. Такой подход называется `эмбеддингами графов`.\n\n## Эмбеддинги графов\n\nЭмбеддинг — это способ превратить объект (например, слова в тексте, вершины или целый граф) в вектор чисел, с которым удобно работать в машинном обучении.\n\nЭтот подход нужен потому, что классические алгоритмы машинного обучения, такие как логистическая регрессия, бустинг и другие, не умеют работать с графами напрямую. Они ожидают на вход стандартную таблицу признаков. В NLP-задачах похожие эмбеддинги получают слова, встречающиеся в схожих контекстах. А как в графах?\n\nОсновной принцип такой: вершины, имеющие схожее структурное положение в графе, должны получить близкие векторные представления.\n\nРассмотрим на примере графа социальной сети. Пусть вершины — люди, а рёбра — наличие друг друга в списке друзей. Если у двух людей много общих друзей, они занимают похожее место в социальной структуре. Следовательно, хороший алгоритм построения эмбеддингов должен поместить их векторы близко друг к другу в признаковом пространстве.\n\nПредставим двух людей, которые сначала учились в одном классе, а потом в одной группе в университете. Логично, что с точки зрения социального контекста они будут похожими, и их эмбеддинги это отразят.\n\nРассмотрим несколько методов создания эмбеддингов:\n\n- DeepWalk, использующий случайные блуждания;\n- Node2Vec, который делает эти блуждания более управляемыми;\n- Graph2Vec, обобщающий идею на целые графы.\n\n### DeepWalk\n\nВыше уже неоднократно говорили про случайные блуждания. Давайте и здесь ими воспользуемся!\n\nАлгоритм будет такой:\n\n1. Запускаем из каждой вершины несколько случайных прогулок (например, длиной 10–80 шагов). Каждая прогулка — это «предложение» из вершин, или последовательность узлов.\n1. Накопив много таких последовательностей, получаем «текст» на алфавите вершин.\n1. На этом сгенерированном корпусе предложений обучается модель [Word2Vec](https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Word2Vec%3A%20a%20Prediction%2DBased%20Method) (CBOW или Skip-Gram с negative sampling). Это популярная нейросетевая модель из NLP, которая получает векторы для слов, предсказывая их контекст в окне размера $w$.\n\n{% block padding=s border=dashed %}\n\nУзлы графа $\\rightarrow$ «слова»,\nпоследовательности из случайных блужданий $\\rightarrow$ «предложения».\n\n{% endblock %}\n\nКак видите, идея DeepWalk заключается в использовании хорошо зарекомендовавшего себя алгоритма с модификациями: узлы графа — это слова, а случайные блуждания — это предложения.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_3_2_99c2bbf213.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"Пример одного случайного блуждания\"\n  \u003e\n  \u003cfigcaption\u003e\n\nПример одного случайного блуждания: A → B → D → C → E. Таких нужно собрать много, после чего можно приступать к обучению Word2Vec.\n\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nВ итоге мы получим матрицу эмбеддингов $\\Phi \\in \\mathbb{R}^{|V|\\times d}$: у каждой вершины есть $d$-мерный вектор. Вершины, которые часто встречаются вместе в коротких прогулках, то есть находятся в похожем сетевом окружении, получают близкие векторы. Эти эмбеддинги удобно использовать для:\n\n- классификации вершин,\n- предсказания рёбер (англ. link prediction),\n- поиска похожих узлов и кластеризации (косинусное сходство и т. п.).\n\nИз практических настроек достаточно задать длину прогулки $\\ell$, число прогулок с каждой вершины $r$, размер окна $w$, размерность $d$ и число негативных примеров при обучении.\n\n### Node2Vec\n\nРазобранный выше алгоритм работает неплохо, но он гуляет случайно и не может отличить разные типы соседей: близких соседей от узлов, которые находятся в одной «роли». Тем не менее зачастую нам хочется сфокусироваться на локальной или на глобальной структуре.\n\nЗдесь стоит вспомнить про алгоритмы обхода в глубину (англ. Depth-First Search, DFS) и обхода в ширину (англ. Breadth-First Search, BFS). Возможно ли мотивы из этих алгоритмов применить для подсчета эмбеддингов?\n\nДа, давайте запустим случайную прогулку с модификацией: на каждом шаге выбираем следующую вершину не просто равновероятно из соседей, а с весами, зависящими от того, откуда мы пришли.\n\nПусть теперь вероятность перехода из узла $v$ в узел $x$ зависит от предыдущего узла $t$, из которого мы пришли в $v$. Эта вероятность определяется весом:\n\n$$\\pi_{v x}=\\alpha_{p q}(t,x)\\cdot w_{v x},$$\nгде $w_{v x}$ — вес ребра между узлами $v$ и $x$ (в общем случае он необязательно равен $1$), $\\alpha_{p q}(t,x)$ — bias, или смещение, который рассчитывается следующим образом:\n\n$$\\alpha_{p q}(t,x)=\\left\\{\\begin{array}{l l}{{\\frac{1}{p}}}\u0026{{\\mathrm{если}\\,d_{t x}=0}}\u0026{{\\textrm{(шаг назад)}}}\\\\ {{1}}\u0026{{\\mathrm{если}\\,d_{t x}=1}}\u0026{{\\textrm{(шаг в соседа)}}}\\\\ {{\\frac{1}{q}}}\u0026{{\\mathrm{если}\\,d_{t x}=2}}\u0026{{\\textrm{(шаг вдаль от начальной точки)}}}\\end{array}\\right.$$\n\nгде $d_{tx}$ — кратчайшее расстояние между предыдущим узлом $t$ и узлом $x$.\n\nУ нас появилось две «ручки», которые можно крутить:\n\n1. $p$ — параметр возврата: \n   - $p\u003e1$ : мы избегаем возврата назад (поскольку $\\alpha_{pq}$ будет меньше 1)\n   - $p\u003c1$ : мы поощряем возврат назад (поскольку $\\alpha_{pq}$ будет больше 1)\n1. $q$ — параметр исследования:\n   - $q\u003e1$ : алгоритм напоминает BFS и как бы фокусируется на локальных связях\n   - $q\u003c1 :$ алгоритм напоминает DFS и стремится уйти как можно дальше\n\nПосле построения «предложений» модифицированной версией DeepWalk [обучают](https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami#:~:text=предложили две стратегии%3A-,Skip-gram,-и CBOW (Сontinuous) Word2Vec на графе.\n\n## Graph2Vec\n\nКак быть, если мы хотим шагнуть вперед и отойти от вершин к целым графам? Возьмем алгоритм Graph2Vec. Если Node2Vec учит векторы для узлов, то Graph2Vec делает это для целых графов.\n\nИдея Graph2Vec заимствована из модели doc2vec в NLP: каждый граф рассматривается как «документ», а его локальные подструктуры — как «слова». Цель — обучить для всего графа единый вектор, который будет хорошо предсказывать, какие «слова» (подструктуры) в нём содержатся.\n\nКак мы будем это делать:\n\n1. Запускаем для каждого графа алгоритм Weisfeiler-Lehman: на каждом шаге вершина переобозначается функцией от своей метки и мультимножества меток соседей, 2–5 итераций дают устойчивые «подписи» локальных окрестностей.\n2. Обучаем модель Skip-Gram, максимизируя вероятность:\n\n$$ P(s\\mid\\Phi(G))=\\frac{\\exp(\\bar{\\Phi}(s)^{\\textsf{T}}\\cdot\\bar{\\Phi}(G))}{\\sum_{s^{\\prime}\\in S}\\exp(\\bar{\\Phi}(s^{\\prime})^{\\mathrm{T}}\\cdot\\bar{\\Phi}(G))} $$\n\nгде:\n\n- $\\Phi(G)\\in\\mathbb{R}^{d}$ — эмбеддинг графа;\n- $\\Phi(s)\\in\\mathbb{R}^{d}$ — эмбеддинг подструктуры;\n- $S$ — множество всех возможных подструктур.\n\n{% block padding=s border=dashed %}\n\nНа практике эту сумму приближают методом отрицательных примеров (negative sampling): вместо суммирования по всем $s'\\in S$ берут небольшое случайное подмножество «негативных подструктур».\n\n{% endblock %}\n\nВ чём преимущество? Теперь мы можем работать с целыми графами — у нас на руках компактный вектор $\\Phi(G)$, с которым можно решать привычные задачи: классифицировать целые графы (например, токсичность молекул), искать похожие графы по косинусной близости, визуализировать коллекции графов. Тот же принцип работает и на уровне вершин (Node2Vec).\n\nЛогичный следующий шаг — группировать эти векторы. Далее мы разберём спектральную кластеризацию (`Ng-Jordan-Weiss`) и покажем, как она выявляет сообщества, опираясь на графовую структуру. Практически полезно L2-нормализовать эмбеддинги и подобрать разумную размерность $d$ (часто 32–256), чтобы сбалансировать качество и устойчивость. А для быстрого поиска похожих графов в больших коллекциях удобно использовать приближённые индексы ближайших соседей (например, HNSW/FAISS).\n\n## Кластеризация и важность узлов в графах\n\nВ этой части узнаем, что даже без использования графовых нейросетей (GNN) можно извлекать информативные признаки из структуры графа и успешно применять их в табличных ML-моделях.\n\nОдин из ярких примеров таких методов — `Ng-Jordan-Weiss (NJW)`. Это популярный алгоритм спектральной кластеризации, который особенно хорошо работает с данными в виде графов.\n\nОсновная идея NJW заключается в замене задачи кластеризации исходных данных, где могут быть сложные границы между кластерами, на задачу кластеризации в спектральном пространстве, где эти границы становятся проще. Давайте рассмотрим, как работает этот алгоритм:\n\n1. Считаем нормализованный симметричный лапласиан в виде:\n\n$$ L=D^{-1/2}A D^{-1/2} $$\n\n2. Решаем задачу поиска собственных векторов:\n\n$$ Lu_i = \\lambda_i u_i $$\n\n3. Берем только $k$ из них, соответствующие самым большим собственным значениям.\n4. Собираем векторы в матрицу:\n\n$$ U=[u_{1},u_{2},\\cdot\\cdot\\cdot,u_{k}], U \\in \\mathbb{R}^{n \\times k} $$\n\n5. Нормируем строки:\n\n$$ T_{i j}=\\frac{U_{i j}}{\\sqrt{\\sum_{i=1}^{k}U_{i l}^{2}}}, \\quad T\\in\\mathbb{R}^{n\\times k} $$\n\nРезультатом этой нормировки становится то, что каждый объект теперь представлен как точка на $(k-1)$-мерной единичной сфере.\n\n6. К строкам $T$ применяем алгоритм $Kmeans$.\n\nКлассический пример, когда спектральная кластеризация работает гораздо лучше обычного $KMeans$, — это [датасет с лунами](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) :\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_3_3_d2ff16a48c.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"спектральная кластеризация\"\n\u003c/figure\u003e\n\nДанный алгоритм достаточно тесно связан с задачей минимального нормализованного разреза (англ. Normalized Cut), в которой мы хотим разбить граф на части так, чтобы между кластерами связей было минимум, но при этом полученные части были большими в смысле суммы степеней. Для двух частей это можно записать вот так:\n\n$$\\operatorname{Ncut}(A,B)={\\frac{\\operatorname{cut}(A,B)}{\\operatorname{vol}(A)}}+{\\frac{\\operatorname{cut}(A,B)}{\\operatorname{vol}(B)}}$$\n\nРазобранный нами алгоритм буквально выдаёт приближенное решение этой задачи.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_3_4_3bdde6b676.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"спектральная кластеризация\"\n\u003c/figure\u003e\n\nСпектральная кластеризация, запущенная на, например, модифицированном графе социальной сети (вершины — люди, а рёбра обладают весом, пропорциональным частоте общения), выделит сообщества друзей/коллег/одноклассников.\n\nКластеризация — не единственный способ снять структуру графа. Даже когда мы не хотим разбивать вершины на группы, нам часто нужны осмысленные признаки узлов и рёбер, чтобы подать их в табличные ML-модели. Ниже соберём такой набор.\n\n## Признаки из графа для DS-задач\n\nВыше мы разобрались с устройством графов и с некоторыми особенностями работы на них при помощи лапласиана и ядер. Сейчас соберём и чуть расширим набор признаков, которые мы можем извлечь из графа и подать в классические табличные ML-модели (логистическая регрессия, деревья/бустинг — например, CatBoost, XGBoost).\n\nТакой подход нужен потому, что эти модели принимают на вход привычные векторные признаки, а не сырые графовые структуры.\n\n**Степень** (англ. degree). Количество рёбер вершины. В случае ориентированного графа делят на входящую и исходящую степени.\n\n**Коэффициент кластеризации** (англ. clustering coefficient). Показывает, насколько плотно связаны между собой соседи вершины. Записывается для вершины $v$ c $k$ соседями так:\n\n$$ C(v)=\\frac{\\textrm{кол-во рёбер между соседями v}}{k(k-1)} $$\n\nВ случае ненаправленного графа стоит считать количество рёбер с коэффициентом два (один раз за входящее и один раз за исходящее ребро), поскольку всего рёбер у таких соседей может быть не $k(k-1)$ как в ориентированном графе, а $\\frac{k(k-1)}{2}$.\n\n**Центральность** (англ. centrality). Оценивает важность узла в графе и бывает нескольких видов:\n\n- Центральность по степени (degree centrality): $C_{D}(v)={\\frac{\\deg(v)}{|V|-1}}$ , но иногда нормировку $|V|-1$ опускают.\n- Количество кратчайших путей, которые проходит узел (англ. betweenness): $C_{B}(v)=\\sum_{s\\neq v\\neq t}{\\frac{\\sigma_{s t}(v)}{\\sigma_{s t}}}$, тут $\\sigma_{s t}$ — число кратчайших путей из $s$ в $t$, а $\\sigma_{s t}(v)$ — число таких путей через $v$.\n- Средняя близость к остальным (англ. closeness): $C_{C}(v)=\\frac{|V|-1}{\\sum_{u\\in V,u\\neq v}d(u,v)}$\n\nСпектральные признаки. Уже полюбившиеся нам собственные вектора $L_{\\mathrm{sym}}$ (или их функции) как компактное спектральное описание локальной/глобальной структуры. Обычно берут несколько первых в качестве эмбеддингов.\n\n**Эмбеддинги.** Полученные методами из прошлых частей нашего рассказа.\n\nДальше для каждой вершины можно собрать, например, такую конструкцию:\n\n$$ [\\mathrm{deg}(v),{\\cal C}(v),{\\cal C}{D}(v),{\\cal C}{B}(v),{\\cal C}_{C}(v),{\\bf T o p\\,k} -\\mathrm{Eigenvectors}]. $$\n\nПроделав действия выше для каждой вершины, мы получим таблицу. И уже её можно передать вашему любимому ML-алгоритму для таблиц.\n\nПриведём несколько задач, в которых такие признаки будут полезны:\n\n- Антифрод. Поиск мошеннических транзакций или мошеннических аккаунтов: подозрительные операции и пользователи часто имеют аномальные степени и низкий коэффициент кластеризации.\n- Соцсети. Поиск лидеров групп или сообществ с помощью центральности.\n- Кредитный скоринг в развивающихся странах. Работает по принципу «скажи, кто твой друг, а я скажу, кто ты»: если у человека много связей с вершинами-банкротами, то вполне вероятно, что такой человек более склонен к банкротству.\n\n## Navigable Small World и приближённый поиск\n\nПознакомимся теперь с алгоритмом Navigable Small World (NSW) и его иерархической модификацией (HNSW).\n\nNSW / HNSW — это структуры графа, в которых точки (векторы) соединяются рёбрами с несколькими ближайшими «друзьями», так что «жадный» поиск по графу (следуя к ближайшему соседу) быстро приводит к узлам, близким к запросу.\n\nПро NSW можно думать так: «каждый человек хранит контакты $M$ ближайших знакомых». Чтобы найти человека с нужными интересами, ты начинаешь с известного «центра» и шаг за шагом переходишь к знакомым, которые кажутся ближе к цели, и очень быстро подходишь к «кластеру» нужного типа.\n\nЗачем это надо? Если у вас достаточно большой граф, то честный поиск затянется и будет работать очень долго. Но мы с вами знаем, что предоставить быстрый примерный ответ лучше, чем отправить пользователя ждать. Можно ли как-то быстро получить быстрый примерный ответ и потенциально доработать его?\n\nГраф строится итеративно: каждая новая вершина добавляется в уже построенный граф и соединяется с $M$ вершинами, ближайшими к ней по локальному поиску, а не по всему множеству.\n\nАлгоритм вставки вершины $x$:\n\n1. Выполняется локальный эвристический поиск ближайших к $x$ вершин.\n1. Выбираются $M$ ближайших среди найденных.\n1. Добавляются рёбра между $x$ и этими вершинами.\n\nЭто создаёт граф, обладающий свойствами малого мира с короткими путями между произвольными вершинами и высокой кластеризацией.\n\nАлгоритм поиска:\n\n1. Начинаем с произвольной вершины $v_0$.\n1. Если находим соседа $u$ — такого, что: $dist(q,u)\u003cdist(q,v)$, переходим в $u$.\n1. Иначе возвращаем $v$ как результат.\n\nЭто локальный жадный поиск (англ. greedy search), останавливающийся в локальном минимуме.\n\nВ чем же идея иерархический модификации?\n\nДобавляется иерархия уровней — граф становится многоуровневым. Верхние уровни разрежены, нижние — плотные. Навигация начинается с самого верхнего иерархического уровня и постепенно переходит вниз, улучшая приближение.\n\nПредставьте, будто у вас есть несколько вариантов транспорта: нижний уровень будет идти пешком, повыше — автобусом или трамваем, уровень ещё выше — поездом, а самый высокий — самолётом. Пешком вы можете достичь любой точки, но медленно, а самолет позволяет быстро перемещаться, но только между аэропортами.\n\nАлгоритм вставки точки такой:\n\n1. Случайно назначаем точке $x$ максимальный уровень $L'(x)$ (геометрическое распределение).\n1. Стартуем с текущего верхнего уровня $L$ и жадно спускаемся до $L'(x)$, каждый раз переходя к ближайшим к $x$.\n1. На каждом уровне $\\ell \\le L'(x)$ выполняем локальный поиск кандидатов и соединяем $x$ с $M$ «разнообразными» ближайшими узлами (используется эвристика отбора соседей).\n1. Обновляем верхний уровень $L$ при необходимости.\n\nАлгоритм поиска такой:\n\n1. Пусть запрос $q$. Начинаем с вершины на верхнем уровне $G_L$ и выполняем жадный поиск по уровню.\n1. Переходим на уровень ниже, стартуя с лучшего найденного узла.\n1. Повторяем до нулевого уровня.\n1. На уровне 0 выполняется расширенный поиск с буфером из нескольких вершин (priority queue), чтобы избежать локального минимума.\n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"https://yastatic.net/s3/education-portal/media/8_3_5_f519e3474a.webp\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    alt=\"спектральная кластеризация\"\n\u003c/figure\u003e\n\n***\n\nВ этом параграфе мы изучили, как рассматривать граф как сигнальное пространство: определять его спектр и частоты, моделировать распространение информации через диффузию и случайные блуждания, выделять значимые паттерны с помощью фильтров, а также представлять вершины и целые графы в виде эмбеддингов. Мы познакомились с ядрами и методами кластеризации, которые позволяют выявлять скрытые структуры и сходства, даже не прибегая к нейросетям.\n\nЭти подходы образуют математический фундамент графовых нейронных сетей (GNN). Спектральный анализ задаёт язык сверток на графах, диффузионные процессы описывают передачу сигналов между вершинами, а эмбеддинги обеспечивают компактное кодирование структуры.\n\nА сейчас — пройдите квиз, чтобы закрепить знания, и переходите к заключительному параграфу главы: там мы коротко подведём итоги.\n"])</script><script nonce="">self.__next_f.push([1,"82:Tef1,"])</script><script nonce="">self.__next_f.push([1,"В этой главе вы перешли от интуитивных представлений о графах к полноценному инструментарию их анализа и моделирования. Давайте подведём итог.\n\nВы научились:\n\n- Смотреть на граф через спектр. Работать с лапласианом, его собственными значениями/векторами и графовым Фурье-преобразованием. Читать связность и сообщества по спектру.\n- Моделировать динамику. Описывать диффузию и случайные блуждания, понимать стационарные распределения. Выводить PageRank и Label Propagation как частные случаи одного диффузионного механизма.\n- Фильтровать сигналы на графах. Применять полиномиальные и чебышёвские фильтры, видеть связь со слоями GCN и «ренормализацией».\n- Сравнивать графы ядрами. Использовать случайные блуждания, кратчайший путь и Weisfeiler-Lehman для измерения структурного сходства без явного перехода к признаковому пространству.\n- Строить эмбеддинги. Получать векторные представления вершин и целых графов (DeepWalk, Node2Vec, Graph2Vec) и решать на них прикладные задачи вроде классификации.\n- Применять знания на практике. Делать спектральную кластеризацию, собирать структурные признаки для табличных моделей и организовывать быстрый поиск по эмбеддингам.\n\nВы увидели, как абстрактные идеи — спектр, диффузия, фильтры — превращаются в рабочие алгоритмы для реальных задач: от ранжирования страниц и выявления сообществ до анализа молекул и антифрода.\n\nНа этом наше путешествие по фундаментальной математике для анализа данных завершается. Вы прошли путь от основ линейной алгебры и матанализа до тонкостей теории вероятностей и продвинутых методов работы с графами.\n\nТеперь у вас есть прочный фундамент и необходимый инструментарий, чтобы уверенно погружаться в мир современного машинного и глубинного обучения. Вы готовы не просто применять готовые модели, но и понимать их внутреннюю работу, адаптировать под свои задачи и даже создавать новые подходы.\n\nА если вам интересно окунуться ещё глубже — советуем обратить внимание на [хендбук](https://education.yandex.ru/handbook/ml) по машинному обучению от ШАД, в котором разбираются исторически важные и современные алгоритмы и их реализация.\n\nУдачи в ваших будущих проектах!"])</script><script nonce="">self.__next_f.push([1,"1c:[\"$\",\"$L56\",null,{\"bookSlug\":\"math\",\"articleSlug\":\"math-glava-chetire-chemu-vi-nauchilis\",\"children\":[\"$\",\"$L57\",null,{\"articleName\":\"4.14 Чему вы научились\",\"children\":[\"$\",\"$L58\",null,{\"chapters\":[{\"id\":196,\"Name\":\"1. Введение\",\"Articles\":[{\"Name\":\"math | 1.1. Зачем вам этот учебник и как его читать\",\"Slug\":\"math-zachem-vam-etot-uchebnik-i-kak-yego-chitat\",\"createdAt\":\"2024-11-25T12:43:05.088Z\",\"updatedAt\":\"2025-10-28T09:55:40.313Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$59\",\"Title\":\"Зачем вам этот учебник и как его читать\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"1.1\"},{\"Name\":\"math | 1.2. Как пользоваться хендбуком\",\"Slug\":\"math-kak-polzovatsia-khendbukom\",\"createdAt\":\"2024-11-29T11:26:08.645Z\",\"updatedAt\":\"2025-10-01T12:18:42.142Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"Если вы хотите вернуться к параграфу попозже, то можете оставить закладку на том месте, где вы остановились.\\n\\nДля этого можно отметить главы как прочитанные. Плюс из этого же меню можно перейти к задачам хендбука:\\n\\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_1_1_1b00eb0564.webp)\\n\\nИзменение статуса главы отразится на странице содержания и в прогресс-баре хендбука:\\n\\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_2_3_a94219c466.webp)\\n\\nДля перемещения по хендбуку вы можете воспользоваться навигацией внутри страниц:\\n\\n![Python](https://yastatic.net/s3/education-portal/media/Python_hb_screenshot_4_e01772afef.webp)\\n\\nВот и всё! Как видите, это не сложно. Надеемся, у вас всё получится. А если нет — вступайте в [коммьюнити](https://t.me/+bikU3_M1x0s0YzMy)  хендбука: там вам обязательно придут на помощь.\",\"Title\":\"Как пользоваться хендбуком\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"1.2\"},{\"Name\":\"math | 1.3 Как работать с системой проверки заданий\",\"Slug\":\"math-kak-rabotat-s-sistemoi-proverki-zadanii\",\"createdAt\":\"2024-11-29T11:25:19.163Z\",\"updatedAt\":\"2025-10-01T12:17:50.019Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$5a\",\"Title\":\"Как работать с системой проверки заданий\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"1.3\"}]},{\"id\":195,\"Name\":\"2. Введение в теорию графов\",\"Articles\":[{\"Name\":\"math | 2.1. О чём мы поговорим в этой главе\",\"Slug\":\"math-glava-dva-o-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2024-11-25T12:43:46.827Z\",\"updatedAt\":\"2024-12-02T08:09:05.465Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$5b\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"2.1\"},{\"Name\":\"math | 2.2. Основы теории графов\",\"Slug\":\"osnovi-teorii-grafov\",\"createdAt\":\"2024-11-25T12:44:31.005Z\",\"updatedAt\":\"2025-09-15T13:35:16.359Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/71932/problem\",\"Lead\":null,\"Content\":\"$5c\",\"Title\":\"Основы теории графов\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"2.2\"},{\"Name\":\"math | 2.3. Типы графов\",\"Slug\":\"tipi-grafov\",\"createdAt\":\"2024-11-25T12:44:53.006Z\",\"updatedAt\":\"2025-08-27T12:14:27.882Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/71962/problem\",\"Lead\":null,\"Content\":\"$5d\",\"Title\":\"Типы графов\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"2.3\"},{\"Name\":\"math | 2.4. Деревья в теории графов\",\"Slug\":\"derevia-v-teorii-grafov\",\"createdAt\":\"2024-11-25T12:45:18.465Z\",\"updatedAt\":\"2025-11-17T17:11:52.283Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/71965/problem\",\"Lead\":null,\"Content\":\"$5e\",\"Title\":\"Деревья в теории графов\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"2.4\"},{\"Name\":\"math | 2.5. Чему мы научились\",\"Slug\":\"math-glava-dva-chemu-mi-nauchilis\",\"createdAt\":\"2024-11-25T12:46:03.269Z\",\"updatedAt\":\"2024-12-17T08:39:28.419Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$5f\",\"Title\":\"Чему мы научились\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"2.5\"}]},{\"id\":197,\"Name\":\"3. Основы математического анализа\",\"Articles\":[{\"Name\":\"math | 3.1 О чём мы поговорим в этой главе\",\"Slug\":\"math-glava-tri-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2024-11-28T15:21:39.462Z\",\"updatedAt\":\"2024-12-17T08:39:53.747Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$60\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"3.1\"},{\"Name\":\"math | 3.2. Пределы и непрерывность функций\",\"Slug\":\"predeli-i-neprerivnost-funktsii\",\"createdAt\":\"2024-11-28T15:22:38.789Z\",\"updatedAt\":\"2025-08-25T11:32:53.587Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/72000/problem\",\"Lead\":null,\"Content\":\"$61\",\"Title\":\"Пределы и непрерывность функций\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"3.2\"},{\"Name\":\"math | 3.3. Дифференцирование функций одной переменной\",\"Slug\":\"differentsirovanie-funktsii-odnoi-peremennoi\",\"createdAt\":\"2024-11-28T15:23:04.935Z\",\"updatedAt\":\"2025-03-19T12:38:43.705Z\",\"publishedAt\":\"2024-12-02T08:05:38.763Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/72001/problem\",\"Lead\":null,\"Content\":\"$62\",\"Title\":\"Дифференцирование функций одной переменной\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"3.3\"}]},{\"id\":233,\"Name\":\"4. Линейная алгебра\",\"Articles\":[{\"Name\":\"math | 4.1 О чём мы поговорим в этой главе\",\"Slug\":\"chetupe-o-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-11-19T11:43:19.462Z\",\"publishedAt\":\"2025-03-19T14:25:42.052Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$63\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.1\"},{\"Name\":\"math | 4.2 Векторы\",\"Slug\":\"vektori\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-12-29T13:39:27.648Z\",\"publishedAt\":\"2025-03-19T10:25:42.152Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/76236/problem\",\"Lead\":null,\"Content\":\"$64\",\"Title\":\"Векторы\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.2\"},{\"Name\":\"math | 4.3. Матрицы\",\"Slug\":\"matritsi\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-08-21T10:37:28.188Z\",\"publishedAt\":\"2025-03-19T10:25:35.664Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/76237/problem\",\"Lead\":null,\"Content\":\"$65\",\"Title\":\"Матрицы\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.3\"},{\"Name\":\"math | 4.4. Системы линейных уравнений основы\",\"Slug\":\"istemi-lineinikh-uravnenii-osnovi\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-11-19T09:22:04.703Z\",\"publishedAt\":\"2025-03-19T10:25:30.834Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contest/81812/problem\",\"Lead\":null,\"Content\":\"$66\",\"Title\":\"Системы линейных уравнений основы\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.4\"},{\"Name\":\"math | 4.5 Системы линейных уравнений: продвинутые методы решения\",\"Slug\":\"sistemi-lineinikh-uravnenii-prodvinutie-metodi-resheniia\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-11-19T09:22:26.704Z\",\"publishedAt\":\"2025-08-19T18:18:10.787Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contest/81821/problem\",\"Lead\":null,\"Content\":\"$67\",\"Title\":\"Системы линейных уравнений: продвинутые методы решения\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.5\"},{\"Name\":\"math | 4.6 Определитель\",\"Slug\":\"opredelitel\",\"createdAt\":\"2025-03-18T10:01:25.136Z\",\"updatedAt\":\"2025-12-12T12:13:48.091Z\",\"publishedAt\":\"2025-08-20T13:43:16.018Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contest/81824/problem\",\"Lead\":null,\"Content\":\"$68\",\"Title\":\"Определитель\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.6\"},{\"Name\":\"math | 4.7. Геометрия признакового пространства: нормы и расстояния\",\"Slug\":\"geometriia-priznakovogo-prostranstva\",\"createdAt\":\"2025-11-15T19:37:11.171Z\",\"updatedAt\":\"2025-12-25T11:11:28.773Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/85629/problem\",\"Lead\":null,\"Content\":\"$69\",\"Title\":\"Геометрия признакового пространства: нормы и расстояния\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.7\"},{\"Name\":\"math | 4.8. Проекции, углы и ортогональность\",\"Slug\":\"proektsii-ugli-i-ortogonalnost\",\"createdAt\":\"2025-11-15T23:50:45.250Z\",\"updatedAt\":\"2025-12-29T13:42:04.601Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/85630/problem\",\"Lead\":null,\"Content\":\"$6a\",\"Title\":\"Проекции, углы и ортогональность\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.8\"},{\"Name\":\"math | 4.9. Спектральные методы и матричные разложения\",\"Slug\":\"spektralnie-metodi-i-matrichnie-razlozheniia\",\"createdAt\":\"2025-11-17T09:15:08.573Z\",\"updatedAt\":\"2025-12-25T10:17:21.476Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/85631/problem\",\"Lead\":null,\"Content\":\"$6b\",\"Title\":\"Спектральные методы и матричные разложения\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.9\"},{\"Name\":\"math | 4.10. Снижение размерности и латентные факторы\",\"Slug\":\"snizhenie-razmernosti-i-latentnie-faktori\",\"createdAt\":\"2025-11-17T13:47:33.299Z\",\"updatedAt\":\"2025-12-25T10:22:01.732Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/85632/problem\",\"Lead\":null,\"Content\":\"$6c\",\"Title\":\"Снижение размерности и латентные факторы\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.10\"},{\"Name\":\"math | 4.11. Линейные модели и регуляризация\",\"Slug\":\"lineinie-modeli-i-reguliarizatsiia\",\"createdAt\":\"2025-11-17T14:42:16.606Z\",\"updatedAt\":\"2025-12-25T10:27:56.290Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/85633/problem\",\"Lead\":null,\"Content\":\"$6d\",\"Title\":\"Линейные модели и регуляризация\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.11\"},{\"Name\":\"math | 4.12. SVM и ядровой трюк\",\"Slug\":\"svm-i-yadrovoi-triuk\",\"createdAt\":\"2025-11-17T15:27:28.243Z\",\"updatedAt\":\"2025-12-27T16:12:51.703Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/87782/start\",\"Lead\":null,\"Content\":\"$6e\",\"Title\":\"SVM и ядровой трюк\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.12\"},{\"Name\":\"math | 4.13. Препроцессинг признаков\",\"Slug\":\"preprotsessing-priznakov\",\"createdAt\":\"2025-11-17T16:02:21.473Z\",\"updatedAt\":\"2025-12-27T16:13:22.173Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/87783/start\",\"Lead\":null,\"Content\":\"$6f\",\"Title\":\"Препроцессинг признаков\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.13\"},{\"Name\":\"math | 4.14. Чему вы научились\",\"Slug\":\"math-glava-chetire-chemu-vi-nauchilis\",\"createdAt\":\"2025-11-17T16:44:34.022Z\",\"updatedAt\":\"2025-12-01T15:27:39.561Z\",\"publishedAt\":\"2025-11-19T12:00:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$70\",\"Title\":\"Чему вы научились\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"4.14\"}]},{\"id\":341,\"Name\":\"5. Комбинаторика\",\"Articles\":[{\"Name\":\"math | 5.1. О чём мы поговорим в этой главе\",\"Slug\":\"math-glava-pyat-o-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2025-11-26T11:59:14.938Z\",\"updatedAt\":\"2025-12-01T15:06:34.969Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$71\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.1\"},{\"Name\":\"math | 5.2. Основные понятия и элементарные операции в комбинаторике\",\"Slug\":\"osnovnie-poniatiia-i-elementarnie-operatsii-v-kombinatorike\",\"createdAt\":\"2025-11-26T12:00:37.741Z\",\"updatedAt\":\"2025-11-28T10:40:00.190Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$72\",\"Title\":\"Основные понятия и элементарные операции в комбинаторике\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.2\"},{\"Name\":\"math | 5.3. Правила суммы и произведения, сочетания, размещения и другие операции в комбинаторике\",\"Slug\":\"pravila-summi-i-proizvedeniia-sochetaniia-razmeshcheniia-i-drugie-operatsii-v-kombinatorike\",\"createdAt\":\"2025-11-27T10:55:22.316Z\",\"updatedAt\":\"2025-12-01T15:24:38.531Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$73\",\"Title\":\"Правила суммы и произведения, сочетания, размещения и другие операции в комбинаторике\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.3\"},{\"Name\":\"math | 5.4. Метод включений-исключений и структура подмножеств\",\"Slug\":\"metod-vkliuchenii-iskliuchenii-i-struktura-podmnozhestv\",\"createdAt\":\"2025-11-27T14:09:04.427Z\",\"updatedAt\":\"2025-12-29T08:37:40.308Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$74\",\"Title\":\"Метод включений-исключений и структура подмножеств\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.4\"},{\"Name\":\"math | 5.5. Комбинаторика в машинном обучении: от перебора гиперпараметров до проклятия размерности\",\"Slug\":\"kombinatorika-v-mashinnom-obuchenii-ot-perebora-giperparametrov-do-prokliatiia-razmernosti\",\"createdAt\":\"2025-11-27T14:42:24.755Z\",\"updatedAt\":\"2025-11-28T10:57:16.888Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$75\",\"Title\":\"Комбинаторика в машинном обучении: от перебора гиперпараметров до проклятия размерности\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.5\"},{\"Name\":\"math | 5.6. Чему вы научились\",\"Slug\":\"math-glava-pyat-chemu-vi-nauchilis\",\"createdAt\":\"2025-11-27T16:36:27.620Z\",\"updatedAt\":\"2025-12-01T15:28:05.129Z\",\"publishedAt\":\"2025-11-28T10:40:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$76\",\"Title\":\"Чему вы научились\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"5.6\"}]},{\"id\":234,\"Name\":\"6. Теория вероятностей\",\"Articles\":[{\"Name\":\"math | 6.1. О чём мы поговорим в этой главе\",\"Slug\":\"math-glava-shest-o-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2025-03-18T09:35:41.418Z\",\"updatedAt\":\"2025-12-01T15:07:46.006Z\",\"publishedAt\":\"2025-03-19T10:25:25.071Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$77\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.1\"},{\"Name\":\"math | 6.2. Вероятностное пространство\",\"Slug\":\"veroiatnostnoe-prostranstvo\",\"createdAt\":\"2025-03-18T09:35:41.418Z\",\"updatedAt\":\"2025-12-01T14:31:42.957Z\",\"publishedAt\":\"2025-03-19T10:25:15.741Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/76238/problem\",\"Lead\":null,\"Content\":\"$78\",\"Title\":\"Вероятностное пространство\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.2\"},{\"Name\":\"math | 6.3. Условная вероятность и независимость событий\",\"Slug\":\"uslovnaia-veroiatnost-i-nezavisimost-sobitii\",\"createdAt\":\"2025-03-18T09:35:41.418Z\",\"updatedAt\":\"2025-12-01T14:31:59.275Z\",\"publishedAt\":\"2025-03-19T10:25:13.301Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/76239/problem\",\"Lead\":null,\"Content\":\"$79\",\"Title\":\"Условная вероятность и независимость событий\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.3\"},{\"Name\":\"math | 6.4. Дискретные случайные величины\",\"Slug\":\"diskretnie-sluchainie-velichini\",\"createdAt\":\"2025-03-18T09:35:41.418Z\",\"updatedAt\":\"2025-12-01T14:32:17.742Z\",\"publishedAt\":\"2025-03-19T10:25:00.487Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contest/81827/problem\",\"Lead\":null,\"Content\":\"$7a\",\"Title\":\"Дискретные случайные величины\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.4\"},{\"Name\":\"math | 6.5. Непрерывные случайные величины\",\"Slug\":\"neprerivnie-sluchainie-velichini\",\"createdAt\":\"2025-03-18T09:35:41.418Z\",\"updatedAt\":\"2025-12-16T19:46:38.154Z\",\"publishedAt\":\"2025-08-18T13:07:52.088Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contest/81831/problem\",\"Lead\":null,\"Content\":\"$7b\",\"Title\":\"Непрерывные случайные величины\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.5\"},{\"Name\":\"math | 6.6. Совместное распределение. Ковариация и коэффициент корреляции\",\"Slug\":\"sovmestnoe-raspredelenie-kovariatsiia-i-koeffitsient-korreliatsii\",\"createdAt\":\"2025-10-27T23:35:00.575Z\",\"updatedAt\":\"2025-12-01T14:32:43.999Z\",\"publishedAt\":\"2025-10-30T10:58:26.116Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/84293/problem\",\"Lead\":null,\"Content\":\"$7c\",\"Title\":\"Совместное распределение. Ковариация и коэффициент корреляции\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.6\"},{\"Name\":\"math | 6.7. Энтропия. Перплексия. KL-дивергенция\",\"Slug\":\"entropiia-perpleksiia-kl-divergentsiia\",\"createdAt\":\"2025-10-28T19:42:00.027Z\",\"updatedAt\":\"2025-12-01T14:32:56.254Z\",\"publishedAt\":\"2025-10-30T10:58:33.974Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/84318/problem\",\"Lead\":null,\"Content\":\"$7d\",\"Title\":\"Энтропия. Перплексия. KL-дивергенция\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.7\"},{\"Name\":\"math | 6.8. Чему вы научились\",\"Slug\":\"math-glava-shest-chemu-vi-nauchilis\",\"createdAt\":\"2025-10-29T13:51:12.030Z\",\"updatedAt\":\"2025-12-01T15:28:27.578Z\",\"publishedAt\":\"2025-10-30T10:58:41.103Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$7e\",\"Title\":\"Чему вы научились\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"6.8\"}]},{\"id\":342,\"Name\":\"7. Продвинутый анализ графов\",\"Articles\":[{\"Name\":\"math | 8.1. О чём мы поговорим в этой главе\",\"Slug\":\"math-glava-vosem-o-chyom-mi-pogovorim-v-etoi-glave\",\"createdAt\":\"2025-11-28T15:58:53.805Z\",\"updatedAt\":\"2025-12-01T15:08:10.884Z\",\"publishedAt\":\"2025-12-01T15:00:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$7f\",\"Title\":\"О чём мы поговорим в этой главе\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"7.1\"},{\"Name\":\"math | 8.2. Спектральные методы и диффузия на графах\",\"Slug\":\"spektralnie-metodi-i-diffuziia-na-grafakh\",\"createdAt\":\"2025-11-28T16:02:36.476Z\",\"updatedAt\":\"2025-12-27T16:06:10.852Z\",\"publishedAt\":\"2025-12-01T15:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/87800/start\",\"Lead\":null,\"Content\":\"$80\",\"Title\":\"Спектральные методы и\u0026nbsp;диффузия на\u0026nbsp;графах\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"7.2\"},{\"Name\":\"math | 8.3. Ядра и эмбеддинги графов\",\"Slug\":\"yadra-i-embeddingi-grafov\",\"createdAt\":\"2025-12-01T12:46:19.878Z\",\"updatedAt\":\"2025-12-27T16:07:28.405Z\",\"publishedAt\":\"2025-12-01T15:00:00.000Z\",\"ContestURL\":\"https://new.contest.yandex.ru/contests/87801/start\",\"Lead\":null,\"Content\":\"$81\",\"Title\":\"Ядра и эмбеддинги графов\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"7.3\"},{\"Name\":\"math | 8.4. Чему вы научились\",\"Slug\":\"math-glava-vosem-chemu-vi-nauchilis\",\"createdAt\":\"2025-12-01T13:54:00.174Z\",\"updatedAt\":\"2025-12-01T15:28:52.891Z\",\"publishedAt\":\"2025-12-01T15:00:00.000Z\",\"ContestURL\":null,\"Lead\":null,\"Content\":\"$82\",\"Title\":\"Чему вы научились\",\"LikesCount\":0,\"Quiz\":null,\"StoreCardId\":null,\"numberInCourse\":\"7.4\"}]}],\"activeArticleSlug\":\"math-glava-chetire-chemu-vi-nauchilis\",\"csrfToken\":\"\",\"hasNotes\":true,\"children\":\"$L83\"}]}]}]\n"])</script><script nonce="">self.__next_f.push([1,"83:[[[\"$\",\"meta\",null,{\"property\":\"og:type\",\"content\":\"article\",\"itemProp\":\"\"}],[\"$\",\"meta\",null,{\"property\":\"article:modified_time\",\"content\":\"2025-12-01T15:27:39.561Z\",\"itemProp\":\"\"}],[],[\"$\",\"meta\",null,{\"property\":\"article:section\",\"content\":\"Математика для анализа данных\",\"itemProp\":\"\"}]],\"$L84\"]\n"])</script><script nonce="">self.__next_f.push([1,"85:I[9348,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"ReadingProgress\"]\n86:I[16524,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"BookBreadcrumbs\"]\n87:I[87301,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"WYSIWYGClient\"]\n88:T132f,"])</script><script nonce="">self.__next_f.push([1,"В этой главе мы собрали воедино линейно-алгебраические инструменты, которые чаще всего встречаются в анализе данных и машинном обучении, и показали их на практических задачах.\n\n- Освежили базу: векторы и матрицы, операции (сложение, умножение, транспонирование), свойства следа и их роль в регуляризации и упрощении выражений.\n- Научились решать системы линейных уравнений: метод Гаусса, LU-разложение (и узнали случаи, когда им пользоваться); обсудили смысл определителя.\n- Разобрали ортогонализацию и QR-разложение (включая Грама — Шмидта) и увидели, как через ортогональные проекции формулируется и решается МНК-регрессия.\n- Ввели нормы и расстояния, обсудили обусловленность и устойчивость вычислений и разобрали, почему масштаб признаков влияет на сходимость алгоритмов.\n- Перешли к спектральным методам: разобрали собственные значения/векторы и диагонализацию; сингулярное разложение (SVD) как основу низкоранговых приближений, сжатия и поиска структуры.\n- Показали, как из SVD получается метод главных компонент (PCA) для снижения размерности. На практике рассмотрели, как по спектру выбирать число компонент и что означает объяснённая дисперсия.\n- Рассмотрели разложения для текстов и скрытых тем: LSA и NMF, их интерпретируемость и ограничения.\n- Разобрали матричную факторизацию в рекомендательных системах: как разреженная матрица рейтингов сворачивается в общие латентные факторы пользователей и объектов\n- Рассмотрели работу SVM, вывели двойственную задачу и обсудили ядровой трюк, позволяющий работать в высокоразмерных пространствах без явного преобразования признаков.\n- Завершили предобработкой признаков: центрирование, стандартизация, робастное масштабирование и их влияние на устойчивость и качество моделей.\n\nТеперь у вас есть целостное представление о том, как методы линейной алгебры применяются в современных алгоритмах анализа данных, и вы умеете использовать эти инструменты для повышения устойчивости, интерпретируемости и эффективности моделей в практических задачах.\n\nОсвоив геометрию признакового пространства, вы научились работать с данными в непрерывных пространствах, представляя их векторами и находя в них геометрические структуры. Однако многие задачи в машинном обучении — от подбора признаков до настройки моделей — сводятся к работе с конечными наборами и выбору из огромного числа вариантов. Чтобы научиться оценивать сложность таких задач и понимать, почему полный перебор часто невозможен, понадобится аппарат комбинаторики.\n\nТак что в следующей главе мы погрузимся в мир множеств, перестановок и сочетаний, чтобы разобраться, как принципы подсчёта лежат в основе настройки моделей и помогают осознать то самое «проклятие размерности»."])</script><script nonce="">self.__next_f.push([1,"89:T1399,"])</script><script nonce="">self.__next_f.push([1,"\u003cp\u003eВ этой главе мы собрали воедино линейно-алгебраические инструменты, которые чаще всего встречаются в анализе данных и машинном обучении, и показали их на практических задачах.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eОсвежили базу: векторы и матрицы, операции (сложение, умножение, транспонирование), свойства следа и их роль в регуляризации и упрощении выражений.\u003c/li\u003e\n\u003cli\u003eНаучились решать системы линейных уравнений: метод Гаусса, LU-разложение (и узнали случаи, когда им пользоваться); обсудили смысл определителя.\u003c/li\u003e\n\u003cli\u003eРазобрали ортогонализацию и QR-разложение (включая Грама — Шмидта) и увидели, как через ортогональные проекции формулируется и решается МНК-регрессия.\u003c/li\u003e\n\u003cli\u003eВвели нормы и расстояния, обсудили обусловленность и устойчивость вычислений и разобрали, почему масштаб признаков влияет на сходимость алгоритмов.\u003c/li\u003e\n\u003cli\u003eПерешли к спектральным методам: разобрали собственные значения/векторы и диагонализацию; сингулярное разложение (SVD) как основу низкоранговых приближений, сжатия и поиска структуры.\u003c/li\u003e\n\u003cli\u003eПоказали, как из SVD получается метод главных компонент (PCA) для снижения размерности. На практике рассмотрели, как по спектру выбирать число компонент и что означает объяснённая дисперсия.\u003c/li\u003e\n\u003cli\u003eРассмотрели разложения для текстов и скрытых тем: LSA и NMF, их интерпретируемость и ограничения.\u003c/li\u003e\n\u003cli\u003eРазобрали матричную факторизацию в рекомендательных системах: как разреженная матрица рейтингов сворачивается в общие латентные факторы пользователей и объектов\u003c/li\u003e\n\u003cli\u003eРассмотрели работу SVM, вывели двойственную задачу и обсудили ядровой трюк, позволяющий работать в высокоразмерных пространствах без явного преобразования признаков.\u003c/li\u003e\n\u003cli\u003eЗавершили предобработкой признаков: центрирование, стандартизация, робастное масштабирование и их влияние на устойчивость и качество моделей.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eТеперь у вас есть целостное представление о том, как методы линейной алгебры применяются в современных алгоритмах анализа данных, и вы умеете использовать эти инструменты для повышения устойчивости, интерпретируемости и эффективности моделей в практических задачах.\u003c/p\u003e\n\u003cp\u003eОсвоив геометрию признакового пространства, вы научились работать с данными в непрерывных пространствах, представляя их векторами и находя в них геометрические структуры. Однако многие задачи в машинном обучении — от подбора признаков до настройки моделей — сводятся к работе с конечными наборами и выбору из огромного числа вариантов. Чтобы научиться оценивать сложность таких задач и понимать, почему полный перебор часто невозможен, понадобится аппарат комбинаторики.\u003c/p\u003e\n\u003cp\u003eТак что в следующей главе мы погрузимся в мир множеств, перестановок и сочетаний, чтобы разобраться, как принципы подсчёта лежат в основе настройки моделей и помогают осознать то самое «проклятие размерности».\u003c/p\u003e\n"])</script><script nonce="">self.__next_f.push([1,"84:[\"$\",\"div\",null,{\"className\":\"styles_container__SBhDc\",\"children\":[[\"$\",\"$L85\",null,{\"targetEl\":\"#article-content\",\"offset\":60}],[\"$\",\"div\",null,{\"className\":\"styles_articleCover__EKYZn\",\"children\":[[\"$\",\"$L86\",null,{\"className\":\"styles_breadcrumbs__nMPkF\",\"items\":[{\"title\":\"Хендбуки\",\"href\":\"/handbook\",\"hardNavigate\":true},{\"title\":\"Математика для анализа данных\",\"href\":\"/handbook/math\",\"showMobile\":true},{\"title\":\"Чему вы научились\"}]}],null,[\"$\",\"h1\",null,{\"ref\":\"$undefined\",\"className\":\"styles_root__EmBCZ styles_title__Ae0WW\",\"style\":{},\"data-search-hidden\":true,\"children\":\"4.14 Чему вы научились\",\"data-variant\":\"heading\",\"data-weight\":\"medium\",\"data-color\":\"primary\"}],null]}],[\"$\",\"main\",null,{\"className\":\"styles_root__R5rVX\",\"data-testid\":\"ArticleContent-root\",\"data-ai-main-material\":true,\"children\":[null,[\"$\",\"div\",null,{\"className\":\"hljs_hljs-atelier-heath__2Efzm styles_content__jb6Og\",\"id\":\"article-content\",\"children\":[\"$\",\"$L87\",null,{\"Content\":\"$88\",\"csrfToken\":\"\",\"isNotesAvailable\":true,\"vars\":{},\"html\":\"$89\"}]}]]}],\"$L8a\",\"$L8b\",\"$L8c\",null,\"$L8d\"]}]\n"])</script><script nonce="">self.__next_f.push([1,"8e:I[78166,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"ArticleProgressConnected\"]\n8f:I[87289,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"ArticleActionConnected\"]\n90:I[554,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"NavigationBlock\"]\n91:I[30885,[\"1281\",\"static/chunks/1281-08bad9a3d2a60887.js\",\"5080\",\"static/chunks/5080-82f4daed94452227.js\",\"4194\",\"static/chunks/4194-172db2b948bd13ee.js\",\"5609\",\"static/chunks/5609-185e25579372eca4.js\",\"8619\",\"static/chunks/8619-3a701f3773450188.js\",\"9275\",\"static/chunks/9275-7c8f52465eb2613c.js\",\"9515\",\"static/chunks/9515-b7b857111af67561.js\",\"3121\",\"static/chunks/app/handbook/%5Bbook%5D/article/%5Barticle%5D/page-c867d0c23edc611e.js\"],\"HanbookCommunityBanner\"]\n8a:[\"$\",\"$L8e\",null,{\"bookSlug\":\"math\",\"articleSlug\":\"math-glava-chetire-chemu-vi-nauchilis\",\"passportUrl\":\"https://passport.yandex.ru/auth/?origin=education\u0026retpath=https%3A%2F%2"])</script><script nonce="">self.__next_f.push([1,"Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis\",\"csrfToken\":\"\",\"ErrorAction\":{\"id\":4399,\"Text\":\"Сообщить об ошибке\",\"URL\":\"https://forms.yandex.ru/surveys/academy/?proekt=handbooks\",\"Title\":\"Сообщить об ошибке\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null},\"paragraphV2\":true,\"isAuthorized\":false}]\n8b:[\"$\",\"$L8f\",null,{\"contestURL\":null,\"CTAs\":[],\"ErrorAction\":\"$8a:props:ErrorAction\",\"questions\":null,\"passportUrl\":\"https://passport.yandex.ru/auth/?origin=education\u0026retpath=https%3A%2F%2Feducation.yandex.ru%2Fhandbook%2Fmath%2Farticle%2Fmath-glava-chetire-chemu-vi-nauchilis\",\"isAuthorized\":false,\"csrfToken\":\"\",\"paragraphV2\":true}]\n8c:[\"$\",\"div\",null,{\"className\":\"styles_article-navigate__SKki7\",\"data-testid\":\"ArticleNavigate-root\",\"children\":[[\"$\",\"$L90\",\"preprotsessing-priznakov\",{\"label\":\"Предыдущий параграф\",\"title\":\"4.13. Препроцессинг признаков\",\"lead\":null,\"url\":\"./preprotsessing-priznakov\",\"direction\":\"back\"}],[\"$\",\"$L90\",\"math-glava-pyat-o-chyom-mi-pogovorim-v-etoi-glave\",{\"label\":\"Следующий параграф\",\"title\":\"5.1. О чём мы поговорим в этой главе\",\"lead\":null,\"url\":\"./math-glava-pyat-o-chyom-mi-pogovorim-v-etoi-glave\",\"direction\":\"forward\"}]]}]\n"])</script><script nonce="">self.__next_f.push([1,"8d:[\"$\",\"$L91\",null,{\"banner\":{\"id\":106,\"Title\":\"Вступайте в\u0026nbsp;сообщество хендбука\",\"Description\":\"Здесь можно найти единомышленников, экспертов и\u0026nbsp;просто интересных собеседников. А\u0026nbsp;ещё\u0026nbsp;— получить помощь или поделиться знаниями.\",\"ActionType\":null,\"Icon\":{\"data\":{\"id\":16699,\"attributes\":{\"name\":\"Poluchajte\",\"alternativeText\":null,\"caption\":null,\"width\":237,\"height\":124,\"formats\":null,\"hash\":\"Poluchajte_obnovleniya_ot_obrazovaniya_30910b54df_61ea47b298\",\"ext\":\".svg\",\"mime\":\"image/svg+xml\",\"size\":33.89,\"url\":\"https://yastatic.net/s3/education-portal/media/Poluchajte_obnovleniya_ot_obrazovaniya_30910b54df_61ea47b298.svg\",\"previewUrl\":null,\"provider\":\"strapi-plugin-yandexify\",\"provider_metadata\":null,\"createdAt\":\"2025-09-02T08:53:15.484Z\",\"updatedAt\":\"2025-12-23T22:37:04.357Z\"}}},\"Action\":{\"id\":5538,\"Text\":\"Вступить\",\"URL\":\"https://t.me/+bikU3_M1x0s0YzMy\",\"Title\":\"Вступить\",\"MetrikaCounter\":null,\"MetrikaGoalId\":null,\"IsExternal\":true,\"Variant\":\"primary\",\"IsDisabled\":false,\"HardNavigate\":false,\"Description\":null,\"MetrikaGoalParams\":null}},\"referer\":\"https://education.yandex.ru/showcaptcha?form-fb-hint=8.77\u0026mt=1A1D517A55B311C1D8BA417B5270441998AF35146A58224E0F1C5E5C503D35BD2F02040D6200275E5549973B5A91CBF45018589B0091B6F7C6DA77D60E20E127EE3ED798DEEF2E0146B330ABD398C34FFE679BE5DF149FB78EC1B52FA137D2A7DABB3E8B8DDFADF75E514AAE437CC85DB889F8C7E2C41074E1F3BC46C2F515C288A2CC3EF5455DE10D3C64EE6B4B43B03E46D304A2DB33EC1C418F0B128F585120DD9F2E2146D27D8824D6A1F4CB367CD66418DAA89F5C24BEF9C8CBF1A9C69C59B2075EAC160132779D3FB0924119356515DA633EDCF30CB5D2D8D2CDEE\u0026retpath=aHR0cHM6Ly9lZHVjYXRpb24ueWFuZGV4LnJ1L2hhbmRib29rL21hdGgvYXJ0aWNsZS9tYXRoLWdsYXZhLWNoZXRpcmUtY2hlbXUtdmktbmF1Y2hpbGlzPw%2C%2C_6cc104a4e5d977a33a502e6015e451a8\u0026t=2%252F1768064960%252F42ad1d131815cb789d4b0cef24f04498\u0026u=7972366549262711656\u0026s=39c6e58ecb2f1aa1e1e6b8c522216e0c\"}]\n"])</script><script src="https://yastatic.net/s3/cloud/forms/_/embed.js" nonce="" data-nscript="afterInteractive"></script><next-route-announcer style="position: absolute;"></next-route-announcer><div class="gdpr-popup-v3-main" role="alert" aria-labelledby="gdpr-popup-v3-main-title" aria-live="assertive" lang="ru-RU">
            <div class="gdpr-popup-v3-main__content-container">
                <h1 class="gdpr-popup-v3-main__main-title gdpr-popup-v3-main__title" id="gdpr-popup-v3-main-title">
                    Yandex uses cookies
                </h1>
                <div class="gdpr-popup-v3-main__main-description" id="gdpr-popup-v3-main-description">
                    They ensure the smooth operation of all Yandex sites and services. For more information, please read our <a href="https://yandex.com/legal/cookies_policy_eng/" target="_blank">Cookie Policy</a>
                </div>
                <div class="gdpr-popup-v3-main__main-buttons gdpr-popup-v3-main__buttons">
                <div class="gdpr-popup-v3-button gdpr-popup-v3-button_id_all" aria-label="" role="button" tabindex="0" id="gdpr-popup-v3-button-all">Allow all</div><div class="gdpr-popup-v3-button gdpr-popup-v3-button_id_mandatory" aria-label="" role="button" tabindex="0" id="gdpr-popup-v3-button-mandatory">Allow essential cookies</div><div class="gdpr-popup-v3-button gdpr-popup-v3-button_theme_dark gdpr-popup-v3-button_id_settings" aria-label="" role="button" tabindex="0" id="gdpr-popup-v3-button-settings">Settings</div></div>
            </div>
            <div class="gdpr-popup-v3-main__settings-container gdpr-popup-v3-hidden">
                <div class="gdpr-popup-v3-main__settings-title"><div class="gdpr-popup-v3-button gdpr-popup-v3-button_theme_transparent gdpr-popup-v3-button_id_mobile-back" aria-label="Back to home page" role="button" tabindex="0" id="gdpr-popup-v3-button-mobile-back">
        <svg class="gdpr-popup-v3-svg" height="1em" width="1em" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M10.471 12.195a.667.667 0 0 1-.942.943l-4.431-4.43a1 1 0 0 1 0-1.415l4.43-4.431a.667.667 0 0 1 .943.943L6.276 8l4.195 4.195Z" fill="currentColor" fill-opacity="1">
        </path></svg></div>
                    <h1 class="gdpr-popup-v3-main__title">Cookie settings</h1>
                </div>
                <div class="gdpr-popup-v3-main__settings-description">
                    Yandex uses technical cookies to ensure the site/app functions properly. However, we would also like to use optional marketing, analytics, and other types of cookies. These cookies help Yandex improve services, your experience, and ad relevancy. For more information about cookies, please read our <a href="https://yandex.com/legal/cookies_policy_eng/" target="_blank">Cookie Policy</a>
                </div>
                <div class="gdpr-popup-v3-main__spoilers">
                <div class="gdpr-popup-v3-spoiler gdpr-popup-v3-spoiler_id_technical">
        <div class="gdpr-popup-v3-spoiler__header">
            <div class="gdpr-popup-v3-spoiler__opener" role="button" tabindex="0" aria-label="Show cookie descriptions" id="gdpr-popup-v3-spoiler-opener-technical" aria-controls="gdpr-popup-v3-spoiler-content-technical" aria-expanded="false">
            <svg class="gdpr-popup-v3-svg" height="1.5em" width="1.5em" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M10 2.5a.833.833 0 0 0-.833.833v5.834H3.333a.833.833 0 0 0 0 1.666h5.834v5.834a.833.833 0 0 0 1.666 0v-5.834h5.834a.833.833 0 0 0 0-1.666h-5.834V3.333A.833.833 0 0 0 10 2.5Z" fill="currentColor" fill-opacity="0.5">
        </path></svg></div>
            <h3 class="gdpr-popup-v3-spoiler__label" id="gdpr-popup-v3-spoiler-label-technical">
                Technical, always active
            </h3>
            <div class="gdpr-popup-v3-spoiler__checkbox-container">
            <div class="gdpr-popup-v3-checkbox gdpr-popup-v3-checkbox_checked_yes gdpr-popup-v3-checkbox_disabled_yes" aria-checked="true" aria-disabled="true" role="switch" aria-labelledby="gdpr-popup-v3-spoiler-label-technical" tabindex="0" id="gdpr-popup-v3-check-technical">   
            <div class="gdpr-popup-v3-checkbox__check"></div>
        </div></div>
        </div>
        <div class="gdpr-popup-v3-spoiler__content gdpr-popup-v3-hidden" role="region" id="gdpr-popup-v3-spoiler-content-technical" aria-labelledby="gdpr-popup-v3-spoiler-label-technical">
            <p class="gdpr-popup-v3-spoiler__description" id="gdpr-popup-v3-spoiler-description-technical">
                These cookies are necessary for crucial web browsing functions, such as authorization or navigation. They are essential to the service's functionality, so they are used by default.
            </p>
            
        </div>
    </div><div class="gdpr-popup-v3-spoiler gdpr-popup-v3-spoiler_id_analytics">
        <div class="gdpr-popup-v3-spoiler__header">
            <div class="gdpr-popup-v3-spoiler__opener" role="button" tabindex="0" aria-label="Show cookie descriptions" id="gdpr-popup-v3-spoiler-opener-analytics" aria-controls="gdpr-popup-v3-spoiler-content-analytics" aria-expanded="false">
            <svg class="gdpr-popup-v3-svg" height="1.5em" width="1.5em" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M10 2.5a.833.833 0 0 0-.833.833v5.834H3.333a.833.833 0 0 0 0 1.666h5.834v5.834a.833.833 0 0 0 1.666 0v-5.834h5.834a.833.833 0 0 0 0-1.666h-5.834V3.333A.833.833 0 0 0 10 2.5Z" fill="currentColor" fill-opacity="0.5">
        </path></svg></div>
            <h3 class="gdpr-popup-v3-spoiler__label" id="gdpr-popup-v3-spoiler-label-analytics">
                Analytics/marketing
            </h3>
            <div class="gdpr-popup-v3-spoiler__checkbox-container">
            <div class="gdpr-popup-v3-checkbox gdpr-popup-v3-checkbox_checked_yes gdpr-popup-v3-checkbox_disabled_no" aria-checked="true" aria-disabled="false" role="switch" aria-labelledby="gdpr-popup-v3-spoiler-label-analytics" tabindex="0" id="gdpr-popup-v3-check-analytics">   
            <div class="gdpr-popup-v3-checkbox__check"></div>
        </div></div>
        </div>
        <div class="gdpr-popup-v3-spoiler__content gdpr-popup-v3-hidden" role="region" id="gdpr-popup-v3-spoiler-content-analytics" aria-labelledby="gdpr-popup-v3-spoiler-label-analytics">
            <p class="gdpr-popup-v3-spoiler__description" id="gdpr-popup-v3-spoiler-description-analytics">
                These cookies improve the quality of Yandex services. They remember preference settings, anonymously analyze website traffic, and help displaying relevant ads.
            </p>
            
        </div>
    </div><div class="gdpr-popup-v3-spoiler gdpr-popup-v3-spoiler_id_other">
        <div class="gdpr-popup-v3-spoiler__header">
            <div class="gdpr-popup-v3-spoiler__opener" role="button" tabindex="0" aria-label="Show cookie descriptions" id="gdpr-popup-v3-spoiler-opener-other" aria-controls="gdpr-popup-v3-spoiler-content-other" aria-expanded="false">
            <svg class="gdpr-popup-v3-svg" height="1.5em" width="1.5em" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M10 2.5a.833.833 0 0 0-.833.833v5.834H3.333a.833.833 0 0 0 0 1.666h5.834v5.834a.833.833 0 0 0 1.666 0v-5.834h5.834a.833.833 0 0 0 0-1.666h-5.834V3.333A.833.833 0 0 0 10 2.5Z" fill="currentColor" fill-opacity="0.5">
        </path></svg></div>
            <h3 class="gdpr-popup-v3-spoiler__label" id="gdpr-popup-v3-spoiler-label-other">
                Other cookies
            </h3>
            <div class="gdpr-popup-v3-spoiler__checkbox-container">
            <div class="gdpr-popup-v3-checkbox gdpr-popup-v3-checkbox_checked_yes gdpr-popup-v3-checkbox_disabled_no" aria-checked="true" aria-disabled="false" role="switch" aria-labelledby="gdpr-popup-v3-spoiler-label-other" tabindex="0" id="gdpr-popup-v3-check-other">   
            <div class="gdpr-popup-v3-checkbox__check"></div>
        </div></div>
        </div>
        <div class="gdpr-popup-v3-spoiler__content gdpr-popup-v3-hidden" role="region" id="gdpr-popup-v3-spoiler-content-other" aria-labelledby="gdpr-popup-v3-spoiler-label-other">
            <p class="gdpr-popup-v3-spoiler__description" id="gdpr-popup-v3-spoiler-description-other">
                These cookies are non-essential. They help to improve the experience of working on Yandex services and websites. They help to restore web page sessions, remember preferred regions, save personal preferences, and more.
            </p>
            
        </div>
    </div></div>
                <div class="gdpr-popup-v3-main__settings-buttons  gdpr-popup-v3-main__buttons">
                <div class="gdpr-popup-v3-button gdpr-popup-v3-button_theme_dark gdpr-popup-v3-button_id_back" aria-label="Back to home page" role="button" tabindex="0" id="gdpr-popup-v3-button-back">
        <svg class="gdpr-popup-v3-svg" height="1em" width="1em" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M10.471 12.195a.667.667 0 0 1-.942.943l-4.431-4.43a1 1 0 0 1 0-1.415l4.43-4.431a.667.667 0 0 1 .943.943L6.276 8l4.195 4.195Z" fill="currentColor" fill-opacity="1">
        </path></svg></div><div class="gdpr-popup-v3-button gdpr-popup-v3-button_id_selected" aria-label="" role="button" tabindex="0" id="gdpr-popup-v3-button-selected">Allow selected cookies</div><div class="gdpr-popup-v3-button gdpr-popup-v3-button_theme_dark gdpr-popup-v3-button_id_settings-mandatory" aria-label="" role="button" tabindex="0" id="gdpr-popup-v3-button-settings-mandatory">Allow essential cookies</div></div>
            </div>
        </div></body></html>